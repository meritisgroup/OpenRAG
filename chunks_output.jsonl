{"name_doc": "RAG_scalabilité.pdf", "doc_id": 0, "chunk_id": 1, "chunk_content": "RAG Scalability\nClement Hardy, Benoit Joly\nAugust 25, 2025\nAbstract\nThe following study will compare different techniques\non a specific use case that we have made. The goal is\nto determine the impact of features on a RAG method\nretrieval and post-retrieval part.1\nScalability Metrics\n1.1\nMethodology\nWe suppose to have a set of documents D and a set of\nquestions Q on these documents. For each question\nq ∈Q, we determined the context C(q) needed to\nanswer the question q.For every splitter s, we denote Cs the set of chunks\nthat represent the set of documents D.For every\nquestion q ∈D, we denote the set of chunks Cs(q)\ncontaining the context C(q).For every RAG method that retrieves chunks as part\nof context for the LLM, we calculate the minimum\n"}
{"name_doc": "RAG_scalabilité.pdf", "doc_id": 0, "chunk_id": 2, "chunk_content": "of context for the LLM, we calculate the minimum\nof chunks retrieved such that the context contains\nall the needed information. Let us denote by R the\nretrieve function of the RAG method and MR the\nmaximum of retrieved chunks of R The goal is to\nstudy the restriction of R to the set of question Q.R\n:\nQ\n→\nCs\nq\n7→\nR(q) ,\nwhere R(q) is an ordered list of chunks.The absolute goal of a RAG method is for R to\nsatisfy C(q) ⊂R(q) for every user query q while\nminimizing MR.Hence we will run a serie of tests of, such call, best\npractices modifying our RAG method to determine\nthe impact of every feature on the retrieve function R.1.2\nMetrics\nFirst, it is important to differentiate the retrive\nfunction R of a RAG method to the actual context\n"}
{"name_doc": "RAG_scalabilité.pdf", "doc_id": 0, "chunk_id": 3, "chunk_content": "function R of a RAG method to the actual context\ngiven to the final LLM to generte the user answer.This context is created by a fixed portion of the\nretrieved chunks determine by the RAG method and\nthe context length of the LLM used. Usually, for a\nquery q, only the \"best\" chunks from R(q) is kept,\nHence we can study Rn the retrieval function where\nwe look for the n best chunks (if the method allows\nit) without modifying the studied RAG method and\nthe final context.For every feature, we calculate the proportion Pn(q)\nof usefull context in the retrieved context Rn(q) of\nevery question q for a fixed sample of n :\nPn(q) = #Cs(q) ∩Rn(q)\n#Rn(q)\nWe also calculate the best and the worst place\nof the elements of #Cs(q) ∩Rn(q) denoted Bs(q)\n"}
{"name_doc": "RAG_scalabilité.pdf", "doc_id": 0, "chunk_id": 4, "chunk_content": "of the elements of #Cs(q) ∩Rn(q) denoted Bs(q)\nand Ws(q) to follow it’s evolutions thought different\nfeatures. Note that Bs and Ws doesn’t depend on n.The second metrics we will estimate is the overall\nquality of the order of the retrieval using the nDCGp\nmetric defined as follows :\n1\nnDCGp(q) =\nPp\ni=1\n2reli(q)−1\nlog2(i+1)\nPp\ni=1\n2rel∗\ni (q)−1\nlog2(i+1)\n,\n(1)\nwhere p is the number of retrieve chunks by the\nretrieval, reli(q) is the relevance of each chunk\nretrieved and rel∗\ni (q) the ideal relevance of the set of\nretrieved chunks for the query q.For a clear automatization, we will define i to be 3\nif the chunk is in Cs(q), 1 if the chunk is in the right\ndocument and 0 else.The computation of this metric can be done post\n"}
{"name_doc": "RAG_scalabilité.pdf", "doc_id": 0, "chunk_id": 5, "chunk_content": "The computation of this metric can be done post\nruns, as we can try distinct p ≤n and get an estimate\nof a good p for a RAG method without considering\ncontext length of the LLM.1.3\nLimits\nThe first limit is for every methods that can’t recover\nan unlimited number of chunks as threshold technics.However, as explained, the goal is to analyse the\nimpact of features on the retrieve part and not on the\ncreation of the context. Indeed, threshold techniques\nare used to minimize the context lenght and then the\ninference time after the retrieve phase.Another limit is the difficulty to estimate why some\nnon-usefull chunks are retrieved and how close they\nare to the actual retrieve chunks. This information\nstill remains crucial to avoid hallucination from the\n"}
{"name_doc": "RAG_scalabilité.pdf", "doc_id": 0, "chunk_id": 6, "chunk_content": "still remains crucial to avoid hallucination from the\nLLM generating the final answer but won’t be study\nin our first approach.The scalability is limited as the maximal number\nof documents may not be relevant for some more\ndelicate use cases.2\nFeatures to evaluate\nWe enumerate all the differents features to tests\n1. Data preparation\n2. Splitters\n3. Rerankers\n4. Query Reformulations\n5. Embeddings\n6. Metadatas\n2.0.1\nData Preparation\nWe want to estimate the differences between Dp ways\nto prepare the data before splitting it. We have 2\ndata preparation to compare :\n1. PDFs without prepations\n2. Markdowns separation using dockling\nRemark. There are multiple ways to parse the data\nbefore chunking. Some are not relevant on our use\n"}
{"name_doc": "RAG_scalabilité.pdf", "doc_id": 0, "chunk_id": 7, "chunk_content": " Some are not relevant on our use\ncase that we are testing.2.0.2\nSplitters\nWe will try S several splitting technics :\n1. Length splitting\n2. Semantic Splitting\n3. Recursive Splitting\n4. Markdown Splitting (only available for the Mark-\ndown data preparation)\n2.1\nMetadatas\nWe will consider the impact of metadatas. We will\ndetermine some kinf of metadatas relevant with our\nfake use case and see the impact of each of them and\neach combination of them.2\n2.1.1\nRerankers\nWe will try distincts rerankers separatively.Best\nwould be to benchmark all rerankers providers as in\nAutoRAG. unfortunately it won’t be the case in first\ninstance but would be possible to test them as we\nwill keep all chunks retrieved pre-reranking. Indeed,\nthe reranking phase do not require to re run the\n"}
{"name_doc": "RAG_scalabilité.pdf", "doc_id": 0, "chunk_id": 8, "chunk_content": "the reranking phase do not require to re run the\nentire pipeline as it only rerank the chunks already\nordered and retrieved.We denote by R the number of tested rerankers\n2.1.2\nQuery Reformulation\nWe won’t study naive query reformulation as we con-\nsider our prompt as \"good\" prompts that respect best\npractice of prompt engineering. Moreover, we can\nstudy the multi query expansion. This would impose\nto fix a merge strategy of the contexts retrieved from\nall queries or another metric to evaluate if all the use-\nful chunks C(q) are retrieved through all the queries\ngenerated from the user’s one.2.1.3\nEmbeddings\nWe could try all the embeddings, but let us be\nrealistic, we will compare few ones already consider\nas best by Huggingface leader boards."}
{"name_doc": "RAG_scalabilité.pdf", "doc_id": 0, "chunk_id": 9, "chunk_content": "as best by Huggingface leader boards. We will also\ntry the impact of a finetuned embedding.Let us\ndenote by E the number of embeddings.3\nFeatures that won’t affect the\nretrieval study\nWe enumerate all the different features that will not\ninfluence the retrieval but would affect the RAG ac-\ncuracy overall.1. Number of retrieved chunks\n2. Passage augmenter\n3. Passage filter (Thresholds)\n4. Passage Compressor\nAs well we won’t study the server optimizations as\nsharding.4\nNumber\nof\nruns\n(without\nmetadatas)\nWe will measure the impact of features on different\ndataset size by adding more documents not relevant\nto our queries sample. Let us compute all the tests\nwe would :\nT = [(Dp −1) ∗(S −1) ∗E ∗R + S ∗E ∗R] ∗N,\n(2)\ntests to get all information about scalability, where\n"}
{"name_doc": "RAG_scalabilité.pdf", "doc_id": 0, "chunk_id": 10, "chunk_content": "tests to get all information about scalability, where\nN is the number of dataset we are testing.For Dp = 2, S = 4, E = 3, R = 3, N = 1 we have\nalready 63 tests to run. We can run all the tests for\nn = 500 and then study the results as if n was smaller\nsince the retrieved phases are not directly impacted\nby n.5\nCurves to print\nFor every n we will print Pn(q) and nNDCG(q) as a\nconfidence interval diagrams.For every feature, we will plot Pn(q) for different\nvalues of n.For every feature we will compute the minimum\nnmin(p) such that Pn(q) = 1 if possible. Note that\nnmin is calculated before posr-retrived features.To evaluate rerankers we especially compare the\ndifference between nmin(p) and nDCGp(q) before post-\nretrieval and nmin(p) and nDCGp(q) for the post-\n"}
{"name_doc": "RAG_scalabilité.pdf", "doc_id": 0, "chunk_id": 11, "chunk_content": "retrieval and nmin(p) and nDCGp(q) for the post-\nretrieval part.3\n"}
{"name_doc": "3002033669_Scaling Intelligence_ The Exponential Growth of AI's Power Needs.PDF", "doc_id": 1, "chunk_id": 1, "chunk_content": "Extension de fichier inconnu\n"}
{"name_doc": "RAG_scalabilité.pdf", "doc_id": 0, "chunk_id": 1, "chunk_content": "RAG Scalability\nClement Hardy, Benoit Joly\nAugust 25, 2025\nAbstract\nThe following study will compare different techniques\non a specific use case that we have made. The goal is\nto determine the impact of features on a RAG method\nretrieval and post-retrieval part.1\nScalability Metrics\n1.1\nMethodology\nWe suppose to have a set of documents D and a set of\nquestions Q on these documents. For each question\nq ∈Q, we determined the context C(q) needed to\nanswer the question q.For every splitter s, we denote Cs the set of chunks\nthat represent the set of documents D.For every\nquestion q ∈D, we denote the set of chunks Cs(q)\ncontaining the context C(q).For every RAG method that retrieves chunks as part\nof context for the LLM, we calculate the minimum\n"}
{"name_doc": "RAG_scalabilité.pdf", "doc_id": 0, "chunk_id": 2, "chunk_content": "of context for the LLM, we calculate the minimum\nof chunks retrieved such that the context contains\nall the needed information. Let us denote by R the\nretrieve function of the RAG method and MR the\nmaximum of retrieved chunks of R The goal is to\nstudy the restriction of R to the set of question Q.R\n:\nQ\n→\nCs\nq\n7→\nR(q) ,\nwhere R(q) is an ordered list of chunks.The absolute goal of a RAG method is for R to\nsatisfy C(q) ⊂R(q) for every user query q while\nminimizing MR.Hence we will run a serie of tests of, such call, best\npractices modifying our RAG method to determine\nthe impact of every feature on the retrieve function R.1.2\nMetrics\nFirst, it is important to differentiate the retrive\nfunction R of a RAG method to the actual context\n"}
{"name_doc": "RAG_scalabilité.pdf", "doc_id": 0, "chunk_id": 3, "chunk_content": "function R of a RAG method to the actual context\ngiven to the final LLM to generte the user answer.This context is created by a fixed portion of the\nretrieved chunks determine by the RAG method and\nthe context length of the LLM used. Usually, for a\nquery q, only the \"best\" chunks from R(q) is kept,\nHence we can study Rn the retrieval function where\nwe look for the n best chunks (if the method allows\nit) without modifying the studied RAG method and\nthe final context.For every feature, we calculate the proportion Pn(q)\nof usefull context in the retrieved context Rn(q) of\nevery question q for a fixed sample of n :\nPn(q) = #Cs(q) ∩Rn(q)\n#Rn(q)\nWe also calculate the best and the worst place\nof the elements of #Cs(q) ∩Rn(q) denoted Bs(q)\n"}
{"name_doc": "RAG_scalabilité.pdf", "doc_id": 0, "chunk_id": 4, "chunk_content": "of the elements of #Cs(q) ∩Rn(q) denoted Bs(q)\nand Ws(q) to follow it’s evolutions thought different\nfeatures. Note that Bs and Ws doesn’t depend on n.The second metrics we will estimate is the overall\nquality of the order of the retrieval using the nDCGp\nmetric defined as follows :\n1\nnDCGp(q) =\nPp\ni=1\n2reli(q)−1\nlog2(i+1)\nPp\ni=1\n2rel∗\ni (q)−1\nlog2(i+1)\n,\n(1)\nwhere p is the number of retrieve chunks by the\nretrieval, reli(q) is the relevance of each chunk\nretrieved and rel∗\ni (q) the ideal relevance of the set of\nretrieved chunks for the query q.For a clear automatization, we will define i to be 3\nif the chunk is in Cs(q), 1 if the chunk is in the right\ndocument and 0 else.The computation of this metric can be done post\n"}
{"name_doc": "RAG_scalabilité.pdf", "doc_id": 0, "chunk_id": 5, "chunk_content": "The computation of this metric can be done post\nruns, as we can try distinct p ≤n and get an estimate\nof a good p for a RAG method without considering\ncontext length of the LLM.1.3\nLimits\nThe first limit is for every methods that can’t recover\nan unlimited number of chunks as threshold technics.However, as explained, the goal is to analyse the\nimpact of features on the retrieve part and not on the\ncreation of the context. Indeed, threshold techniques\nare used to minimize the context lenght and then the\ninference time after the retrieve phase.Another limit is the difficulty to estimate why some\nnon-usefull chunks are retrieved and how close they\nare to the actual retrieve chunks. This information\nstill remains crucial to avoid hallucination from the\n"}
{"name_doc": "RAG_scalabilité.pdf", "doc_id": 0, "chunk_id": 6, "chunk_content": "still remains crucial to avoid hallucination from the\nLLM generating the final answer but won’t be study\nin our first approach.The scalability is limited as the maximal number\nof documents may not be relevant for some more\ndelicate use cases.2\nFeatures to evaluate\nWe enumerate all the differents features to tests\n1. Data preparation\n2. Splitters\n3. Rerankers\n4. Query Reformulations\n5. Embeddings\n6. Metadatas\n2.0.1\nData Preparation\nWe want to estimate the differences between Dp ways\nto prepare the data before splitting it. We have 2\ndata preparation to compare :\n1. PDFs without prepations\n2. Markdowns separation using dockling\nRemark. There are multiple ways to parse the data\nbefore chunking. Some are not relevant on our use\n"}
{"name_doc": "RAG_scalabilité.pdf", "doc_id": 0, "chunk_id": 7, "chunk_content": " Some are not relevant on our use\ncase that we are testing.2.0.2\nSplitters\nWe will try S several splitting technics :\n1. Length splitting\n2. Semantic Splitting\n3. Recursive Splitting\n4. Markdown Splitting (only available for the Mark-\ndown data preparation)\n2.1\nMetadatas\nWe will consider the impact of metadatas. We will\ndetermine some kinf of metadatas relevant with our\nfake use case and see the impact of each of them and\neach combination of them.2\n2.1.1\nRerankers\nWe will try distincts rerankers separatively.Best\nwould be to benchmark all rerankers providers as in\nAutoRAG. unfortunately it won’t be the case in first\ninstance but would be possible to test them as we\nwill keep all chunks retrieved pre-reranking. Indeed,\nthe reranking phase do not require to re run the\n"}
{"name_doc": "RAG_scalabilité.pdf", "doc_id": 0, "chunk_id": 8, "chunk_content": "the reranking phase do not require to re run the\nentire pipeline as it only rerank the chunks already\nordered and retrieved.We denote by R the number of tested rerankers\n2.1.2\nQuery Reformulation\nWe won’t study naive query reformulation as we con-\nsider our prompt as \"good\" prompts that respect best\npractice of prompt engineering. Moreover, we can\nstudy the multi query expansion. This would impose\nto fix a merge strategy of the contexts retrieved from\nall queries or another metric to evaluate if all the use-\nful chunks C(q) are retrieved through all the queries\ngenerated from the user’s one.2.1.3\nEmbeddings\nWe could try all the embeddings, but let us be\nrealistic, we will compare few ones already consider\nas best by Huggingface leader boards."}
{"name_doc": "RAG_scalabilité.pdf", "doc_id": 0, "chunk_id": 9, "chunk_content": "as best by Huggingface leader boards. We will also\ntry the impact of a finetuned embedding.Let us\ndenote by E the number of embeddings.3\nFeatures that won’t affect the\nretrieval study\nWe enumerate all the different features that will not\ninfluence the retrieval but would affect the RAG ac-\ncuracy overall.1. Number of retrieved chunks\n2. Passage augmenter\n3. Passage filter (Thresholds)\n4. Passage Compressor\nAs well we won’t study the server optimizations as\nsharding.4\nNumber\nof\nruns\n(without\nmetadatas)\nWe will measure the impact of features on different\ndataset size by adding more documents not relevant\nto our queries sample. Let us compute all the tests\nwe would :\nT = [(Dp −1) ∗(S −1) ∗E ∗R + S ∗E ∗R] ∗N,\n(2)\ntests to get all information about scalability, where\n"}
{"name_doc": "RAG_scalabilité.pdf", "doc_id": 0, "chunk_id": 10, "chunk_content": "tests to get all information about scalability, where\nN is the number of dataset we are testing.For Dp = 2, S = 4, E = 3, R = 3, N = 1 we have\nalready 63 tests to run. We can run all the tests for\nn = 500 and then study the results as if n was smaller\nsince the retrieved phases are not directly impacted\nby n.5\nCurves to print\nFor every n we will print Pn(q) and nNDCG(q) as a\nconfidence interval diagrams.For every feature, we will plot Pn(q) for different\nvalues of n.For every feature we will compute the minimum\nnmin(p) such that Pn(q) = 1 if possible. Note that\nnmin is calculated before posr-retrived features.To evaluate rerankers we especially compare the\ndifference between nmin(p) and nDCGp(q) before post-\nretrieval and nmin(p) and nDCGp(q) for the post-\n"}
{"name_doc": "RAG_scalabilité.pdf", "doc_id": 0, "chunk_id": 11, "chunk_content": "retrieval and nmin(p) and nDCGp(q) for the post-\nretrieval part.3\n"}
{"name_doc": "3002033669_Scaling Intelligence_ The Exponential Growth of AI's Power Needs.PDF", "doc_id": 1, "chunk_id": 1, "chunk_content": "Extension de fichier inconnu\n"}
{"name_doc": "RAG_scalabilité.pdf", "doc_id": 0, "chunk_id": 1, "chunk_content": "RAG Scalability\nClement Hardy, Benoit Joly\nAugust 25, 2025\nAbstract\nThe following study will compare different techniques\non a specific use case that we have made. The goal is\nto determine the impact of features on a RAG method\nretrieval and post-retrieval part.1\nScalability Metrics\n1.1\nMethodology\nWe suppose to have a set of documents D and a set of\nquestions Q on these documents. For each question\nq ∈Q, we determined the context C(q) needed to\nanswer the question q.For every splitter s, we denote Cs the set of chunks\nthat represent the set of documents D.For every\nquestion q ∈D, we denote the set of chunks Cs(q)\ncontaining the context C(q).For every RAG method that retrieves chunks as part\nof context for the LLM, we calculate the minimum\n"}
{"name_doc": "RAG_scalabilité.pdf", "doc_id": 0, "chunk_id": 2, "chunk_content": "of context for the LLM, we calculate the minimum\nof chunks retrieved such that the context contains\nall the needed information. Let us denote by R the\nretrieve function of the RAG method and MR the\nmaximum of retrieved chunks of R The goal is to\nstudy the restriction of R to the set of question Q.R\n:\nQ\n→\nCs\nq\n7→\nR(q) ,\nwhere R(q) is an ordered list of chunks.The absolute goal of a RAG method is for R to\nsatisfy C(q) ⊂R(q) for every user query q while\nminimizing MR.Hence we will run a serie of tests of, such call, best\npractices modifying our RAG method to determine\nthe impact of every feature on the retrieve function R.1.2\nMetrics\nFirst, it is important to differentiate the retrive\nfunction R of a RAG method to the actual context\n"}
{"name_doc": "RAG_scalabilité.pdf", "doc_id": 0, "chunk_id": 3, "chunk_content": "function R of a RAG method to the actual context\ngiven to the final LLM to generte the user answer.This context is created by a fixed portion of the\nretrieved chunks determine by the RAG method and\nthe context length of the LLM used. Usually, for a\nquery q, only the \"best\" chunks from R(q) is kept,\nHence we can study Rn the retrieval function where\nwe look for the n best chunks (if the method allows\nit) without modifying the studied RAG method and\nthe final context.For every feature, we calculate the proportion Pn(q)\nof usefull context in the retrieved context Rn(q) of\nevery question q for a fixed sample of n :\nPn(q) = #Cs(q) ∩Rn(q)\n#Rn(q)\nWe also calculate the best and the worst place\nof the elements of #Cs(q) ∩Rn(q) denoted Bs(q)\n"}
{"name_doc": "RAG_scalabilité.pdf", "doc_id": 0, "chunk_id": 4, "chunk_content": "of the elements of #Cs(q) ∩Rn(q) denoted Bs(q)\nand Ws(q) to follow it’s evolutions thought different\nfeatures. Note that Bs and Ws doesn’t depend on n.The second metrics we will estimate is the overall\nquality of the order of the retrieval using the nDCGp\nmetric defined as follows :\n1\nnDCGp(q) =\nPp\ni=1\n2reli(q)−1\nlog2(i+1)\nPp\ni=1\n2rel∗\ni (q)−1\nlog2(i+1)\n,\n(1)\nwhere p is the number of retrieve chunks by the\nretrieval, reli(q) is the relevance of each chunk\nretrieved and rel∗\ni (q) the ideal relevance of the set of\nretrieved chunks for the query q.For a clear automatization, we will define i to be 3\nif the chunk is in Cs(q), 1 if the chunk is in the right\ndocument and 0 else.The computation of this metric can be done post\n"}
{"name_doc": "RAG_scalabilité.pdf", "doc_id": 0, "chunk_id": 5, "chunk_content": "The computation of this metric can be done post\nruns, as we can try distinct p ≤n and get an estimate\nof a good p for a RAG method without considering\ncontext length of the LLM.1.3\nLimits\nThe first limit is for every methods that can’t recover\nan unlimited number of chunks as threshold technics.However, as explained, the goal is to analyse the\nimpact of features on the retrieve part and not on the\ncreation of the context. Indeed, threshold techniques\nare used to minimize the context lenght and then the\ninference time after the retrieve phase.Another limit is the difficulty to estimate why some\nnon-usefull chunks are retrieved and how close they\nare to the actual retrieve chunks. This information\nstill remains crucial to avoid hallucination from the\n"}
{"name_doc": "RAG_scalabilité.pdf", "doc_id": 0, "chunk_id": 6, "chunk_content": "still remains crucial to avoid hallucination from the\nLLM generating the final answer but won’t be study\nin our first approach.The scalability is limited as the maximal number\nof documents may not be relevant for some more\ndelicate use cases.2\nFeatures to evaluate\nWe enumerate all the differents features to tests\n1. Data preparation\n2. Splitters\n3. Rerankers\n4. Query Reformulations\n5. Embeddings\n6. Metadatas\n2.0.1\nData Preparation\nWe want to estimate the differences between Dp ways\nto prepare the data before splitting it. We have 2\ndata preparation to compare :\n1. PDFs without prepations\n2. Markdowns separation using dockling\nRemark. There are multiple ways to parse the data\nbefore chunking. Some are not relevant on our use\n"}
{"name_doc": "RAG_scalabilité.pdf", "doc_id": 0, "chunk_id": 7, "chunk_content": " Some are not relevant on our use\ncase that we are testing.2.0.2\nSplitters\nWe will try S several splitting technics :\n1. Length splitting\n2. Semantic Splitting\n3. Recursive Splitting\n4. Markdown Splitting (only available for the Mark-\ndown data preparation)\n2.1\nMetadatas\nWe will consider the impact of metadatas. We will\ndetermine some kinf of metadatas relevant with our\nfake use case and see the impact of each of them and\neach combination of them.2\n2.1.1\nRerankers\nWe will try distincts rerankers separatively.Best\nwould be to benchmark all rerankers providers as in\nAutoRAG. unfortunately it won’t be the case in first\ninstance but would be possible to test them as we\nwill keep all chunks retrieved pre-reranking. Indeed,\nthe reranking phase do not require to re run the\n"}
{"name_doc": "RAG_scalabilité.pdf", "doc_id": 0, "chunk_id": 8, "chunk_content": "the reranking phase do not require to re run the\nentire pipeline as it only rerank the chunks already\nordered and retrieved.We denote by R the number of tested rerankers\n2.1.2\nQuery Reformulation\nWe won’t study naive query reformulation as we con-\nsider our prompt as \"good\" prompts that respect best\npractice of prompt engineering. Moreover, we can\nstudy the multi query expansion. This would impose\nto fix a merge strategy of the contexts retrieved from\nall queries or another metric to evaluate if all the use-\nful chunks C(q) are retrieved through all the queries\ngenerated from the user’s one.2.1.3\nEmbeddings\nWe could try all the embeddings, but let us be\nrealistic, we will compare few ones already consider\nas best by Huggingface leader boards."}
{"name_doc": "RAG_scalabilité.pdf", "doc_id": 0, "chunk_id": 9, "chunk_content": "as best by Huggingface leader boards. We will also\ntry the impact of a finetuned embedding.Let us\ndenote by E the number of embeddings.3\nFeatures that won’t affect the\nretrieval study\nWe enumerate all the different features that will not\ninfluence the retrieval but would affect the RAG ac-\ncuracy overall.1. Number of retrieved chunks\n2. Passage augmenter\n3. Passage filter (Thresholds)\n4. Passage Compressor\nAs well we won’t study the server optimizations as\nsharding.4\nNumber\nof\nruns\n(without\nmetadatas)\nWe will measure the impact of features on different\ndataset size by adding more documents not relevant\nto our queries sample. Let us compute all the tests\nwe would :\nT = [(Dp −1) ∗(S −1) ∗E ∗R + S ∗E ∗R] ∗N,\n(2)\ntests to get all information about scalability, where\n"}
{"name_doc": "RAG_scalabilité.pdf", "doc_id": 0, "chunk_id": 10, "chunk_content": "tests to get all information about scalability, where\nN is the number of dataset we are testing.For Dp = 2, S = 4, E = 3, R = 3, N = 1 we have\nalready 63 tests to run. We can run all the tests for\nn = 500 and then study the results as if n was smaller\nsince the retrieved phases are not directly impacted\nby n.5\nCurves to print\nFor every n we will print Pn(q) and nNDCG(q) as a\nconfidence interval diagrams.For every feature, we will plot Pn(q) for different\nvalues of n.For every feature we will compute the minimum\nnmin(p) such that Pn(q) = 1 if possible. Note that\nnmin is calculated before posr-retrived features.To evaluate rerankers we especially compare the\ndifference between nmin(p) and nDCGp(q) before post-\nretrieval and nmin(p) and nDCGp(q) for the post-\n"}
{"name_doc": "RAG_scalabilité.pdf", "doc_id": 0, "chunk_id": 11, "chunk_content": "retrieval and nmin(p) and nDCGp(q) for the post-\nretrieval part.3\n"}
{"name_doc": "RAG_scalabilité.pdf", "doc_id": 0, "chunk_id": 1, "chunk_content": "RAG Scalability\nClement Hardy, Benoit Joly\nAugust 25, 2025\nAbstract\nThe following study will compare different techniques\non a specific use case that we have made. The goal is\nto determine the impact of features on a RAG method\nretrieval and post-retrieval part.1\nScalability Metrics\n1.1\nMethodology\nWe suppose to have a set of documents D and a set of\nquestions Q on these documents. For each question\nq ∈Q, we determined the context C(q) needed to\nanswer the question q.For every splitter s, we denote Cs the set of chunks\nthat represent the set of documents D.For every\nquestion q ∈D, we denote the set of chunks Cs(q)\ncontaining the context C(q).For every RAG method that retrieves chunks as part\nof context for the LLM, we calculate the minimum\n"}
{"name_doc": "RAG_scalabilité.pdf", "doc_id": 0, "chunk_id": 2, "chunk_content": "of context for the LLM, we calculate the minimum\nof chunks retrieved such that the context contains\nall the needed information. Let us denote by R the\nretrieve function of the RAG method and MR the\nmaximum of retrieved chunks of R The goal is to\nstudy the restriction of R to the set of question Q.R\n:\nQ\n→\nCs\nq\n7→\nR(q) ,\nwhere R(q) is an ordered list of chunks.The absolute goal of a RAG method is for R to\nsatisfy C(q) ⊂R(q) for every user query q while\nminimizing MR.Hence we will run a serie of tests of, such call, best\npractices modifying our RAG method to determine\nthe impact of every feature on the retrieve function R.1.2\nMetrics\nFirst, it is important to differentiate the retrive\nfunction R of a RAG method to the actual context\n"}
{"name_doc": "RAG_scalabilité.pdf", "doc_id": 0, "chunk_id": 3, "chunk_content": "function R of a RAG method to the actual context\ngiven to the final LLM to generte the user answer.This context is created by a fixed portion of the\nretrieved chunks determine by the RAG method and\nthe context length of the LLM used. Usually, for a\nquery q, only the \"best\" chunks from R(q) is kept,\nHence we can study Rn the retrieval function where\nwe look for the n best chunks (if the method allows\nit) without modifying the studied RAG method and\nthe final context.For every feature, we calculate the proportion Pn(q)\nof usefull context in the retrieved context Rn(q) of\nevery question q for a fixed sample of n :\nPn(q) = #Cs(q) ∩Rn(q)\n#Rn(q)\nWe also calculate the best and the worst place\nof the elements of #Cs(q) ∩Rn(q) denoted Bs(q)\n"}
{"name_doc": "RAG_scalabilité.pdf", "doc_id": 0, "chunk_id": 4, "chunk_content": "of the elements of #Cs(q) ∩Rn(q) denoted Bs(q)\nand Ws(q) to follow it’s evolutions thought different\nfeatures. Note that Bs and Ws doesn’t depend on n.The second metrics we will estimate is the overall\nquality of the order of the retrieval using the nDCGp\nmetric defined as follows :\n1\nnDCGp(q) =\nPp\ni=1\n2reli(q)−1\nlog2(i+1)\nPp\ni=1\n2rel∗\ni (q)−1\nlog2(i+1)\n,\n(1)\nwhere p is the number of retrieve chunks by the\nretrieval, reli(q) is the relevance of each chunk\nretrieved and rel∗\ni (q) the ideal relevance of the set of\nretrieved chunks for the query q.For a clear automatization, we will define i to be 3\nif the chunk is in Cs(q), 1 if the chunk is in the right\ndocument and 0 else.The computation of this metric can be done post\n"}
{"name_doc": "RAG_scalabilité.pdf", "doc_id": 0, "chunk_id": 5, "chunk_content": "The computation of this metric can be done post\nruns, as we can try distinct p ≤n and get an estimate\nof a good p for a RAG method without considering\ncontext length of the LLM.1.3\nLimits\nThe first limit is for every methods that can’t recover\nan unlimited number of chunks as threshold technics.However, as explained, the goal is to analyse the\nimpact of features on the retrieve part and not on the\ncreation of the context. Indeed, threshold techniques\nare used to minimize the context lenght and then the\ninference time after the retrieve phase.Another limit is the difficulty to estimate why some\nnon-usefull chunks are retrieved and how close they\nare to the actual retrieve chunks. This information\nstill remains crucial to avoid hallucination from the\n"}
{"name_doc": "RAG_scalabilité.pdf", "doc_id": 0, "chunk_id": 6, "chunk_content": "still remains crucial to avoid hallucination from the\nLLM generating the final answer but won’t be study\nin our first approach.The scalability is limited as the maximal number\nof documents may not be relevant for some more\ndelicate use cases.2\nFeatures to evaluate\nWe enumerate all the differents features to tests\n1. Data preparation\n2. Splitters\n3. Rerankers\n4. Query Reformulations\n5. Embeddings\n6. Metadatas\n2.0.1\nData Preparation\nWe want to estimate the differences between Dp ways\nto prepare the data before splitting it. We have 2\ndata preparation to compare :\n1. PDFs without prepations\n2. Markdowns separation using dockling\nRemark. There are multiple ways to parse the data\nbefore chunking. Some are not relevant on our use\n"}
{"name_doc": "RAG_scalabilité.pdf", "doc_id": 0, "chunk_id": 7, "chunk_content": " Some are not relevant on our use\ncase that we are testing.2.0.2\nSplitters\nWe will try S several splitting technics :\n1. Length splitting\n2. Semantic Splitting\n3. Recursive Splitting\n4. Markdown Splitting (only available for the Mark-\ndown data preparation)\n2.1\nMetadatas\nWe will consider the impact of metadatas. We will\ndetermine some kinf of metadatas relevant with our\nfake use case and see the impact of each of them and\neach combination of them.2\n2.1.1\nRerankers\nWe will try distincts rerankers separatively.Best\nwould be to benchmark all rerankers providers as in\nAutoRAG. unfortunately it won’t be the case in first\ninstance but would be possible to test them as we\nwill keep all chunks retrieved pre-reranking. Indeed,\nthe reranking phase do not require to re run the\n"}
{"name_doc": "RAG_scalabilité.pdf", "doc_id": 0, "chunk_id": 8, "chunk_content": "the reranking phase do not require to re run the\nentire pipeline as it only rerank the chunks already\nordered and retrieved.We denote by R the number of tested rerankers\n2.1.2\nQuery Reformulation\nWe won’t study naive query reformulation as we con-\nsider our prompt as \"good\" prompts that respect best\npractice of prompt engineering. Moreover, we can\nstudy the multi query expansion. This would impose\nto fix a merge strategy of the contexts retrieved from\nall queries or another metric to evaluate if all the use-\nful chunks C(q) are retrieved through all the queries\ngenerated from the user’s one.2.1.3\nEmbeddings\nWe could try all the embeddings, but let us be\nrealistic, we will compare few ones already consider\nas best by Huggingface leader boards."}
{"name_doc": "RAG_scalabilité.pdf", "doc_id": 0, "chunk_id": 9, "chunk_content": "as best by Huggingface leader boards. We will also\ntry the impact of a finetuned embedding.Let us\ndenote by E the number of embeddings.3\nFeatures that won’t affect the\nretrieval study\nWe enumerate all the different features that will not\ninfluence the retrieval but would affect the RAG ac-\ncuracy overall.1. Number of retrieved chunks\n2. Passage augmenter\n3. Passage filter (Thresholds)\n4. Passage Compressor\nAs well we won’t study the server optimizations as\nsharding.4\nNumber\nof\nruns\n(without\nmetadatas)\nWe will measure the impact of features on different\ndataset size by adding more documents not relevant\nto our queries sample. Let us compute all the tests\nwe would :\nT = [(Dp −1) ∗(S −1) ∗E ∗R + S ∗E ∗R] ∗N,\n(2)\ntests to get all information about scalability, where\n"}
{"name_doc": "RAG_scalabilité.pdf", "doc_id": 0, "chunk_id": 10, "chunk_content": "tests to get all information about scalability, where\nN is the number of dataset we are testing.For Dp = 2, S = 4, E = 3, R = 3, N = 1 we have\nalready 63 tests to run. We can run all the tests for\nn = 500 and then study the results as if n was smaller\nsince the retrieved phases are not directly impacted\nby n.5\nCurves to print\nFor every n we will print Pn(q) and nNDCG(q) as a\nconfidence interval diagrams.For every feature, we will plot Pn(q) for different\nvalues of n.For every feature we will compute the minimum\nnmin(p) such that Pn(q) = 1 if possible. Note that\nnmin is calculated before posr-retrived features.To evaluate rerankers we especially compare the\ndifference between nmin(p) and nDCGp(q) before post-\nretrieval and nmin(p) and nDCGp(q) for the post-\n"}
{"name_doc": "RAG_scalabilité.pdf", "doc_id": 0, "chunk_id": 11, "chunk_content": "retrieval and nmin(p) and nDCGp(q) for the post-\nretrieval part.3\n"}
{"name_doc": "3002033669_Scaling Intelligence_ The Exponential Growth of AI's Power Needs.PDF", "doc_id": 1, "chunk_id": 1, "chunk_content": "Extension de fichier inconnu\n"}
{"name_doc": "RAG_scalabilité.pdf", "doc_id": 0, "chunk_id": 1, "chunk_content": "RAG Scalability\nClement Hardy, Benoit Joly\nAugust 25, 2025\nAbstract\nThe following study will compare different techniques\non a specific use case that we have made. The goal is\nto determine the impact of features on a RAG method\nretrieval and post-retrieval part.1\nScalability Metrics\n1.1\nMethodology\nWe suppose to have a set of documents D and a set of\nquestions Q on these documents. For each question\nq ∈Q, we determined the context C(q) needed to\nanswer the question q.For every splitter s, we denote Cs the set of chunks\nthat represent the set of documents D.For every\nquestion q ∈D, we denote the set of chunks Cs(q)\ncontaining the context C(q).For every RAG method that retrieves chunks as part\nof context for the LLM, we calculate the minimum\n"}
{"name_doc": "RAG_scalabilité.pdf", "doc_id": 0, "chunk_id": 2, "chunk_content": "of context for the LLM, we calculate the minimum\nof chunks retrieved such that the context contains\nall the needed information. Let us denote by R the\nretrieve function of the RAG method and MR the\nmaximum of retrieved chunks of R The goal is to\nstudy the restriction of R to the set of question Q.R\n:\nQ\n→\nCs\nq\n7→\nR(q) ,\nwhere R(q) is an ordered list of chunks.The absolute goal of a RAG method is for R to\nsatisfy C(q) ⊂R(q) for every user query q while\nminimizing MR.Hence we will run a serie of tests of, such call, best\npractices modifying our RAG method to determine\nthe impact of every feature on the retrieve function R.1.2\nMetrics\nFirst, it is important to differentiate the retrive\nfunction R of a RAG method to the actual context\n"}
{"name_doc": "RAG_scalabilité.pdf", "doc_id": 0, "chunk_id": 3, "chunk_content": "function R of a RAG method to the actual context\ngiven to the final LLM to generte the user answer.This context is created by a fixed portion of the\nretrieved chunks determine by the RAG method and\nthe context length of the LLM used. Usually, for a\nquery q, only the \"best\" chunks from R(q) is kept,\nHence we can study Rn the retrieval function where\nwe look for the n best chunks (if the method allows\nit) without modifying the studied RAG method and\nthe final context.For every feature, we calculate the proportion Pn(q)\nof usefull context in the retrieved context Rn(q) of\nevery question q for a fixed sample of n :\nPn(q) = #Cs(q) ∩Rn(q)\n#Rn(q)\nWe also calculate the best and the worst place\nof the elements of #Cs(q) ∩Rn(q) denoted Bs(q)\n"}
{"name_doc": "RAG_scalabilité.pdf", "doc_id": 0, "chunk_id": 4, "chunk_content": "of the elements of #Cs(q) ∩Rn(q) denoted Bs(q)\nand Ws(q) to follow it’s evolutions thought different\nfeatures. Note that Bs and Ws doesn’t depend on n.The second metrics we will estimate is the overall\nquality of the order of the retrieval using the nDCGp\nmetric defined as follows :\n1\nnDCGp(q) =\nPp\ni=1\n2reli(q)−1\nlog2(i+1)\nPp\ni=1\n2rel∗\ni (q)−1\nlog2(i+1)\n,\n(1)\nwhere p is the number of retrieve chunks by the\nretrieval, reli(q) is the relevance of each chunk\nretrieved and rel∗\ni (q) the ideal relevance of the set of\nretrieved chunks for the query q.For a clear automatization, we will define i to be 3\nif the chunk is in Cs(q), 1 if the chunk is in the right\ndocument and 0 else.The computation of this metric can be done post\n"}
{"name_doc": "RAG_scalabilité.pdf", "doc_id": 0, "chunk_id": 5, "chunk_content": "The computation of this metric can be done post\nruns, as we can try distinct p ≤n and get an estimate\nof a good p for a RAG method without considering\ncontext length of the LLM.1.3\nLimits\nThe first limit is for every methods that can’t recover\nan unlimited number of chunks as threshold technics.However, as explained, the goal is to analyse the\nimpact of features on the retrieve part and not on the\ncreation of the context. Indeed, threshold techniques\nare used to minimize the context lenght and then the\ninference time after the retrieve phase.Another limit is the difficulty to estimate why some\nnon-usefull chunks are retrieved and how close they\nare to the actual retrieve chunks. This information\nstill remains crucial to avoid hallucination from the\n"}
{"name_doc": "RAG_scalabilité.pdf", "doc_id": 0, "chunk_id": 6, "chunk_content": "still remains crucial to avoid hallucination from the\nLLM generating the final answer but won’t be study\nin our first approach.The scalability is limited as the maximal number\nof documents may not be relevant for some more\ndelicate use cases.2\nFeatures to evaluate\nWe enumerate all the differents features to tests\n1. Data preparation\n2. Splitters\n3. Rerankers\n4. Query Reformulations\n5. Embeddings\n6. Metadatas\n2.0.1\nData Preparation\nWe want to estimate the differences between Dp ways\nto prepare the data before splitting it. We have 2\ndata preparation to compare :\n1. PDFs without prepations\n2. Markdowns separation using dockling\nRemark. There are multiple ways to parse the data\nbefore chunking. Some are not relevant on our use\n"}
{"name_doc": "RAG_scalabilité.pdf", "doc_id": 0, "chunk_id": 7, "chunk_content": " Some are not relevant on our use\ncase that we are testing.2.0.2\nSplitters\nWe will try S several splitting technics :\n1. Length splitting\n2. Semantic Splitting\n3. Recursive Splitting\n4. Markdown Splitting (only available for the Mark-\ndown data preparation)\n2.1\nMetadatas\nWe will consider the impact of metadatas. We will\ndetermine some kinf of metadatas relevant with our\nfake use case and see the impact of each of them and\neach combination of them.2\n2.1.1\nRerankers\nWe will try distincts rerankers separatively.Best\nwould be to benchmark all rerankers providers as in\nAutoRAG. unfortunately it won’t be the case in first\ninstance but would be possible to test them as we\nwill keep all chunks retrieved pre-reranking. Indeed,\nthe reranking phase do not require to re run the\n"}
{"name_doc": "RAG_scalabilité.pdf", "doc_id": 0, "chunk_id": 8, "chunk_content": "the reranking phase do not require to re run the\nentire pipeline as it only rerank the chunks already\nordered and retrieved.We denote by R the number of tested rerankers\n2.1.2\nQuery Reformulation\nWe won’t study naive query reformulation as we con-\nsider our prompt as \"good\" prompts that respect best\npractice of prompt engineering. Moreover, we can\nstudy the multi query expansion. This would impose\nto fix a merge strategy of the contexts retrieved from\nall queries or another metric to evaluate if all the use-\nful chunks C(q) are retrieved through all the queries\ngenerated from the user’s one.2.1.3\nEmbeddings\nWe could try all the embeddings, but let us be\nrealistic, we will compare few ones already consider\nas best by Huggingface leader boards."}
{"name_doc": "RAG_scalabilité.pdf", "doc_id": 0, "chunk_id": 9, "chunk_content": "as best by Huggingface leader boards. We will also\ntry the impact of a finetuned embedding.Let us\ndenote by E the number of embeddings.3\nFeatures that won’t affect the\nretrieval study\nWe enumerate all the different features that will not\ninfluence the retrieval but would affect the RAG ac-\ncuracy overall.1. Number of retrieved chunks\n2. Passage augmenter\n3. Passage filter (Thresholds)\n4. Passage Compressor\nAs well we won’t study the server optimizations as\nsharding.4\nNumber\nof\nruns\n(without\nmetadatas)\nWe will measure the impact of features on different\ndataset size by adding more documents not relevant\nto our queries sample. Let us compute all the tests\nwe would :\nT = [(Dp −1) ∗(S −1) ∗E ∗R + S ∗E ∗R] ∗N,\n(2)\ntests to get all information about scalability, where\n"}
{"name_doc": "RAG_scalabilité.pdf", "doc_id": 0, "chunk_id": 10, "chunk_content": "tests to get all information about scalability, where\nN is the number of dataset we are testing.For Dp = 2, S = 4, E = 3, R = 3, N = 1 we have\nalready 63 tests to run. We can run all the tests for\nn = 500 and then study the results as if n was smaller\nsince the retrieved phases are not directly impacted\nby n.5\nCurves to print\nFor every n we will print Pn(q) and nNDCG(q) as a\nconfidence interval diagrams.For every feature, we will plot Pn(q) for different\nvalues of n.For every feature we will compute the minimum\nnmin(p) such that Pn(q) = 1 if possible. Note that\nnmin is calculated before posr-retrived features.To evaluate rerankers we especially compare the\ndifference between nmin(p) and nDCGp(q) before post-\nretrieval and nmin(p) and nDCGp(q) for the post-\n"}
{"name_doc": "RAG_scalabilité.pdf", "doc_id": 0, "chunk_id": 11, "chunk_content": "retrieval and nmin(p) and nDCGp(q) for the post-\nretrieval part.3\n"}
{"name_doc": "3002033669_Scaling Intelligence_ The Exponential Growth of AI's Power Needs.PDF", "doc_id": 1, "chunk_id": 1, "chunk_content": "Extension de fichier inconnu\n"}
{"name_doc": "RAG_scalabilité.pdf", "doc_id": 0, "chunk_id": 1, "chunk_content": "RAG Scalability\nClement Hardy, Benoit Joly\nAugust 25, 2025\nAbstract\nThe following study will compare different techniques\non a specific use case that we have made. The goal is\nto determine the impact of features on a RAG method\nretrieval and post-retrieval part.1\nScalability Metrics\n1.1\nMethodology\nWe suppose to have a set of documents D and a set of\nquestions Q on these documents. For each question\nq ∈Q, we determined the context C(q) needed to\nanswer the question q.For every splitter s, we denote Cs the set of chunks\nthat represent the set of documents D.For every\nquestion q ∈D, we denote the set of chunks Cs(q)\ncontaining the context C(q).For every RAG method that retrieves chunks as part\nof context for the LLM, we calculate the minimum\n"}
{"name_doc": "RAG_scalabilité.pdf", "doc_id": 0, "chunk_id": 2, "chunk_content": "of context for the LLM, we calculate the minimum\nof chunks retrieved such that the context contains\nall the needed information. Let us denote by R the\nretrieve function of the RAG method and MR the\nmaximum of retrieved chunks of R The goal is to\nstudy the restriction of R to the set of question Q.R\n:\nQ\n→\nCs\nq\n7→\nR(q) ,\nwhere R(q) is an ordered list of chunks.The absolute goal of a RAG method is for R to\nsatisfy C(q) ⊂R(q) for every user query q while\nminimizing MR.Hence we will run a serie of tests of, such call, best\npractices modifying our RAG method to determine\nthe impact of every feature on the retrieve function R.1.2\nMetrics\nFirst, it is important to differentiate the retrive\nfunction R of a RAG method to the actual context\n"}
{"name_doc": "RAG_scalabilité.pdf", "doc_id": 0, "chunk_id": 3, "chunk_content": "function R of a RAG method to the actual context\ngiven to the final LLM to generte the user answer.This context is created by a fixed portion of the\nretrieved chunks determine by the RAG method and\nthe context length of the LLM used. Usually, for a\nquery q, only the \"best\" chunks from R(q) is kept,\nHence we can study Rn the retrieval function where\nwe look for the n best chunks (if the method allows\nit) without modifying the studied RAG method and\nthe final context.For every feature, we calculate the proportion Pn(q)\nof usefull context in the retrieved context Rn(q) of\nevery question q for a fixed sample of n :\nPn(q) = #Cs(q) ∩Rn(q)\n#Rn(q)\nWe also calculate the best and the worst place\nof the elements of #Cs(q) ∩Rn(q) denoted Bs(q)\n"}
{"name_doc": "RAG_scalabilité.pdf", "doc_id": 0, "chunk_id": 4, "chunk_content": "of the elements of #Cs(q) ∩Rn(q) denoted Bs(q)\nand Ws(q) to follow it’s evolutions thought different\nfeatures. Note that Bs and Ws doesn’t depend on n.The second metrics we will estimate is the overall\nquality of the order of the retrieval using the nDCGp\nmetric defined as follows :\n1\nnDCGp(q) =\nPp\ni=1\n2reli(q)−1\nlog2(i+1)\nPp\ni=1\n2rel∗\ni (q)−1\nlog2(i+1)\n,\n(1)\nwhere p is the number of retrieve chunks by the\nretrieval, reli(q) is the relevance of each chunk\nretrieved and rel∗\ni (q) the ideal relevance of the set of\nretrieved chunks for the query q.For a clear automatization, we will define i to be 3\nif the chunk is in Cs(q), 1 if the chunk is in the right\ndocument and 0 else.The computation of this metric can be done post\n"}
{"name_doc": "RAG_scalabilité.pdf", "doc_id": 0, "chunk_id": 5, "chunk_content": "The computation of this metric can be done post\nruns, as we can try distinct p ≤n and get an estimate\nof a good p for a RAG method without considering\ncontext length of the LLM.1.3\nLimits\nThe first limit is for every methods that can’t recover\nan unlimited number of chunks as threshold technics.However, as explained, the goal is to analyse the\nimpact of features on the retrieve part and not on the\ncreation of the context. Indeed, threshold techniques\nare used to minimize the context lenght and then the\ninference time after the retrieve phase.Another limit is the difficulty to estimate why some\nnon-usefull chunks are retrieved and how close they\nare to the actual retrieve chunks. This information\nstill remains crucial to avoid hallucination from the\n"}
{"name_doc": "RAG_scalabilité.pdf", "doc_id": 0, "chunk_id": 6, "chunk_content": "still remains crucial to avoid hallucination from the\nLLM generating the final answer but won’t be study\nin our first approach.The scalability is limited as the maximal number\nof documents may not be relevant for some more\ndelicate use cases.2\nFeatures to evaluate\nWe enumerate all the differents features to tests\n1. Data preparation\n2. Splitters\n3. Rerankers\n4. Query Reformulations\n5. Embeddings\n6. Metadatas\n2.0.1\nData Preparation\nWe want to estimate the differences between Dp ways\nto prepare the data before splitting it. We have 2\ndata preparation to compare :\n1. PDFs without prepations\n2. Markdowns separation using dockling\nRemark. There are multiple ways to parse the data\nbefore chunking. Some are not relevant on our use\n"}
{"name_doc": "RAG_scalabilité.pdf", "doc_id": 0, "chunk_id": 7, "chunk_content": " Some are not relevant on our use\ncase that we are testing.2.0.2\nSplitters\nWe will try S several splitting technics :\n1. Length splitting\n2. Semantic Splitting\n3. Recursive Splitting\n4. Markdown Splitting (only available for the Mark-\ndown data preparation)\n2.1\nMetadatas\nWe will consider the impact of metadatas. We will\ndetermine some kinf of metadatas relevant with our\nfake use case and see the impact of each of them and\neach combination of them.2\n2.1.1\nRerankers\nWe will try distincts rerankers separatively.Best\nwould be to benchmark all rerankers providers as in\nAutoRAG. unfortunately it won’t be the case in first\ninstance but would be possible to test them as we\nwill keep all chunks retrieved pre-reranking. Indeed,\nthe reranking phase do not require to re run the\n"}
{"name_doc": "RAG_scalabilité.pdf", "doc_id": 0, "chunk_id": 8, "chunk_content": "the reranking phase do not require to re run the\nentire pipeline as it only rerank the chunks already\nordered and retrieved.We denote by R the number of tested rerankers\n2.1.2\nQuery Reformulation\nWe won’t study naive query reformulation as we con-\nsider our prompt as \"good\" prompts that respect best\npractice of prompt engineering. Moreover, we can\nstudy the multi query expansion. This would impose\nto fix a merge strategy of the contexts retrieved from\nall queries or another metric to evaluate if all the use-\nful chunks C(q) are retrieved through all the queries\ngenerated from the user’s one.2.1.3\nEmbeddings\nWe could try all the embeddings, but let us be\nrealistic, we will compare few ones already consider\nas best by Huggingface leader boards."}
{"name_doc": "RAG_scalabilité.pdf", "doc_id": 0, "chunk_id": 9, "chunk_content": "as best by Huggingface leader boards. We will also\ntry the impact of a finetuned embedding.Let us\ndenote by E the number of embeddings.3\nFeatures that won’t affect the\nretrieval study\nWe enumerate all the different features that will not\ninfluence the retrieval but would affect the RAG ac-\ncuracy overall.1. Number of retrieved chunks\n2. Passage augmenter\n3. Passage filter (Thresholds)\n4. Passage Compressor\nAs well we won’t study the server optimizations as\nsharding.4\nNumber\nof\nruns\n(without\nmetadatas)\nWe will measure the impact of features on different\ndataset size by adding more documents not relevant\nto our queries sample. Let us compute all the tests\nwe would :\nT = [(Dp −1) ∗(S −1) ∗E ∗R + S ∗E ∗R] ∗N,\n(2)\ntests to get all information about scalability, where\n"}
{"name_doc": "RAG_scalabilité.pdf", "doc_id": 0, "chunk_id": 10, "chunk_content": "tests to get all information about scalability, where\nN is the number of dataset we are testing.For Dp = 2, S = 4, E = 3, R = 3, N = 1 we have\nalready 63 tests to run. We can run all the tests for\nn = 500 and then study the results as if n was smaller\nsince the retrieved phases are not directly impacted\nby n.5\nCurves to print\nFor every n we will print Pn(q) and nNDCG(q) as a\nconfidence interval diagrams.For every feature, we will plot Pn(q) for different\nvalues of n.For every feature we will compute the minimum\nnmin(p) such that Pn(q) = 1 if possible. Note that\nnmin is calculated before posr-retrived features.To evaluate rerankers we especially compare the\ndifference between nmin(p) and nDCGp(q) before post-\nretrieval and nmin(p) and nDCGp(q) for the post-\n"}
{"name_doc": "RAG_scalabilité.pdf", "doc_id": 0, "chunk_id": 11, "chunk_content": "retrieval and nmin(p) and nDCGp(q) for the post-\nretrieval part.3\n"}
{"name_doc": "3002033669_Scaling Intelligence_ The Exponential Growth of AI's Power Needs.PDF", "doc_id": 1, "chunk_id": 1, "chunk_content": "Extension de fichier inconnu\n"}
{"name_doc": "RAG_scalabilité.pdf", "doc_id": 0, "chunk_id": 1, "chunk_content": "RAG Scalability\nClement Hardy, Benoit Joly\nAugust 25, 2025\nAbstract\nThe following study will compare different techniques\non a specific use case that we have made. The goal is\nto determine the impact of features on a RAG method\nretrieval and post-retrieval part.1\nScalability Metrics\n1.1\nMethodology\nWe suppose to have a set of documents D and a set of\nquestions Q on these documents. For each question\nq ∈Q, we determined the context C(q) needed to\nanswer the question q.For every splitter s, we denote Cs the set of chunks\nthat represent the set of documents D.For every\nquestion q ∈D, we denote the set of chunks Cs(q)\ncontaining the context C(q).For every RAG method that retrieves chunks as part\nof context for the LLM, we calculate the minimum\n"}
{"name_doc": "RAG_scalabilité.pdf", "doc_id": 0, "chunk_id": 2, "chunk_content": "of context for the LLM, we calculate the minimum\nof chunks retrieved such that the context contains\nall the needed information. Let us denote by R the\nretrieve function of the RAG method and MR the\nmaximum of retrieved chunks of R The goal is to\nstudy the restriction of R to the set of question Q.R\n:\nQ\n→\nCs\nq\n7→\nR(q) ,\nwhere R(q) is an ordered list of chunks.The absolute goal of a RAG method is for R to\nsatisfy C(q) ⊂R(q) for every user query q while\nminimizing MR.Hence we will run a serie of tests of, such call, best\npractices modifying our RAG method to determine\nthe impact of every feature on the retrieve function R.1.2\nMetrics\nFirst, it is important to differentiate the retrive\nfunction R of a RAG method to the actual context\n"}
{"name_doc": "RAG_scalabilité.pdf", "doc_id": 0, "chunk_id": 3, "chunk_content": "function R of a RAG method to the actual context\ngiven to the final LLM to generte the user answer.This context is created by a fixed portion of the\nretrieved chunks determine by the RAG method and\nthe context length of the LLM used. Usually, for a\nquery q, only the \"best\" chunks from R(q) is kept,\nHence we can study Rn the retrieval function where\nwe look for the n best chunks (if the method allows\nit) without modifying the studied RAG method and\nthe final context.For every feature, we calculate the proportion Pn(q)\nof usefull context in the retrieved context Rn(q) of\nevery question q for a fixed sample of n :\nPn(q) = #Cs(q) ∩Rn(q)\n#Rn(q)\nWe also calculate the best and the worst place\nof the elements of #Cs(q) ∩Rn(q) denoted Bs(q)\n"}
{"name_doc": "RAG_scalabilité.pdf", "doc_id": 0, "chunk_id": 4, "chunk_content": "of the elements of #Cs(q) ∩Rn(q) denoted Bs(q)\nand Ws(q) to follow it’s evolutions thought different\nfeatures. Note that Bs and Ws doesn’t depend on n.The second metrics we will estimate is the overall\nquality of the order of the retrieval using the nDCGp\nmetric defined as follows :\n1\nnDCGp(q) =\nPp\ni=1\n2reli(q)−1\nlog2(i+1)\nPp\ni=1\n2rel∗\ni (q)−1\nlog2(i+1)\n,\n(1)\nwhere p is the number of retrieve chunks by the\nretrieval, reli(q) is the relevance of each chunk\nretrieved and rel∗\ni (q) the ideal relevance of the set of\nretrieved chunks for the query q.For a clear automatization, we will define i to be 3\nif the chunk is in Cs(q), 1 if the chunk is in the right\ndocument and 0 else.The computation of this metric can be done post\n"}
{"name_doc": "RAG_scalabilité.pdf", "doc_id": 0, "chunk_id": 5, "chunk_content": "The computation of this metric can be done post\nruns, as we can try distinct p ≤n and get an estimate\nof a good p for a RAG method without considering\ncontext length of the LLM.1.3\nLimits\nThe first limit is for every methods that can’t recover\nan unlimited number of chunks as threshold technics.However, as explained, the goal is to analyse the\nimpact of features on the retrieve part and not on the\ncreation of the context. Indeed, threshold techniques\nare used to minimize the context lenght and then the\ninference time after the retrieve phase.Another limit is the difficulty to estimate why some\nnon-usefull chunks are retrieved and how close they\nare to the actual retrieve chunks. This information\nstill remains crucial to avoid hallucination from the\n"}
{"name_doc": "RAG_scalabilité.pdf", "doc_id": 0, "chunk_id": 6, "chunk_content": "still remains crucial to avoid hallucination from the\nLLM generating the final answer but won’t be study\nin our first approach.The scalability is limited as the maximal number\nof documents may not be relevant for some more\ndelicate use cases.2\nFeatures to evaluate\nWe enumerate all the differents features to tests\n1. Data preparation\n2. Splitters\n3. Rerankers\n4. Query Reformulations\n5. Embeddings\n6. Metadatas\n2.0.1\nData Preparation\nWe want to estimate the differences between Dp ways\nto prepare the data before splitting it. We have 2\ndata preparation to compare :\n1. PDFs without prepations\n2. Markdowns separation using dockling\nRemark. There are multiple ways to parse the data\nbefore chunking. Some are not relevant on our use\n"}
{"name_doc": "RAG_scalabilité.pdf", "doc_id": 0, "chunk_id": 7, "chunk_content": " Some are not relevant on our use\ncase that we are testing.2.0.2\nSplitters\nWe will try S several splitting technics :\n1. Length splitting\n2. Semantic Splitting\n3. Recursive Splitting\n4. Markdown Splitting (only available for the Mark-\ndown data preparation)\n2.1\nMetadatas\nWe will consider the impact of metadatas. We will\ndetermine some kinf of metadatas relevant with our\nfake use case and see the impact of each of them and\neach combination of them.2\n2.1.1\nRerankers\nWe will try distincts rerankers separatively.Best\nwould be to benchmark all rerankers providers as in\nAutoRAG. unfortunately it won’t be the case in first\ninstance but would be possible to test them as we\nwill keep all chunks retrieved pre-reranking. Indeed,\nthe reranking phase do not require to re run the\n"}
{"name_doc": "RAG_scalabilité.pdf", "doc_id": 0, "chunk_id": 8, "chunk_content": "the reranking phase do not require to re run the\nentire pipeline as it only rerank the chunks already\nordered and retrieved.We denote by R the number of tested rerankers\n2.1.2\nQuery Reformulation\nWe won’t study naive query reformulation as we con-\nsider our prompt as \"good\" prompts that respect best\npractice of prompt engineering. Moreover, we can\nstudy the multi query expansion. This would impose\nto fix a merge strategy of the contexts retrieved from\nall queries or another metric to evaluate if all the use-\nful chunks C(q) are retrieved through all the queries\ngenerated from the user’s one.2.1.3\nEmbeddings\nWe could try all the embeddings, but let us be\nrealistic, we will compare few ones already consider\nas best by Huggingface leader boards."}
{"name_doc": "RAG_scalabilité.pdf", "doc_id": 0, "chunk_id": 9, "chunk_content": "as best by Huggingface leader boards. We will also\ntry the impact of a finetuned embedding.Let us\ndenote by E the number of embeddings.3\nFeatures that won’t affect the\nretrieval study\nWe enumerate all the different features that will not\ninfluence the retrieval but would affect the RAG ac-\ncuracy overall.1. Number of retrieved chunks\n2. Passage augmenter\n3. Passage filter (Thresholds)\n4. Passage Compressor\nAs well we won’t study the server optimizations as\nsharding.4\nNumber\nof\nruns\n(without\nmetadatas)\nWe will measure the impact of features on different\ndataset size by adding more documents not relevant\nto our queries sample. Let us compute all the tests\nwe would :\nT = [(Dp −1) ∗(S −1) ∗E ∗R + S ∗E ∗R] ∗N,\n(2)\ntests to get all information about scalability, where\n"}
{"name_doc": "RAG_scalabilité.pdf", "doc_id": 0, "chunk_id": 10, "chunk_content": "tests to get all information about scalability, where\nN is the number of dataset we are testing.For Dp = 2, S = 4, E = 3, R = 3, N = 1 we have\nalready 63 tests to run. We can run all the tests for\nn = 500 and then study the results as if n was smaller\nsince the retrieved phases are not directly impacted\nby n.5\nCurves to print\nFor every n we will print Pn(q) and nNDCG(q) as a\nconfidence interval diagrams.For every feature, we will plot Pn(q) for different\nvalues of n.For every feature we will compute the minimum\nnmin(p) such that Pn(q) = 1 if possible. Note that\nnmin is calculated before posr-retrived features.To evaluate rerankers we especially compare the\ndifference between nmin(p) and nDCGp(q) before post-\nretrieval and nmin(p) and nDCGp(q) for the post-\n"}
{"name_doc": "RAG_scalabilité.pdf", "doc_id": 0, "chunk_id": 11, "chunk_content": "retrieval and nmin(p) and nDCGp(q) for the post-\nretrieval part.3\n"}
{"name_doc": "RAG_scalabilité.pdf", "doc_id": 0, "chunk_id": 1, "chunk_content": "RAG Scalability\nClement Hardy, Benoit Joly\nAugust 25, 2025\nAbstract\nThe following study will compare different techniques\non a specific use case that we have made. The goal is\nto determine the impact of features on a RAG method\nretrieval and post-retrieval part.1\nScalability Metrics\n1.1\nMethodology\nWe suppose to have a set of documents D and a set of\nquestions Q on these documents. For each question\nq ∈Q, we determined the context C(q) needed to\nanswer the question q.For every splitter s, we denote Cs the set of chunks\nthat represent the set of documents D.For every\nquestion q ∈D, we denote the set of chunks Cs(q)\ncontaining the context C(q).For every RAG method that retrieves chunks as part\nof context for the LLM, we calculate the minimum\n"}
{"name_doc": "RAG_scalabilité.pdf", "doc_id": 0, "chunk_id": 2, "chunk_content": "of context for the LLM, we calculate the minimum\nof chunks retrieved such that the context contains\nall the needed information. Let us denote by R the\nretrieve function of the RAG method and MR the\nmaximum of retrieved chunks of R The goal is to\nstudy the restriction of R to the set of question Q.R\n:\nQ\n→\nCs\nq\n7→\nR(q) ,\nwhere R(q) is an ordered list of chunks.The absolute goal of a RAG method is for R to\nsatisfy C(q) ⊂R(q) for every user query q while\nminimizing MR.Hence we will run a serie of tests of, such call, best\npractices modifying our RAG method to determine\nthe impact of every feature on the retrieve function R.1.2\nMetrics\nFirst, it is important to differentiate the retrive\nfunction R of a RAG method to the actual context\n"}
{"name_doc": "RAG_scalabilité.pdf", "doc_id": 0, "chunk_id": 3, "chunk_content": "function R of a RAG method to the actual context\ngiven to the final LLM to generte the user answer.This context is created by a fixed portion of the\nretrieved chunks determine by the RAG method and\nthe context length of the LLM used. Usually, for a\nquery q, only the \"best\" chunks from R(q) is kept,\nHence we can study Rn the retrieval function where\nwe look for the n best chunks (if the method allows\nit) without modifying the studied RAG method and\nthe final context.For every feature, we calculate the proportion Pn(q)\nof usefull context in the retrieved context Rn(q) of\nevery question q for a fixed sample of n :\nPn(q) = #Cs(q) ∩Rn(q)\n#Rn(q)\nWe also calculate the best and the worst place\nof the elements of #Cs(q) ∩Rn(q) denoted Bs(q)\n"}
{"name_doc": "RAG_scalabilité.pdf", "doc_id": 0, "chunk_id": 4, "chunk_content": "of the elements of #Cs(q) ∩Rn(q) denoted Bs(q)\nand Ws(q) to follow it’s evolutions thought different\nfeatures. Note that Bs and Ws doesn’t depend on n.The second metrics we will estimate is the overall\nquality of the order of the retrieval using the nDCGp\nmetric defined as follows :\n1\nnDCGp(q) =\nPp\ni=1\n2reli(q)−1\nlog2(i+1)\nPp\ni=1\n2rel∗\ni (q)−1\nlog2(i+1)\n,\n(1)\nwhere p is the number of retrieve chunks by the\nretrieval, reli(q) is the relevance of each chunk\nretrieved and rel∗\ni (q) the ideal relevance of the set of\nretrieved chunks for the query q.For a clear automatization, we will define i to be 3\nif the chunk is in Cs(q), 1 if the chunk is in the right\ndocument and 0 else.The computation of this metric can be done post\n"}
{"name_doc": "RAG_scalabilité.pdf", "doc_id": 0, "chunk_id": 5, "chunk_content": "The computation of this metric can be done post\nruns, as we can try distinct p ≤n and get an estimate\nof a good p for a RAG method without considering\ncontext length of the LLM.1.3\nLimits\nThe first limit is for every methods that can’t recover\nan unlimited number of chunks as threshold technics.However, as explained, the goal is to analyse the\nimpact of features on the retrieve part and not on the\ncreation of the context. Indeed, threshold techniques\nare used to minimize the context lenght and then the\ninference time after the retrieve phase.Another limit is the difficulty to estimate why some\nnon-usefull chunks are retrieved and how close they\nare to the actual retrieve chunks. This information\nstill remains crucial to avoid hallucination from the\n"}
{"name_doc": "RAG_scalabilité.pdf", "doc_id": 0, "chunk_id": 6, "chunk_content": "still remains crucial to avoid hallucination from the\nLLM generating the final answer but won’t be study\nin our first approach.The scalability is limited as the maximal number\nof documents may not be relevant for some more\ndelicate use cases.2\nFeatures to evaluate\nWe enumerate all the differents features to tests\n1. Data preparation\n2. Splitters\n3. Rerankers\n4. Query Reformulations\n5. Embeddings\n6. Metadatas\n2.0.1\nData Preparation\nWe want to estimate the differences between Dp ways\nto prepare the data before splitting it. We have 2\ndata preparation to compare :\n1. PDFs without prepations\n2. Markdowns separation using dockling\nRemark. There are multiple ways to parse the data\nbefore chunking. Some are not relevant on our use\n"}
{"name_doc": "RAG_scalabilité.pdf", "doc_id": 0, "chunk_id": 7, "chunk_content": " Some are not relevant on our use\ncase that we are testing.2.0.2\nSplitters\nWe will try S several splitting technics :\n1. Length splitting\n2. Semantic Splitting\n3. Recursive Splitting\n4. Markdown Splitting (only available for the Mark-\ndown data preparation)\n2.1\nMetadatas\nWe will consider the impact of metadatas. We will\ndetermine some kinf of metadatas relevant with our\nfake use case and see the impact of each of them and\neach combination of them.2\n2.1.1\nRerankers\nWe will try distincts rerankers separatively.Best\nwould be to benchmark all rerankers providers as in\nAutoRAG. unfortunately it won’t be the case in first\ninstance but would be possible to test them as we\nwill keep all chunks retrieved pre-reranking. Indeed,\nthe reranking phase do not require to re run the\n"}
{"name_doc": "RAG_scalabilité.pdf", "doc_id": 0, "chunk_id": 8, "chunk_content": "the reranking phase do not require to re run the\nentire pipeline as it only rerank the chunks already\nordered and retrieved.We denote by R the number of tested rerankers\n2.1.2\nQuery Reformulation\nWe won’t study naive query reformulation as we con-\nsider our prompt as \"good\" prompts that respect best\npractice of prompt engineering. Moreover, we can\nstudy the multi query expansion. This would impose\nto fix a merge strategy of the contexts retrieved from\nall queries or another metric to evaluate if all the use-\nful chunks C(q) are retrieved through all the queries\ngenerated from the user’s one.2.1.3\nEmbeddings\nWe could try all the embeddings, but let us be\nrealistic, we will compare few ones already consider\nas best by Huggingface leader boards."}
{"name_doc": "RAG_scalabilité.pdf", "doc_id": 0, "chunk_id": 9, "chunk_content": "as best by Huggingface leader boards. We will also\ntry the impact of a finetuned embedding.Let us\ndenote by E the number of embeddings.3\nFeatures that won’t affect the\nretrieval study\nWe enumerate all the different features that will not\ninfluence the retrieval but would affect the RAG ac-\ncuracy overall.1. Number of retrieved chunks\n2. Passage augmenter\n3. Passage filter (Thresholds)\n4. Passage Compressor\nAs well we won’t study the server optimizations as\nsharding.4\nNumber\nof\nruns\n(without\nmetadatas)\nWe will measure the impact of features on different\ndataset size by adding more documents not relevant\nto our queries sample. Let us compute all the tests\nwe would :\nT = [(Dp −1) ∗(S −1) ∗E ∗R + S ∗E ∗R] ∗N,\n(2)\ntests to get all information about scalability, where\n"}
{"name_doc": "RAG_scalabilité.pdf", "doc_id": 0, "chunk_id": 10, "chunk_content": "tests to get all information about scalability, where\nN is the number of dataset we are testing.For Dp = 2, S = 4, E = 3, R = 3, N = 1 we have\nalready 63 tests to run. We can run all the tests for\nn = 500 and then study the results as if n was smaller\nsince the retrieved phases are not directly impacted\nby n.5\nCurves to print\nFor every n we will print Pn(q) and nNDCG(q) as a\nconfidence interval diagrams.For every feature, we will plot Pn(q) for different\nvalues of n.For every feature we will compute the minimum\nnmin(p) such that Pn(q) = 1 if possible. Note that\nnmin is calculated before posr-retrived features.To evaluate rerankers we especially compare the\ndifference between nmin(p) and nDCGp(q) before post-\nretrieval and nmin(p) and nDCGp(q) for the post-\n"}
{"name_doc": "RAG_scalabilité.pdf", "doc_id": 0, "chunk_id": 11, "chunk_content": "retrieval and nmin(p) and nDCGp(q) for the post-\nretrieval part.3\n"}
{"name_doc": "RAG_scalabilité.pdf", "doc_id": 0, "chunk_id": 1, "chunk_content": "RAG Scalability\nClement Hardy, Benoit Joly\nAugust 25, 2025\nAbstract\nThe following study will compare different techniques\non a specific use case that we have made. The goal is\nto determine the impact of features on a RAG method\nretrieval and post-retrieval part.1\nScalability Metrics\n1.1\nMethodology\nWe suppose to have a set of documents D and a set of\nquestions Q on these documents. For each question\nq ∈Q, we determined the context C(q) needed to\nanswer the question q.For every splitter s, we denote Cs the set of chunks\nthat represent the set of documents D.For every\nquestion q ∈D, we denote the set of chunks Cs(q)\ncontaining the context C(q).For every RAG method that retrieves chunks as part\nof context for the LLM, we calculate the minimum\n"}
{"name_doc": "RAG_scalabilité.pdf", "doc_id": 0, "chunk_id": 2, "chunk_content": "of context for the LLM, we calculate the minimum\nof chunks retrieved such that the context contains\nall the needed information. Let us denote by R the\nretrieve function of the RAG method and MR the\nmaximum of retrieved chunks of R The goal is to\nstudy the restriction of R to the set of question Q.R\n:\nQ\n→\nCs\nq\n7→\nR(q) ,\nwhere R(q) is an ordered list of chunks.The absolute goal of a RAG method is for R to\nsatisfy C(q) ⊂R(q) for every user query q while\nminimizing MR.Hence we will run a serie of tests of, such call, best\npractices modifying our RAG method to determine\nthe impact of every feature on the retrieve function R.1.2\nMetrics\nFirst, it is important to differentiate the retrive\nfunction R of a RAG method to the actual context\n"}
{"name_doc": "RAG_scalabilité.pdf", "doc_id": 0, "chunk_id": 3, "chunk_content": "function R of a RAG method to the actual context\ngiven to the final LLM to generte the user answer.This context is created by a fixed portion of the\nretrieved chunks determine by the RAG method and\nthe context length of the LLM used. Usually, for a\nquery q, only the \"best\" chunks from R(q) is kept,\nHence we can study Rn the retrieval function where\nwe look for the n best chunks (if the method allows\nit) without modifying the studied RAG method and\nthe final context.For every feature, we calculate the proportion Pn(q)\nof usefull context in the retrieved context Rn(q) of\nevery question q for a fixed sample of n :\nPn(q) = #Cs(q) ∩Rn(q)\n#Rn(q)\nWe also calculate the best and the worst place\nof the elements of #Cs(q) ∩Rn(q) denoted Bs(q)\n"}
{"name_doc": "RAG_scalabilité.pdf", "doc_id": 0, "chunk_id": 4, "chunk_content": "of the elements of #Cs(q) ∩Rn(q) denoted Bs(q)\nand Ws(q) to follow it’s evolutions thought different\nfeatures. Note that Bs and Ws doesn’t depend on n.The second metrics we will estimate is the overall\nquality of the order of the retrieval using the nDCGp\nmetric defined as follows :\n1\nnDCGp(q) =\nPp\ni=1\n2reli(q)−1\nlog2(i+1)\nPp\ni=1\n2rel∗\ni (q)−1\nlog2(i+1)\n,\n(1)\nwhere p is the number of retrieve chunks by the\nretrieval, reli(q) is the relevance of each chunk\nretrieved and rel∗\ni (q) the ideal relevance of the set of\nretrieved chunks for the query q.For a clear automatization, we will define i to be 3\nif the chunk is in Cs(q), 1 if the chunk is in the right\ndocument and 0 else.The computation of this metric can be done post\n"}
{"name_doc": "RAG_scalabilité.pdf", "doc_id": 0, "chunk_id": 5, "chunk_content": "The computation of this metric can be done post\nruns, as we can try distinct p ≤n and get an estimate\nof a good p for a RAG method without considering\ncontext length of the LLM.1.3\nLimits\nThe first limit is for every methods that can’t recover\nan unlimited number of chunks as threshold technics.However, as explained, the goal is to analyse the\nimpact of features on the retrieve part and not on the\ncreation of the context. Indeed, threshold techniques\nare used to minimize the context lenght and then the\ninference time after the retrieve phase.Another limit is the difficulty to estimate why some\nnon-usefull chunks are retrieved and how close they\nare to the actual retrieve chunks. This information\nstill remains crucial to avoid hallucination from the\n"}
{"name_doc": "RAG_scalabilité.pdf", "doc_id": 0, "chunk_id": 6, "chunk_content": "still remains crucial to avoid hallucination from the\nLLM generating the final answer but won’t be study\nin our first approach.The scalability is limited as the maximal number\nof documents may not be relevant for some more\ndelicate use cases.2\nFeatures to evaluate\nWe enumerate all the differents features to tests\n1. Data preparation\n2. Splitters\n3. Rerankers\n4. Query Reformulations\n5. Embeddings\n6. Metadatas\n2.0.1\nData Preparation\nWe want to estimate the differences between Dp ways\nto prepare the data before splitting it. We have 2\ndata preparation to compare :\n1. PDFs without prepations\n2. Markdowns separation using dockling\nRemark. There are multiple ways to parse the data\nbefore chunking. Some are not relevant on our use\n"}
{"name_doc": "RAG_scalabilité.pdf", "doc_id": 0, "chunk_id": 7, "chunk_content": " Some are not relevant on our use\ncase that we are testing.2.0.2\nSplitters\nWe will try S several splitting technics :\n1. Length splitting\n2. Semantic Splitting\n3. Recursive Splitting\n4. Markdown Splitting (only available for the Mark-\ndown data preparation)\n2.1\nMetadatas\nWe will consider the impact of metadatas. We will\ndetermine some kinf of metadatas relevant with our\nfake use case and see the impact of each of them and\neach combination of them.2\n2.1.1\nRerankers\nWe will try distincts rerankers separatively.Best\nwould be to benchmark all rerankers providers as in\nAutoRAG. unfortunately it won’t be the case in first\ninstance but would be possible to test them as we\nwill keep all chunks retrieved pre-reranking. Indeed,\nthe reranking phase do not require to re run the\n"}
{"name_doc": "RAG_scalabilité.pdf", "doc_id": 0, "chunk_id": 8, "chunk_content": "the reranking phase do not require to re run the\nentire pipeline as it only rerank the chunks already\nordered and retrieved.We denote by R the number of tested rerankers\n2.1.2\nQuery Reformulation\nWe won’t study naive query reformulation as we con-\nsider our prompt as \"good\" prompts that respect best\npractice of prompt engineering. Moreover, we can\nstudy the multi query expansion. This would impose\nto fix a merge strategy of the contexts retrieved from\nall queries or another metric to evaluate if all the use-\nful chunks C(q) are retrieved through all the queries\ngenerated from the user’s one.2.1.3\nEmbeddings\nWe could try all the embeddings, but let us be\nrealistic, we will compare few ones already consider\nas best by Huggingface leader boards."}
{"name_doc": "RAG_scalabilité.pdf", "doc_id": 0, "chunk_id": 9, "chunk_content": "as best by Huggingface leader boards. We will also\ntry the impact of a finetuned embedding.Let us\ndenote by E the number of embeddings.3\nFeatures that won’t affect the\nretrieval study\nWe enumerate all the different features that will not\ninfluence the retrieval but would affect the RAG ac-\ncuracy overall.1. Number of retrieved chunks\n2. Passage augmenter\n3. Passage filter (Thresholds)\n4. Passage Compressor\nAs well we won’t study the server optimizations as\nsharding.4\nNumber\nof\nruns\n(without\nmetadatas)\nWe will measure the impact of features on different\ndataset size by adding more documents not relevant\nto our queries sample. Let us compute all the tests\nwe would :\nT = [(Dp −1) ∗(S −1) ∗E ∗R + S ∗E ∗R] ∗N,\n(2)\ntests to get all information about scalability, where\n"}
{"name_doc": "RAG_scalabilité.pdf", "doc_id": 0, "chunk_id": 10, "chunk_content": "tests to get all information about scalability, where\nN is the number of dataset we are testing.For Dp = 2, S = 4, E = 3, R = 3, N = 1 we have\nalready 63 tests to run. We can run all the tests for\nn = 500 and then study the results as if n was smaller\nsince the retrieved phases are not directly impacted\nby n.5\nCurves to print\nFor every n we will print Pn(q) and nNDCG(q) as a\nconfidence interval diagrams.For every feature, we will plot Pn(q) for different\nvalues of n.For every feature we will compute the minimum\nnmin(p) such that Pn(q) = 1 if possible. Note that\nnmin is calculated before posr-retrived features.To evaluate rerankers we especially compare the\ndifference between nmin(p) and nDCGp(q) before post-\nretrieval and nmin(p) and nDCGp(q) for the post-\n"}
{"name_doc": "RAG_scalabilité.pdf", "doc_id": 0, "chunk_id": 11, "chunk_content": "retrieval and nmin(p) and nDCGp(q) for the post-\nretrieval part.3\n"}
{"name_doc": "3002033669_Scaling Intelligence_ The Exponential Growth of AI's Power Needs.PDF", "doc_id": 1, "chunk_id": 1, "chunk_content": "Extension de fichier inconnu\n"}
{"name_doc": "RAG_scalabilité.pdf", "doc_id": 0, "chunk_id": 1, "chunk_content": "RAG Scalability\nClement Hardy, Benoit Joly\nAugust 25, 2025\nAbstract\nThe following study will compare different techniques\non a specific use case that we have made. The goal is\nto determine the impact of features on a RAG method\nretrieval and post-retrieval part.1\nScalability Metrics\n1.1\nMethodology\nWe suppose to have a set of documents D and a set of\nquestions Q on these documents. For each question\nq ∈Q, we determined the context C(q) needed to\nanswer the question q.For every splitter s, we denote Cs the set of chunks\nthat represent the set of documents D.For every\nquestion q ∈D, we denote the set of chunks Cs(q)\ncontaining the context C(q).For every RAG method that retrieves chunks as part\nof context for the LLM, we calculate the minimum\n"}
{"name_doc": "RAG_scalabilité.pdf", "doc_id": 0, "chunk_id": 2, "chunk_content": "of context for the LLM, we calculate the minimum\nof chunks retrieved such that the context contains\nall the needed information. Let us denote by R the\nretrieve function of the RAG method and MR the\nmaximum of retrieved chunks of R The goal is to\nstudy the restriction of R to the set of question Q.R\n:\nQ\n→\nCs\nq\n7→\nR(q) ,\nwhere R(q) is an ordered list of chunks.The absolute goal of a RAG method is for R to\nsatisfy C(q) ⊂R(q) for every user query q while\nminimizing MR.Hence we will run a serie of tests of, such call, best\npractices modifying our RAG method to determine\nthe impact of every feature on the retrieve function R.1.2\nMetrics\nFirst, it is important to differentiate the retrive\nfunction R of a RAG method to the actual context\n"}
{"name_doc": "RAG_scalabilité.pdf", "doc_id": 0, "chunk_id": 3, "chunk_content": "function R of a RAG method to the actual context\ngiven to the final LLM to generte the user answer.This context is created by a fixed portion of the\nretrieved chunks determine by the RAG method and\nthe context length of the LLM used. Usually, for a\nquery q, only the \"best\" chunks from R(q) is kept,\nHence we can study Rn the retrieval function where\nwe look for the n best chunks (if the method allows\nit) without modifying the studied RAG method and\nthe final context.For every feature, we calculate the proportion Pn(q)\nof usefull context in the retrieved context Rn(q) of\nevery question q for a fixed sample of n :\nPn(q) = #Cs(q) ∩Rn(q)\n#Rn(q)\nWe also calculate the best and the worst place\nof the elements of #Cs(q) ∩Rn(q) denoted Bs(q)\n"}
{"name_doc": "RAG_scalabilité.pdf", "doc_id": 0, "chunk_id": 4, "chunk_content": "of the elements of #Cs(q) ∩Rn(q) denoted Bs(q)\nand Ws(q) to follow it’s evolutions thought different\nfeatures. Note that Bs and Ws doesn’t depend on n.The second metrics we will estimate is the overall\nquality of the order of the retrieval using the nDCGp\nmetric defined as follows :\n1\nnDCGp(q) =\nPp\ni=1\n2reli(q)−1\nlog2(i+1)\nPp\ni=1\n2rel∗\ni (q)−1\nlog2(i+1)\n,\n(1)\nwhere p is the number of retrieve chunks by the\nretrieval, reli(q) is the relevance of each chunk\nretrieved and rel∗\ni (q) the ideal relevance of the set of\nretrieved chunks for the query q.For a clear automatization, we will define i to be 3\nif the chunk is in Cs(q), 1 if the chunk is in the right\ndocument and 0 else.The computation of this metric can be done post\n"}
{"name_doc": "RAG_scalabilité.pdf", "doc_id": 0, "chunk_id": 5, "chunk_content": "The computation of this metric can be done post\nruns, as we can try distinct p ≤n and get an estimate\nof a good p for a RAG method without considering\ncontext length of the LLM.1.3\nLimits\nThe first limit is for every methods that can’t recover\nan unlimited number of chunks as threshold technics.However, as explained, the goal is to analyse the\nimpact of features on the retrieve part and not on the\ncreation of the context. Indeed, threshold techniques\nare used to minimize the context lenght and then the\ninference time after the retrieve phase.Another limit is the difficulty to estimate why some\nnon-usefull chunks are retrieved and how close they\nare to the actual retrieve chunks. This information\nstill remains crucial to avoid hallucination from the\n"}
{"name_doc": "RAG_scalabilité.pdf", "doc_id": 0, "chunk_id": 6, "chunk_content": "still remains crucial to avoid hallucination from the\nLLM generating the final answer but won’t be study\nin our first approach.The scalability is limited as the maximal number\nof documents may not be relevant for some more\ndelicate use cases.2\nFeatures to evaluate\nWe enumerate all the differents features to tests\n1. Data preparation\n2. Splitters\n3. Rerankers\n4. Query Reformulations\n5. Embeddings\n6. Metadatas\n2.0.1\nData Preparation\nWe want to estimate the differences between Dp ways\nto prepare the data before splitting it. We have 2\ndata preparation to compare :\n1. PDFs without prepations\n2. Markdowns separation using dockling\nRemark. There are multiple ways to parse the data\nbefore chunking. Some are not relevant on our use\n"}
{"name_doc": "RAG_scalabilité.pdf", "doc_id": 0, "chunk_id": 7, "chunk_content": " Some are not relevant on our use\ncase that we are testing.2.0.2\nSplitters\nWe will try S several splitting technics :\n1. Length splitting\n2. Semantic Splitting\n3. Recursive Splitting\n4. Markdown Splitting (only available for the Mark-\ndown data preparation)\n2.1\nMetadatas\nWe will consider the impact of metadatas. We will\ndetermine some kinf of metadatas relevant with our\nfake use case and see the impact of each of them and\neach combination of them.2\n2.1.1\nRerankers\nWe will try distincts rerankers separatively.Best\nwould be to benchmark all rerankers providers as in\nAutoRAG. unfortunately it won’t be the case in first\ninstance but would be possible to test them as we\nwill keep all chunks retrieved pre-reranking. Indeed,\nthe reranking phase do not require to re run the\n"}
{"name_doc": "RAG_scalabilité.pdf", "doc_id": 0, "chunk_id": 8, "chunk_content": "the reranking phase do not require to re run the\nentire pipeline as it only rerank the chunks already\nordered and retrieved.We denote by R the number of tested rerankers\n2.1.2\nQuery Reformulation\nWe won’t study naive query reformulation as we con-\nsider our prompt as \"good\" prompts that respect best\npractice of prompt engineering. Moreover, we can\nstudy the multi query expansion. This would impose\nto fix a merge strategy of the contexts retrieved from\nall queries or another metric to evaluate if all the use-\nful chunks C(q) are retrieved through all the queries\ngenerated from the user’s one.2.1.3\nEmbeddings\nWe could try all the embeddings, but let us be\nrealistic, we will compare few ones already consider\nas best by Huggingface leader boards."}
{"name_doc": "RAG_scalabilité.pdf", "doc_id": 0, "chunk_id": 9, "chunk_content": "as best by Huggingface leader boards. We will also\ntry the impact of a finetuned embedding.Let us\ndenote by E the number of embeddings.3\nFeatures that won’t affect the\nretrieval study\nWe enumerate all the different features that will not\ninfluence the retrieval but would affect the RAG ac-\ncuracy overall.1. Number of retrieved chunks\n2. Passage augmenter\n3. Passage filter (Thresholds)\n4. Passage Compressor\nAs well we won’t study the server optimizations as\nsharding.4\nNumber\nof\nruns\n(without\nmetadatas)\nWe will measure the impact of features on different\ndataset size by adding more documents not relevant\nto our queries sample. Let us compute all the tests\nwe would :\nT = [(Dp −1) ∗(S −1) ∗E ∗R + S ∗E ∗R] ∗N,\n(2)\ntests to get all information about scalability, where\n"}
{"name_doc": "RAG_scalabilité.pdf", "doc_id": 0, "chunk_id": 10, "chunk_content": "tests to get all information about scalability, where\nN is the number of dataset we are testing.For Dp = 2, S = 4, E = 3, R = 3, N = 1 we have\nalready 63 tests to run. We can run all the tests for\nn = 500 and then study the results as if n was smaller\nsince the retrieved phases are not directly impacted\nby n.5\nCurves to print\nFor every n we will print Pn(q) and nNDCG(q) as a\nconfidence interval diagrams.For every feature, we will plot Pn(q) for different\nvalues of n.For every feature we will compute the minimum\nnmin(p) such that Pn(q) = 1 if possible. Note that\nnmin is calculated before posr-retrived features.To evaluate rerankers we especially compare the\ndifference between nmin(p) and nDCGp(q) before post-\nretrieval and nmin(p) and nDCGp(q) for the post-\n"}
{"name_doc": "RAG_scalabilité.pdf", "doc_id": 0, "chunk_id": 11, "chunk_content": "retrieval and nmin(p) and nDCGp(q) for the post-\nretrieval part.3\n"}
{"name_doc": "3002033669_Scaling Intelligence_ The Exponential Growth of AI's Power Needs.PDF", "doc_id": 1, "chunk_id": 1, "chunk_content": "Extension de fichier inconnu\n"}
{"name_doc": "RAG_scalabilité.pdf", "doc_id": 0, "chunk_id": 1, "chunk_content": "RAG Scalability\nClement Hardy, Benoit Joly\nAugust 25, 2025\nAbstract\nThe following study will compare different techniques\non a specific use case that we have made. The goal is\nto determine the impact of features on a RAG method\nretrieval and post-retrieval part.1\nScalability Metrics\n1.1\nMethodology\nWe suppose to have a set of documents D and a set of\nquestions Q on these documents. For each question\nq ∈Q, we determined the context C(q) needed to\nanswer the question q.For every splitter s, we denote Cs the set of chunks\nthat represent the set of documents D.For every\nquestion q ∈D, we denote the set of chunks Cs(q)\ncontaining the context C(q).For every RAG method that retrieves chunks as part\nof context for the LLM, we calculate the minimum\n"}
{"name_doc": "RAG_scalabilité.pdf", "doc_id": 0, "chunk_id": 2, "chunk_content": "of context for the LLM, we calculate the minimum\nof chunks retrieved such that the context contains\nall the needed information. Let us denote by R the\nretrieve function of the RAG method and MR the\nmaximum of retrieved chunks of R The goal is to\nstudy the restriction of R to the set of question Q.R\n:\nQ\n→\nCs\nq\n7→\nR(q) ,\nwhere R(q) is an ordered list of chunks.The absolute goal of a RAG method is for R to\nsatisfy C(q) ⊂R(q) for every user query q while\nminimizing MR.Hence we will run a serie of tests of, such call, best\npractices modifying our RAG method to determine\nthe impact of every feature on the retrieve function R.1.2\nMetrics\nFirst, it is important to differentiate the retrive\nfunction R of a RAG method to the actual context\n"}
{"name_doc": "RAG_scalabilité.pdf", "doc_id": 0, "chunk_id": 3, "chunk_content": "function R of a RAG method to the actual context\ngiven to the final LLM to generte the user answer.This context is created by a fixed portion of the\nretrieved chunks determine by the RAG method and\nthe context length of the LLM used. Usually, for a\nquery q, only the \"best\" chunks from R(q) is kept,\nHence we can study Rn the retrieval function where\nwe look for the n best chunks (if the method allows\nit) without modifying the studied RAG method and\nthe final context.For every feature, we calculate the proportion Pn(q)\nof usefull context in the retrieved context Rn(q) of\nevery question q for a fixed sample of n :\nPn(q) = #Cs(q) ∩Rn(q)\n#Rn(q)\nWe also calculate the best and the worst place\nof the elements of #Cs(q) ∩Rn(q) denoted Bs(q)\n"}
{"name_doc": "RAG_scalabilité.pdf", "doc_id": 0, "chunk_id": 4, "chunk_content": "of the elements of #Cs(q) ∩Rn(q) denoted Bs(q)\nand Ws(q) to follow it’s evolutions thought different\nfeatures. Note that Bs and Ws doesn’t depend on n.The second metrics we will estimate is the overall\nquality of the order of the retrieval using the nDCGp\nmetric defined as follows :\n1\nnDCGp(q) =\nPp\ni=1\n2reli(q)−1\nlog2(i+1)\nPp\ni=1\n2rel∗\ni (q)−1\nlog2(i+1)\n,\n(1)\nwhere p is the number of retrieve chunks by the\nretrieval, reli(q) is the relevance of each chunk\nretrieved and rel∗\ni (q) the ideal relevance of the set of\nretrieved chunks for the query q.For a clear automatization, we will define i to be 3\nif the chunk is in Cs(q), 1 if the chunk is in the right\ndocument and 0 else.The computation of this metric can be done post\n"}
{"name_doc": "RAG_scalabilité.pdf", "doc_id": 0, "chunk_id": 5, "chunk_content": "The computation of this metric can be done post\nruns, as we can try distinct p ≤n and get an estimate\nof a good p for a RAG method without considering\ncontext length of the LLM.1.3\nLimits\nThe first limit is for every methods that can’t recover\nan unlimited number of chunks as threshold technics.However, as explained, the goal is to analyse the\nimpact of features on the retrieve part and not on the\ncreation of the context. Indeed, threshold techniques\nare used to minimize the context lenght and then the\ninference time after the retrieve phase.Another limit is the difficulty to estimate why some\nnon-usefull chunks are retrieved and how close they\nare to the actual retrieve chunks. This information\nstill remains crucial to avoid hallucination from the\n"}
{"name_doc": "RAG_scalabilité.pdf", "doc_id": 0, "chunk_id": 6, "chunk_content": "still remains crucial to avoid hallucination from the\nLLM generating the final answer but won’t be study\nin our first approach.The scalability is limited as the maximal number\nof documents may not be relevant for some more\ndelicate use cases.2\nFeatures to evaluate\nWe enumerate all the differents features to tests\n1. Data preparation\n2. Splitters\n3. Rerankers\n4. Query Reformulations\n5. Embeddings\n6. Metadatas\n2.0.1\nData Preparation\nWe want to estimate the differences between Dp ways\nto prepare the data before splitting it. We have 2\ndata preparation to compare :\n1. PDFs without prepations\n2. Markdowns separation using dockling\nRemark. There are multiple ways to parse the data\nbefore chunking. Some are not relevant on our use\n"}
{"name_doc": "RAG_scalabilité.pdf", "doc_id": 0, "chunk_id": 7, "chunk_content": " Some are not relevant on our use\ncase that we are testing.2.0.2\nSplitters\nWe will try S several splitting technics :\n1. Length splitting\n2. Semantic Splitting\n3. Recursive Splitting\n4. Markdown Splitting (only available for the Mark-\ndown data preparation)\n2.1\nMetadatas\nWe will consider the impact of metadatas. We will\ndetermine some kinf of metadatas relevant with our\nfake use case and see the impact of each of them and\neach combination of them.2\n2.1.1\nRerankers\nWe will try distincts rerankers separatively.Best\nwould be to benchmark all rerankers providers as in\nAutoRAG. unfortunately it won’t be the case in first\ninstance but would be possible to test them as we\nwill keep all chunks retrieved pre-reranking. Indeed,\nthe reranking phase do not require to re run the\n"}
{"name_doc": "RAG_scalabilité.pdf", "doc_id": 0, "chunk_id": 8, "chunk_content": "the reranking phase do not require to re run the\nentire pipeline as it only rerank the chunks already\nordered and retrieved.We denote by R the number of tested rerankers\n2.1.2\nQuery Reformulation\nWe won’t study naive query reformulation as we con-\nsider our prompt as \"good\" prompts that respect best\npractice of prompt engineering. Moreover, we can\nstudy the multi query expansion. This would impose\nto fix a merge strategy of the contexts retrieved from\nall queries or another metric to evaluate if all the use-\nful chunks C(q) are retrieved through all the queries\ngenerated from the user’s one.2.1.3\nEmbeddings\nWe could try all the embeddings, but let us be\nrealistic, we will compare few ones already consider\nas best by Huggingface leader boards."}
{"name_doc": "RAG_scalabilité.pdf", "doc_id": 0, "chunk_id": 9, "chunk_content": "as best by Huggingface leader boards. We will also\ntry the impact of a finetuned embedding.Let us\ndenote by E the number of embeddings.3\nFeatures that won’t affect the\nretrieval study\nWe enumerate all the different features that will not\ninfluence the retrieval but would affect the RAG ac-\ncuracy overall.1. Number of retrieved chunks\n2. Passage augmenter\n3. Passage filter (Thresholds)\n4. Passage Compressor\nAs well we won’t study the server optimizations as\nsharding.4\nNumber\nof\nruns\n(without\nmetadatas)\nWe will measure the impact of features on different\ndataset size by adding more documents not relevant\nto our queries sample. Let us compute all the tests\nwe would :\nT = [(Dp −1) ∗(S −1) ∗E ∗R + S ∗E ∗R] ∗N,\n(2)\ntests to get all information about scalability, where\n"}
{"name_doc": "RAG_scalabilité.pdf", "doc_id": 0, "chunk_id": 10, "chunk_content": "tests to get all information about scalability, where\nN is the number of dataset we are testing.For Dp = 2, S = 4, E = 3, R = 3, N = 1 we have\nalready 63 tests to run. We can run all the tests for\nn = 500 and then study the results as if n was smaller\nsince the retrieved phases are not directly impacted\nby n.5\nCurves to print\nFor every n we will print Pn(q) and nNDCG(q) as a\nconfidence interval diagrams.For every feature, we will plot Pn(q) for different\nvalues of n.For every feature we will compute the minimum\nnmin(p) such that Pn(q) = 1 if possible. Note that\nnmin is calculated before posr-retrived features.To evaluate rerankers we especially compare the\ndifference between nmin(p) and nDCGp(q) before post-\nretrieval and nmin(p) and nDCGp(q) for the post-\n"}
{"name_doc": "RAG_scalabilité.pdf", "doc_id": 0, "chunk_id": 11, "chunk_content": "retrieval and nmin(p) and nDCGp(q) for the post-\nretrieval part.3\n"}
{"name_doc": "3002033669_Scaling Intelligence_ The Exponential Growth of AI's Power Needs.PDF", "doc_id": 1, "chunk_id": 1, "chunk_content": "Extension de fichier inconnu\n"}
{"name_doc": "RAG_scalabilité.pdf", "doc_id": 0, "chunk_id": 1, "chunk_content": "RAG Scalability\nClement Hardy, Benoit Joly\nAugust 25, 2025\nAbstract\nThe following study will compare different techniques\non a specific use case that we have made. The goal is\nto determine the impact of features on a RAG method\nretrieval and post-retrieval part.1\nScalability Metrics\n1.1\nMethodology\nWe suppose to have a set of documents D and a set of\nquestions Q on these documents. For each question\nq ∈Q, we determined the context C(q) needed to\nanswer the question q.For every splitter s, we denote Cs the set of chunks\nthat represent the set of documents D.For every\nquestion q ∈D, we denote the set of chunks Cs(q)\ncontaining the context C(q).For every RAG method that retrieves chunks as part\nof context for the LLM, we calculate the minimum\n"}
{"name_doc": "RAG_scalabilité.pdf", "doc_id": 0, "chunk_id": 2, "chunk_content": "of context for the LLM, we calculate the minimum\nof chunks retrieved such that the context contains\nall the needed information. Let us denote by R the\nretrieve function of the RAG method and MR the\nmaximum of retrieved chunks of R The goal is to\nstudy the restriction of R to the set of question Q.R\n:\nQ\n→\nCs\nq\n7→\nR(q) ,\nwhere R(q) is an ordered list of chunks.The absolute goal of a RAG method is for R to\nsatisfy C(q) ⊂R(q) for every user query q while\nminimizing MR.Hence we will run a serie of tests of, such call, best\npractices modifying our RAG method to determine\nthe impact of every feature on the retrieve function R.1.2\nMetrics\nFirst, it is important to differentiate the retrive\nfunction R of a RAG method to the actual context\n"}
{"name_doc": "RAG_scalabilité.pdf", "doc_id": 0, "chunk_id": 3, "chunk_content": "function R of a RAG method to the actual context\ngiven to the final LLM to generte the user answer.This context is created by a fixed portion of the\nretrieved chunks determine by the RAG method and\nthe context length of the LLM used. Usually, for a\nquery q, only the \"best\" chunks from R(q) is kept,\nHence we can study Rn the retrieval function where\nwe look for the n best chunks (if the method allows\nit) without modifying the studied RAG method and\nthe final context.For every feature, we calculate the proportion Pn(q)\nof usefull context in the retrieved context Rn(q) of\nevery question q for a fixed sample of n :\nPn(q) = #Cs(q) ∩Rn(q)\n#Rn(q)\nWe also calculate the best and the worst place\nof the elements of #Cs(q) ∩Rn(q) denoted Bs(q)\n"}
{"name_doc": "RAG_scalabilité.pdf", "doc_id": 0, "chunk_id": 4, "chunk_content": "of the elements of #Cs(q) ∩Rn(q) denoted Bs(q)\nand Ws(q) to follow it’s evolutions thought different\nfeatures. Note that Bs and Ws doesn’t depend on n.The second metrics we will estimate is the overall\nquality of the order of the retrieval using the nDCGp\nmetric defined as follows :\n1\nnDCGp(q) =\nPp\ni=1\n2reli(q)−1\nlog2(i+1)\nPp\ni=1\n2rel∗\ni (q)−1\nlog2(i+1)\n,\n(1)\nwhere p is the number of retrieve chunks by the\nretrieval, reli(q) is the relevance of each chunk\nretrieved and rel∗\ni (q) the ideal relevance of the set of\nretrieved chunks for the query q.For a clear automatization, we will define i to be 3\nif the chunk is in Cs(q), 1 if the chunk is in the right\ndocument and 0 else.The computation of this metric can be done post\n"}
{"name_doc": "RAG_scalabilité.pdf", "doc_id": 0, "chunk_id": 5, "chunk_content": "The computation of this metric can be done post\nruns, as we can try distinct p ≤n and get an estimate\nof a good p for a RAG method without considering\ncontext length of the LLM.1.3\nLimits\nThe first limit is for every methods that can’t recover\nan unlimited number of chunks as threshold technics.However, as explained, the goal is to analyse the\nimpact of features on the retrieve part and not on the\ncreation of the context. Indeed, threshold techniques\nare used to minimize the context lenght and then the\ninference time after the retrieve phase.Another limit is the difficulty to estimate why some\nnon-usefull chunks are retrieved and how close they\nare to the actual retrieve chunks. This information\nstill remains crucial to avoid hallucination from the\n"}
{"name_doc": "RAG_scalabilité.pdf", "doc_id": 0, "chunk_id": 6, "chunk_content": "still remains crucial to avoid hallucination from the\nLLM generating the final answer but won’t be study\nin our first approach.The scalability is limited as the maximal number\nof documents may not be relevant for some more\ndelicate use cases.2\nFeatures to evaluate\nWe enumerate all the differents features to tests\n1. Data preparation\n2. Splitters\n3. Rerankers\n4. Query Reformulations\n5. Embeddings\n6. Metadatas\n2.0.1\nData Preparation\nWe want to estimate the differences between Dp ways\nto prepare the data before splitting it. We have 2\ndata preparation to compare :\n1. PDFs without prepations\n2. Markdowns separation using dockling\nRemark. There are multiple ways to parse the data\nbefore chunking. Some are not relevant on our use\n"}
{"name_doc": "RAG_scalabilité.pdf", "doc_id": 0, "chunk_id": 7, "chunk_content": " Some are not relevant on our use\ncase that we are testing.2.0.2\nSplitters\nWe will try S several splitting technics :\n1. Length splitting\n2. Semantic Splitting\n3. Recursive Splitting\n4. Markdown Splitting (only available for the Mark-\ndown data preparation)\n2.1\nMetadatas\nWe will consider the impact of metadatas. We will\ndetermine some kinf of metadatas relevant with our\nfake use case and see the impact of each of them and\neach combination of them.2\n2.1.1\nRerankers\nWe will try distincts rerankers separatively.Best\nwould be to benchmark all rerankers providers as in\nAutoRAG. unfortunately it won’t be the case in first\ninstance but would be possible to test them as we\nwill keep all chunks retrieved pre-reranking. Indeed,\nthe reranking phase do not require to re run the\n"}
{"name_doc": "RAG_scalabilité.pdf", "doc_id": 0, "chunk_id": 8, "chunk_content": "the reranking phase do not require to re run the\nentire pipeline as it only rerank the chunks already\nordered and retrieved.We denote by R the number of tested rerankers\n2.1.2\nQuery Reformulation\nWe won’t study naive query reformulation as we con-\nsider our prompt as \"good\" prompts that respect best\npractice of prompt engineering. Moreover, we can\nstudy the multi query expansion. This would impose\nto fix a merge strategy of the contexts retrieved from\nall queries or another metric to evaluate if all the use-\nful chunks C(q) are retrieved through all the queries\ngenerated from the user’s one.2.1.3\nEmbeddings\nWe could try all the embeddings, but let us be\nrealistic, we will compare few ones already consider\nas best by Huggingface leader boards."}
{"name_doc": "RAG_scalabilité.pdf", "doc_id": 0, "chunk_id": 9, "chunk_content": "as best by Huggingface leader boards. We will also\ntry the impact of a finetuned embedding.Let us\ndenote by E the number of embeddings.3\nFeatures that won’t affect the\nretrieval study\nWe enumerate all the different features that will not\ninfluence the retrieval but would affect the RAG ac-\ncuracy overall.1. Number of retrieved chunks\n2. Passage augmenter\n3. Passage filter (Thresholds)\n4. Passage Compressor\nAs well we won’t study the server optimizations as\nsharding.4\nNumber\nof\nruns\n(without\nmetadatas)\nWe will measure the impact of features on different\ndataset size by adding more documents not relevant\nto our queries sample. Let us compute all the tests\nwe would :\nT = [(Dp −1) ∗(S −1) ∗E ∗R + S ∗E ∗R] ∗N,\n(2)\ntests to get all information about scalability, where\n"}
{"name_doc": "RAG_scalabilité.pdf", "doc_id": 0, "chunk_id": 10, "chunk_content": "tests to get all information about scalability, where\nN is the number of dataset we are testing.For Dp = 2, S = 4, E = 3, R = 3, N = 1 we have\nalready 63 tests to run. We can run all the tests for\nn = 500 and then study the results as if n was smaller\nsince the retrieved phases are not directly impacted\nby n.5\nCurves to print\nFor every n we will print Pn(q) and nNDCG(q) as a\nconfidence interval diagrams.For every feature, we will plot Pn(q) for different\nvalues of n.For every feature we will compute the minimum\nnmin(p) such that Pn(q) = 1 if possible. Note that\nnmin is calculated before posr-retrived features.To evaluate rerankers we especially compare the\ndifference between nmin(p) and nDCGp(q) before post-\nretrieval and nmin(p) and nDCGp(q) for the post-\n"}
{"name_doc": "RAG_scalabilité.pdf", "doc_id": 0, "chunk_id": 11, "chunk_content": "retrieval and nmin(p) and nDCGp(q) for the post-\nretrieval part.3\n"}
{"name_doc": "3002033669_Scaling Intelligence_ The Exponential Growth of AI's Power Needs.PDF", "doc_id": 1, "chunk_id": 1, "chunk_content": "Extension de fichier inconnu\n"}
{"name_doc": "RAG_scalabilité.pdf", "doc_id": 0, "chunk_id": 1, "chunk_content": "RAG Scalability\nClement Hardy, Benoit Joly\nAugust 25, 2025\nAbstract\nThe following study will compare different techniques\non a specific use case that we have made. The goal is\nto determine the impact of features on a RAG method\nretrieval and post-retrieval part.1\nScalability Metrics\n1.1\nMethodology\nWe suppose to have a set of documents D and a set of\nquestions Q on these documents. For each question\nq ∈Q, we determined the context C(q) needed to\nanswer the question q.For every splitter s, we denote Cs the set of chunks\nthat represent the set of documents D.For every\nquestion q ∈D, we denote the set of chunks Cs(q)\ncontaining the context C(q).For every RAG method that retrieves chunks as part\nof context for the LLM, we calculate the minimum\n"}
{"name_doc": "RAG_scalabilité.pdf", "doc_id": 0, "chunk_id": 2, "chunk_content": "of context for the LLM, we calculate the minimum\nof chunks retrieved such that the context contains\nall the needed information. Let us denote by R the\nretrieve function of the RAG method and MR the\nmaximum of retrieved chunks of R The goal is to\nstudy the restriction of R to the set of question Q.R\n:\nQ\n→\nCs\nq\n7→\nR(q) ,\nwhere R(q) is an ordered list of chunks.The absolute goal of a RAG method is for R to\nsatisfy C(q) ⊂R(q) for every user query q while\nminimizing MR.Hence we will run a serie of tests of, such call, best\npractices modifying our RAG method to determine\nthe impact of every feature on the retrieve function R.1.2\nMetrics\nFirst, it is important to differentiate the retrive\nfunction R of a RAG method to the actual context\n"}
{"name_doc": "RAG_scalabilité.pdf", "doc_id": 0, "chunk_id": 3, "chunk_content": "function R of a RAG method to the actual context\ngiven to the final LLM to generte the user answer.This context is created by a fixed portion of the\nretrieved chunks determine by the RAG method and\nthe context length of the LLM used. Usually, for a\nquery q, only the \"best\" chunks from R(q) is kept,\nHence we can study Rn the retrieval function where\nwe look for the n best chunks (if the method allows\nit) without modifying the studied RAG method and\nthe final context.For every feature, we calculate the proportion Pn(q)\nof usefull context in the retrieved context Rn(q) of\nevery question q for a fixed sample of n :\nPn(q) = #Cs(q) ∩Rn(q)\n#Rn(q)\nWe also calculate the best and the worst place\nof the elements of #Cs(q) ∩Rn(q) denoted Bs(q)\n"}
{"name_doc": "RAG_scalabilité.pdf", "doc_id": 0, "chunk_id": 4, "chunk_content": "of the elements of #Cs(q) ∩Rn(q) denoted Bs(q)\nand Ws(q) to follow it’s evolutions thought different\nfeatures. Note that Bs and Ws doesn’t depend on n.The second metrics we will estimate is the overall\nquality of the order of the retrieval using the nDCGp\nmetric defined as follows :\n1\nnDCGp(q) =\nPp\ni=1\n2reli(q)−1\nlog2(i+1)\nPp\ni=1\n2rel∗\ni (q)−1\nlog2(i+1)\n,\n(1)\nwhere p is the number of retrieve chunks by the\nretrieval, reli(q) is the relevance of each chunk\nretrieved and rel∗\ni (q) the ideal relevance of the set of\nretrieved chunks for the query q.For a clear automatization, we will define i to be 3\nif the chunk is in Cs(q), 1 if the chunk is in the right\ndocument and 0 else.The computation of this metric can be done post\n"}
{"name_doc": "RAG_scalabilité.pdf", "doc_id": 0, "chunk_id": 5, "chunk_content": "The computation of this metric can be done post\nruns, as we can try distinct p ≤n and get an estimate\nof a good p for a RAG method without considering\ncontext length of the LLM.1.3\nLimits\nThe first limit is for every methods that can’t recover\nan unlimited number of chunks as threshold technics.However, as explained, the goal is to analyse the\nimpact of features on the retrieve part and not on the\ncreation of the context. Indeed, threshold techniques\nare used to minimize the context lenght and then the\ninference time after the retrieve phase.Another limit is the difficulty to estimate why some\nnon-usefull chunks are retrieved and how close they\nare to the actual retrieve chunks. This information\nstill remains crucial to avoid hallucination from the\n"}
{"name_doc": "RAG_scalabilité.pdf", "doc_id": 0, "chunk_id": 6, "chunk_content": "still remains crucial to avoid hallucination from the\nLLM generating the final answer but won’t be study\nin our first approach.The scalability is limited as the maximal number\nof documents may not be relevant for some more\ndelicate use cases.2\nFeatures to evaluate\nWe enumerate all the differents features to tests\n1. Data preparation\n2. Splitters\n3. Rerankers\n4. Query Reformulations\n5. Embeddings\n6. Metadatas\n2.0.1\nData Preparation\nWe want to estimate the differences between Dp ways\nto prepare the data before splitting it. We have 2\ndata preparation to compare :\n1. PDFs without prepations\n2. Markdowns separation using dockling\nRemark. There are multiple ways to parse the data\nbefore chunking. Some are not relevant on our use\n"}
{"name_doc": "RAG_scalabilité.pdf", "doc_id": 0, "chunk_id": 7, "chunk_content": " Some are not relevant on our use\ncase that we are testing.2.0.2\nSplitters\nWe will try S several splitting technics :\n1. Length splitting\n2. Semantic Splitting\n3. Recursive Splitting\n4. Markdown Splitting (only available for the Mark-\ndown data preparation)\n2.1\nMetadatas\nWe will consider the impact of metadatas. We will\ndetermine some kinf of metadatas relevant with our\nfake use case and see the impact of each of them and\neach combination of them.2\n2.1.1\nRerankers\nWe will try distincts rerankers separatively.Best\nwould be to benchmark all rerankers providers as in\nAutoRAG. unfortunately it won’t be the case in first\ninstance but would be possible to test them as we\nwill keep all chunks retrieved pre-reranking. Indeed,\nthe reranking phase do not require to re run the\n"}
{"name_doc": "RAG_scalabilité.pdf", "doc_id": 0, "chunk_id": 8, "chunk_content": "the reranking phase do not require to re run the\nentire pipeline as it only rerank the chunks already\nordered and retrieved.We denote by R the number of tested rerankers\n2.1.2\nQuery Reformulation\nWe won’t study naive query reformulation as we con-\nsider our prompt as \"good\" prompts that respect best\npractice of prompt engineering. Moreover, we can\nstudy the multi query expansion. This would impose\nto fix a merge strategy of the contexts retrieved from\nall queries or another metric to evaluate if all the use-\nful chunks C(q) are retrieved through all the queries\ngenerated from the user’s one.2.1.3\nEmbeddings\nWe could try all the embeddings, but let us be\nrealistic, we will compare few ones already consider\nas best by Huggingface leader boards."}
{"name_doc": "RAG_scalabilité.pdf", "doc_id": 0, "chunk_id": 9, "chunk_content": "as best by Huggingface leader boards. We will also\ntry the impact of a finetuned embedding.Let us\ndenote by E the number of embeddings.3\nFeatures that won’t affect the\nretrieval study\nWe enumerate all the different features that will not\ninfluence the retrieval but would affect the RAG ac-\ncuracy overall.1. Number of retrieved chunks\n2. Passage augmenter\n3. Passage filter (Thresholds)\n4. Passage Compressor\nAs well we won’t study the server optimizations as\nsharding.4\nNumber\nof\nruns\n(without\nmetadatas)\nWe will measure the impact of features on different\ndataset size by adding more documents not relevant\nto our queries sample. Let us compute all the tests\nwe would :\nT = [(Dp −1) ∗(S −1) ∗E ∗R + S ∗E ∗R] ∗N,\n(2)\ntests to get all information about scalability, where\n"}
{"name_doc": "RAG_scalabilité.pdf", "doc_id": 0, "chunk_id": 10, "chunk_content": "tests to get all information about scalability, where\nN is the number of dataset we are testing.For Dp = 2, S = 4, E = 3, R = 3, N = 1 we have\nalready 63 tests to run. We can run all the tests for\nn = 500 and then study the results as if n was smaller\nsince the retrieved phases are not directly impacted\nby n.5\nCurves to print\nFor every n we will print Pn(q) and nNDCG(q) as a\nconfidence interval diagrams.For every feature, we will plot Pn(q) for different\nvalues of n.For every feature we will compute the minimum\nnmin(p) such that Pn(q) = 1 if possible. Note that\nnmin is calculated before posr-retrived features.To evaluate rerankers we especially compare the\ndifference between nmin(p) and nDCGp(q) before post-\nretrieval and nmin(p) and nDCGp(q) for the post-\n"}
{"name_doc": "RAG_scalabilité.pdf", "doc_id": 0, "chunk_id": 11, "chunk_content": "retrieval and nmin(p) and nDCGp(q) for the post-\nretrieval part.3\n"}
{"name_doc": "3002033669_Scaling Intelligence_ The Exponential Growth of AI's Power Needs.PDF", "doc_id": 1, "chunk_id": 1, "chunk_content": "Extension de fichier inconnu\n"}
{"name_doc": "RAG_scalabilité.pdf", "doc_id": 0, "chunk_id": 1, "chunk_content": "RAG Scalability\nClement Hardy, Benoit Joly\nAugust 25, 2025\nAbstract\nThe following study will compare different techniques\non a specific use case that we have made. The goal is\nto determine the impact of features on a RAG method\nretrieval and post-retrieval part.1\nScalability Metrics\n1.1\nMethodology\nWe suppose to have a set of documents D and a set of\nquestions Q on these documents. For each question\nq ∈Q, we determined the context C(q) needed to\nanswer the question q.For every splitter s, we denote Cs the set of chunks\nthat represent the set of documents D.For every\nquestion q ∈D, we denote the set of chunks Cs(q)\ncontaining the context C(q).For every RAG method that retrieves chunks as part\nof context for the LLM, we calculate the minimum\n"}
{"name_doc": "RAG_scalabilité.pdf", "doc_id": 0, "chunk_id": 2, "chunk_content": "of context for the LLM, we calculate the minimum\nof chunks retrieved such that the context contains\nall the needed information. Let us denote by R the\nretrieve function of the RAG method and MR the\nmaximum of retrieved chunks of R The goal is to\nstudy the restriction of R to the set of question Q.R\n:\nQ\n→\nCs\nq\n7→\nR(q) ,\nwhere R(q) is an ordered list of chunks.The absolute goal of a RAG method is for R to\nsatisfy C(q) ⊂R(q) for every user query q while\nminimizing MR.Hence we will run a serie of tests of, such call, best\npractices modifying our RAG method to determine\nthe impact of every feature on the retrieve function R.1.2\nMetrics\nFirst, it is important to differentiate the retrive\nfunction R of a RAG method to the actual context\n"}
{"name_doc": "RAG_scalabilité.pdf", "doc_id": 0, "chunk_id": 3, "chunk_content": "function R of a RAG method to the actual context\ngiven to the final LLM to generte the user answer.This context is created by a fixed portion of the\nretrieved chunks determine by the RAG method and\nthe context length of the LLM used. Usually, for a\nquery q, only the \"best\" chunks from R(q) is kept,\nHence we can study Rn the retrieval function where\nwe look for the n best chunks (if the method allows\nit) without modifying the studied RAG method and\nthe final context.For every feature, we calculate the proportion Pn(q)\nof usefull context in the retrieved context Rn(q) of\nevery question q for a fixed sample of n :\nPn(q) = #Cs(q) ∩Rn(q)\n#Rn(q)\nWe also calculate the best and the worst place\nof the elements of #Cs(q) ∩Rn(q) denoted Bs(q)\n"}
{"name_doc": "RAG_scalabilité.pdf", "doc_id": 0, "chunk_id": 4, "chunk_content": "of the elements of #Cs(q) ∩Rn(q) denoted Bs(q)\nand Ws(q) to follow it’s evolutions thought different\nfeatures. Note that Bs and Ws doesn’t depend on n.The second metrics we will estimate is the overall\nquality of the order of the retrieval using the nDCGp\nmetric defined as follows :\n1\nnDCGp(q) =\nPp\ni=1\n2reli(q)−1\nlog2(i+1)\nPp\ni=1\n2rel∗\ni (q)−1\nlog2(i+1)\n,\n(1)\nwhere p is the number of retrieve chunks by the\nretrieval, reli(q) is the relevance of each chunk\nretrieved and rel∗\ni (q) the ideal relevance of the set of\nretrieved chunks for the query q.For a clear automatization, we will define i to be 3\nif the chunk is in Cs(q), 1 if the chunk is in the right\ndocument and 0 else.The computation of this metric can be done post\n"}
{"name_doc": "RAG_scalabilité.pdf", "doc_id": 0, "chunk_id": 5, "chunk_content": "The computation of this metric can be done post\nruns, as we can try distinct p ≤n and get an estimate\nof a good p for a RAG method without considering\ncontext length of the LLM.1.3\nLimits\nThe first limit is for every methods that can’t recover\nan unlimited number of chunks as threshold technics.However, as explained, the goal is to analyse the\nimpact of features on the retrieve part and not on the\ncreation of the context. Indeed, threshold techniques\nare used to minimize the context lenght and then the\ninference time after the retrieve phase.Another limit is the difficulty to estimate why some\nnon-usefull chunks are retrieved and how close they\nare to the actual retrieve chunks. This information\nstill remains crucial to avoid hallucination from the\n"}
{"name_doc": "RAG_scalabilité.pdf", "doc_id": 0, "chunk_id": 6, "chunk_content": "still remains crucial to avoid hallucination from the\nLLM generating the final answer but won’t be study\nin our first approach.The scalability is limited as the maximal number\nof documents may not be relevant for some more\ndelicate use cases.2\nFeatures to evaluate\nWe enumerate all the differents features to tests\n1. Data preparation\n2. Splitters\n3. Rerankers\n4. Query Reformulations\n5. Embeddings\n6. Metadatas\n2.0.1\nData Preparation\nWe want to estimate the differences between Dp ways\nto prepare the data before splitting it. We have 2\ndata preparation to compare :\n1. PDFs without prepations\n2. Markdowns separation using dockling\nRemark. There are multiple ways to parse the data\nbefore chunking. Some are not relevant on our use\n"}
{"name_doc": "RAG_scalabilité.pdf", "doc_id": 0, "chunk_id": 7, "chunk_content": " Some are not relevant on our use\ncase that we are testing.2.0.2\nSplitters\nWe will try S several splitting technics :\n1. Length splitting\n2. Semantic Splitting\n3. Recursive Splitting\n4. Markdown Splitting (only available for the Mark-\ndown data preparation)\n2.1\nMetadatas\nWe will consider the impact of metadatas. We will\ndetermine some kinf of metadatas relevant with our\nfake use case and see the impact of each of them and\neach combination of them.2\n2.1.1\nRerankers\nWe will try distincts rerankers separatively.Best\nwould be to benchmark all rerankers providers as in\nAutoRAG. unfortunately it won’t be the case in first\ninstance but would be possible to test them as we\nwill keep all chunks retrieved pre-reranking. Indeed,\nthe reranking phase do not require to re run the\n"}
{"name_doc": "RAG_scalabilité.pdf", "doc_id": 0, "chunk_id": 8, "chunk_content": "the reranking phase do not require to re run the\nentire pipeline as it only rerank the chunks already\nordered and retrieved.We denote by R the number of tested rerankers\n2.1.2\nQuery Reformulation\nWe won’t study naive query reformulation as we con-\nsider our prompt as \"good\" prompts that respect best\npractice of prompt engineering. Moreover, we can\nstudy the multi query expansion. This would impose\nto fix a merge strategy of the contexts retrieved from\nall queries or another metric to evaluate if all the use-\nful chunks C(q) are retrieved through all the queries\ngenerated from the user’s one.2.1.3\nEmbeddings\nWe could try all the embeddings, but let us be\nrealistic, we will compare few ones already consider\nas best by Huggingface leader boards."}
{"name_doc": "RAG_scalabilité.pdf", "doc_id": 0, "chunk_id": 9, "chunk_content": "as best by Huggingface leader boards. We will also\ntry the impact of a finetuned embedding.Let us\ndenote by E the number of embeddings.3\nFeatures that won’t affect the\nretrieval study\nWe enumerate all the different features that will not\ninfluence the retrieval but would affect the RAG ac-\ncuracy overall.1. Number of retrieved chunks\n2. Passage augmenter\n3. Passage filter (Thresholds)\n4. Passage Compressor\nAs well we won’t study the server optimizations as\nsharding.4\nNumber\nof\nruns\n(without\nmetadatas)\nWe will measure the impact of features on different\ndataset size by adding more documents not relevant\nto our queries sample. Let us compute all the tests\nwe would :\nT = [(Dp −1) ∗(S −1) ∗E ∗R + S ∗E ∗R] ∗N,\n(2)\ntests to get all information about scalability, where\n"}
{"name_doc": "RAG_scalabilité.pdf", "doc_id": 0, "chunk_id": 10, "chunk_content": "tests to get all information about scalability, where\nN is the number of dataset we are testing.For Dp = 2, S = 4, E = 3, R = 3, N = 1 we have\nalready 63 tests to run. We can run all the tests for\nn = 500 and then study the results as if n was smaller\nsince the retrieved phases are not directly impacted\nby n.5\nCurves to print\nFor every n we will print Pn(q) and nNDCG(q) as a\nconfidence interval diagrams.For every feature, we will plot Pn(q) for different\nvalues of n.For every feature we will compute the minimum\nnmin(p) such that Pn(q) = 1 if possible. Note that\nnmin is calculated before posr-retrived features.To evaluate rerankers we especially compare the\ndifference between nmin(p) and nDCGp(q) before post-\nretrieval and nmin(p) and nDCGp(q) for the post-\n"}
{"name_doc": "RAG_scalabilité.pdf", "doc_id": 0, "chunk_id": 11, "chunk_content": "retrieval and nmin(p) and nDCGp(q) for the post-\nretrieval part.3\n"}
{"name_doc": "3002033669_Scaling Intelligence_ The Exponential Growth of AI's Power Needs.PDF", "doc_id": 1, "chunk_id": 1, "chunk_content": "Extension de fichier inconnu\n"}
