# ğŸš€ OpenRAG by Meritis

**Welcome to OpenRAG** - An open-source, user-friendly RAG (Retrieval-Augmented Generation) benchmark tool that helps you find the perfect RAG method for your specific use case and data.

![OpenRAG Interface](streamlit_/images/screen_rag_maker.png)

---

## ğŸ“‹ Table of Contents

- [ğŸ¯ What is OpenRAG?](#-what-is-openrag)
- [âœ¨ Key Features](#-key-features)
- [ğŸ”§ Prerequisites](#-prerequisites)
- [âš¡ Quick Start](#-quick-start)
- [ğŸ› ï¸ Installation & Setup](#ï¸-installation--setup)
- [ğŸš€ Launch Methods](#-launch-methods)
- [ğŸ“Š Available Configurations](#-available-configurations)
- [ğŸ’» System Requirements](#-system-requirements)
- [ğŸ“± Using the Application](#-using-the-application)
- [ğŸ”§ Troubleshooting](#-troubleshooting)
- [ğŸ“ Support & Contact](#-support--contact)

---

## ğŸ¯ What is OpenRAG?

OpenRAG is a comprehensive benchmark tool designed to help users evaluate and compare different RAG (Retrieval-Augmented Generation) methods. With over a dozen implemented RAG techniques, OpenRAG provides:

- **Quantitative analysis** of RAG performance
- **Customizable configurations** for each RAG method
- **Energy consumption and carbon footprint** tracking
- **Token usage analysis** and cost estimation
- **Response time benchmarking**

The goal is to help you decide which RAG method is most appropriate for your specific use case and data.

---

## âœ¨ Key Features

- ğŸ¤– **12+ RAG Methods**: Naive RAG, Corrective RAG, Self RAG, Graph RAG, and more
- ğŸ”Œ **Multiple LLM Providers**: Ollama, VLLM, OpenAI, Mistral
- ğŸ“Š **Comprehensive Benchmarking**: Performance, speed, energy, and cost analysis
- ğŸ¨ **User-Friendly Interface**: Streamlit-based web interface
- ğŸ”§ **Full Customization**: Tailor each RAG method to your needs
- ğŸ“ **Data Upload**: Support for various document formats
- ğŸŒ **Cross-Platform**: Works on Windows, macOS, and Linux
- ğŸ® **Auto-Detection**: Automatic OS and GPU detection

---

## ğŸ”§ Prerequisites

Before getting started, make sure you have the following installed:

### Required Software
- **Docker** (version 20.10 or higher)
- **Docker Compose** (version 2.0 or higher)
- **Git** (for cloning the repository)

### System Requirements
- **RAM**: Minimum 8GB (16GB+ recommended for GPU usage)
- **Storage**: At least 10GB free space
- **Operating System**: Windows 10+, macOS 10.14+, or Linux

### For GPU Usage (Optional but Recommended)
- **NVIDIA GPU** with CUDA support
- **NVIDIA Docker** runtime installed
- **CUDA 12.1+** for VLLM support

---

## âš¡ Quick Start

Get OpenRAG running in under 5 minutes:

### 1ï¸âƒ£ Clone the Repository
```bash
git clone https://github.com/your-org/OpenRAG.git
cd OpenRAG
```

### 2ï¸âƒ£ Choose Your Launch Method

**Option A: Automatic Detection (Recommended)**
```bash
cd docker
./launch.sh ollama up -d
```

**Option B: Full Service with Elasticsearch**
```bash
./start_services/ollama.sh
```

### 3ï¸âƒ£ Access the Application
Open your browser and navigate to: **[http://localhost:8506](http://localhost:8506)**

---

## ğŸ› ï¸ Installation & Setup

### Step 1: Clone and Setup
```bash
# Clone the repository
git clone https://github.com/your-org/OpenRAG.git
cd OpenRAG

# Make scripts executable (Linux/macOS)
chmod +x docker/launch.sh
chmod +x start_services/*.sh
```

### Step 2: Configure Environment (Optional)
Create a `.env` file in the root directory for custom configurations:
```bash
# Example .env file
FRONT_LOCAL_PORT=8506
ES_LOCAL_PORT=9200
ES_LOCAL_PASSWORD=changeme
ollama_LOCAL_CONTAINER_NAME=openrag_ollama
```

### Step 3: Choose Your Configuration
Based on your system and requirements, choose one of the launch methods below.

---

## ğŸš€ Launch Methods

OpenRAG offers three different ways to launch the application, each suited for different use cases:

### ğŸ¯ Method 1: Auto-Detection Script (Recommended)

**Best for**: Most users who want simplicity with automatic system detection.

```bash
cd docker
./launch.sh [service] [docker-compose-commands]
```

**Examples:**
```bash
# Start Ollama (automatically detects CPU/GPU)
./launch.sh ollama up -d

# Stop all services
./launch.sh ollama down

# View live logs
./launch.sh ollama logs -f

# Restart services
./launch.sh ollama restart

# Check service status
./launch.sh ollama ps
```

### ğŸ”§ Method 2: Manual Docker Compose

**Best for**: Advanced users who want full control over configurations.

#### For CPU-Only Systems (macOS, systems without NVIDIA GPU):
```bash
cd docker

# Launch individual services
docker compose -f docker-compose-ollama.yml up -d
docker compose -f docker-compose-api.yml up -d

# Launch all services (Note: VLLM may not work without GPU)
docker compose -f docker-compose-all.yml up -d
```

#### For GPU Systems (Linux/Windows with NVIDIA GPU):
```bash
cd docker

# Launch with GPU acceleration
docker compose -f docker-compose-ollama.yml -f docker-compose-ollama.gpu.yml up -d
docker compose -f docker-compose-vllm.yml -f docker-compose-vllm.gpu.yml up -d
docker compose -f docker-compose-all.yml -f docker-compose-all.gpu.yml up -d
```

### ğŸª Method 3: Full Service Scripts

**Best for**: Users who want complete service management including Elasticsearch setup.

```bash
# Start Ollama with Elasticsearch
./start_services/ollama.sh

# Start VLLM with Elasticsearch (requires GPU)
./start_services/vllm.sh

# Start all services with Elasticsearch
./start_services/all.sh

# Start API-only mode (Elasticsearch + Frontend)
./start_services/api.sh
```

---

## ğŸ“Š Available Configurations

Choose the configuration that matches your needs:

| Configuration | Services Included | Use Case | GPU Required |
|---------------|-------------------|----------|--------------|
| **ğŸ¤– ollama** | Elasticsearch + Frontend + Ollama | Local LLM with Ollama | Optional (recommended) |
| **âš¡ vllm** | Elasticsearch + Frontend + VLLM | High-performance inference | **Yes** |
| **ğŸš€ all** | Elasticsearch + Frontend + Ollama + VLLM | Full local setup | Optional |
| **ğŸŒ api** | Elasticsearch + Frontend only | Use with API keys (OpenAI, Mistral) | No |

### System Detection Logic

OpenRAG automatically detects your system configuration:

- **ğŸ macOS**: Always uses CPU-only configuration (Apple Silicon/Intel)
- **ğŸ§ Linux**: Checks for NVIDIA GPU with `nvidia-smi`
- **ğŸªŸ Windows**: Checks for NVIDIA GPU with `nvidia-smi`

### LLM Provider Support

| Provider | API Key Required | Local Hardware | GPU Recommended |
|----------|------------------|----------------|-----------------|
| **Ollama** | âŒ No | âœ… Yes | ğŸŸ¡ Optional |
| **VLLM** | âŒ No | âœ… Yes | âœ… Required |
| **OpenAI** | âœ… Yes | âŒ No | âŒ No |
| **Mistral** | âœ… Yes | âŒ No | âŒ No |

---

## ğŸ’» System Requirements

### Minimum Requirements
- **CPU**: 4 cores
- **RAM**: 8GB
- **Storage**: 10GB free space
- **OS**: Windows 10+, macOS 10.14+, Ubuntu 18.04+

### Recommended for GPU Usage
- **CPU**: 8+ cores
- **RAM**: 16GB+
- **GPU**: NVIDIA GPU with 8GB+ VRAM
- **Storage**: 50GB+ free space (for models)

### VLLM Specific Requirements
- **CUDA**: Version 12.1 or higher
- **GPU Memory**: 8GB+ recommended
- **OS**: Linux or Windows with WSL2

---

## ğŸ“± Using the Application

Once OpenRAG is running, access it at **[http://localhost:8506](http://localhost:8506)**

### ğŸ“‚ 1. Upload Your Data
![Database Interface](streamlit_/images/screen_db.png)

- Navigate to the **"ğŸŒ Databases"** page
- Upload documents in various formats (PDF, TXT, DOCX, etc.)
- Configure indexing settings for your data

### ğŸ’¬ 2. Chat & Test RAG Methods
- Go to **"ğŸ’¬ Chat"** to interact with different RAG methods
- Select your preferred RAG technique
- Test responses to assess performance qualitatively

### âš™ï¸ 3. Customize RAG Methods
![RAG Maker Interface](streamlit_/images/screen_rag_maker.png)

- Visit **"ğŸ”§ RAG Maker"** for advanced configurations
- Tune parameters for each RAG method
- Create custom RAG pipelines

### ğŸ“Š 4. Benchmark & Compare
![Report Interface](streamlit_/images/screen_report.png)

- Use **"ğŸ“š Benchmark"** to run quantitative comparisons
- Generate detailed reports on:
  - **Performance metrics** (accuracy, relevance)
  - **Response times**
  - **Energy consumption**
  - **Token usage and costs**
  - **Carbon footprint**

### ğŸ› ï¸ 5. Advanced Configuration
- Access **"ğŸ› ï¸ Advanced Configuration"** for system-level settings
- Configure model parameters, memory usage, and more

---

## ğŸ”§ Troubleshooting

### Common Issues & Solutions

#### ğŸš¨ "Permission denied" when running scripts
```bash
# Make scripts executable
chmod +x docker/launch.sh
chmod +x start_services/*.sh
```

#### ğŸš¨ Port already in use (8506)
```bash
# Check what's using the port
lsof -i :8506

# Kill the process or change port in docker-compose files
# Edit the FRONT_LOCAL_PORT in your .env file
```

#### ğŸš¨ Docker: "no such file or directory"
```bash
# Make sure you're in the correct directory
cd OpenRAG/docker

# Verify files exist
ls -la docker-compose-*.yml
```

#### ğŸš¨ Elasticsearch fails to start
```bash
# Check available memory
free -m

# Ensure port 9200 is available
lsof -i :9200

# On Linux, fix permissions
sudo chown -R 1000:1000 volumes/dev-elasticsearch
```

#### ğŸš¨ VLLM not working on macOS
**Issue**: VLLM requires NVIDIA GPU, which is not available on macOS.

**Solutions**:
- Use Ollama configuration instead: `./launch.sh ollama up -d`
- Use API-only mode: `./launch.sh api up -d`
- Consider using OpenAI or Mistral APIs

#### ğŸš¨ GPU not detected on Linux
```bash
# Check NVIDIA drivers
nvidia-smi

# Install NVIDIA Docker runtime
sudo apt install nvidia-docker2
sudo systemctl restart docker

# Verify GPU access in Docker
docker run --rm --gpus all nvidia/cuda:11.0-base nvidia-smi
```

#### ğŸš¨ Out of memory errors
```bash
# Check Docker memory limits
docker system info

# Reduce model size or batch size in configurations
# Close other applications to free up RAM
```

### ğŸ“ Getting Help

If you encounter issues not covered here:

1. **Check Docker logs**:
   ```bash
   docker logs [container_name]
   ```

2. **Restart services**:
   ```bash
   ./launch.sh [service] restart
   ```

3. **Clean restart**:
   ```bash
   ./launch.sh [service] down
   docker system prune -f
   ./launch.sh [service] up -d
   ```

---

## ğŸ“ Support & Contact

### ğŸ¤ Getting Help

- **Issues & Bugs**: [GitHub Issues](https://github.com/your-org/OpenRAG/issues)
- **Discussions**: [GitHub Discussions](https://github.com/your-org/OpenRAG/discussions)
- **Documentation**: This README and in-app help

### ğŸ’¼ Professional Support

For business inquiries, enterprise support, or custom implementations:

**Meritis Innovation IA Team**
- ğŸŒ Website: [https://meritis.fr/expertise/innovation-ia/](https://meritis.fr/expertise/innovation-ia/)
- ğŸ“§ Contact: [Contact Form](https://meritis.fr/expertise/innovation-ia/#block-form)

### ğŸ¤– Community

- Star â­ the repository if OpenRAG helps your projects
- Share your use cases and feedback
- Contribute to the project development

---

## ğŸ™ Acknowledgments

OpenRAG is developed and maintained by the Innovation IA team at **Meritis**. Special thanks to all contributors and the open-source community for making this project possible.

---

**Happy RAG Benchmarking! ğŸš€**