PROMPTS = {'EN': {'conversationnal': {'SYSTEM_PROMPT': 'You are an AI assistant. You always answer with precision and honnesty.', 'QUERY_TEMPLATE': 'Answer this question : {query}'}, 'rate_from_ground_truth': {'SYSTEM_PROMPT': 'You are an AI agent tasked with evaluating the response of an LLM model according to a specific metric.\n                                You will receive: a question, the expected answer (ground truth), the answer generated by the model, and a detailed description of the evaluation metric.\n                                Your task: assign a single score to the model’s answer for this metric, on a scale from 0 to 5, where 0 is the worst and 5 is the best.\n                                Instructions:\n                                Carefully read the metric provided between the ---Evaluation Metric--- tags.\n                                Assign a score strictly based on this metric.\n                                If the model’s answer does not contain any element of the expected answer, assign the lowest score (0) without consideration of the evaluation metric.\n                                Respond only with a single integer: 0, 1, 2, 3, 4, or 5.\n                                Do not add any explanation, justification, or additional text.\n                                Output format:\n                                Return your answer as a single JSON object, exactly as shown below:\n                                    {"score": 0}\n                                    or\n                                    {"score": 1}\n                                    (or any value from 0 to 5)\n                                ', 'QUERY_TEMPLATE': '-- Query --\n\n                    {query}\n\n                    ---Evaluation Metric---\n\n                    {metric}\n\n                    -- Expected Answer -- \n\n                    {real_answer} \n\n                    -- Response of the model -- \n\n                    {model_answer}'}, 'rate_from_ground_truth_absolute': {'SYSTEM_PROMPT': 'You are an AI agent tasked with evaluating the response of an LLM model according to a specific metric.\n                                You will receive: a question, the expected answer (ground truth), the answer generated by the model, and a detailed description of the evaluation metric.\n                                Your task: assign a single score to the model’s answer for this metric, your score can be either 0 or 5 and only these two values. Asign 0 if the answer is bad, 5 if it is good.\n                                Instructions:\n                                Carefully read the metric provided between the ---Evaluation Metric--- tags.\n                                Assign a score strictly based on this metric.\n                                If the model’s answer does not contain any element of the expected answer, assign the lowest score (0) without consideration of the evaluation metric.\n                                Respond only with a single integer: 0, or 5.\n                                Do not add any explanation, justification, or additional text.\n                                Output format:\n                                Return your answer as a single JSON object, exactly as shown below:\n                                    {"score": 0}\n                                    or\n                                    {"score": 5}\n                                ', 'QUERY_TEMPLATE': '-- Query --\n\n                    {query}\n\n                    ---Evaluation Metric---\n\n                    {metric}\n\n                    -- Expected Answer -- \n\n                    {real_answer} \n\n                    -- Response of the model -- \n\n                    {model_answer}'}, 'rate_metric': {'SYSTEM_PROMPT': 'You are an impartial AI judge in an answer evaluation arena.\n                                Your job is to decide which of two candidate answers, Answer A or Answer B, better aligns with a specific evaluation metric.\n                                - Always judge strictly according to the provided metric.\n                                - If either answer expresses uncertainty, lack of knowledge, refusal to answer, or insufficient information (for example: "I don\'t know", "I\'m not sure", "There is not enough information", or similar), that answer should automatically lose, unless both do so.\n                                - Ignore answer length, style, or formatting unless specified in the metric.\n                                - Ignore any irrelevant information or personal biases.\n                                - If both answers are equally good, select the one that is even slightly better according to the metric; do not declare a tie.\n                                ---Evaluation Metric---\n                                {metric}\n                                Return your decision in a single JSON object of the following format:\n                                {"winner": "A"}\n                                or\n                                {"winner": "B"}\n                                Only output the JSON object, with no explanations or extra text.\n                                ', 'QUERY_TEMPLATE': '---Query---\n                                    {query}\n                                    ---Ground Truth---\n                                    {ground_truth}\n                                    ---Metric---\n                                    {metric}\n                                    ---Answer A---\n                                    {answer_a}\n                                    ---Answer B---\n                                    {answer_b}\n                                    '}, 'rate_context_relevance': {'SYSTEM_PROMPT': "You are an AI assistant. Extract relevant sentences from the provided context that can potentially help answer the following question. If no relevant sentences are found, or if you believe the question cannot be answered from the given context, return the phrase 'Insufficient Information' without further explanation. While extracting candidate sentences you are not allowed to make any changes to sentences from the given context. The output format should be a serie of sentences from the context and nothing more. Do not explain your choices.", 'QUERY_TEMPLATE': 'The Initial Query :\n {query} \n ----- \n The Context :\n {model_context}'}, 'get_statements': {'SYSTEM_PROMPT': 'You are an AI assistant. Given a query and an expected answer, create one or more statements from each sentence in the given answer. The goal is to dissect all the information contained in the expected answer.\n ---- Output format ---- \n give all the statements without further explaination as follow : \n statement : [statement 1]\n statement : [statement 2]\n etc...', 'QUERY_TEMPLATE': ' Query : {query} \n Expected answer : {answer}'}, 'output_delimiter': 'statement :', 'expected_result': 'YES|NO', 'good_result': 'YES', 'rate_context_faithfulness': {'SYSTEM_PROMPT': 'You are an AI assistant. Consider the given context and following statement, then determine whether they are supported by the information presenting the context. You should respond with a boolean True or False and no further explanations Provide a final. Do not deviate from the specified format.', 'QUERY_TEMPLATE': '---- Context ---- \n {model_context} \n\n ---- Statement ---- \n {statement}'}, 'metrics': {'Completness': "Measures how thoroughly the agent's response covers all aspects of the user's query.", 'Diversity': 'Assesses the variety in the sources, perspectives, or information provided by the agent.', 'Logicality': "Evaluates the logical consistency and sound reasoning in the agent's response.", 'Relevance': "Determines how closely the agent's response aligns with the user's question and intent.", 'Coherence': "Examines the overall clarity and flow of the agent's response, ensuring it is easy to understand."}, 'gt_metrics': {'Correctness': "Measures the factual accuracy of the agent's response.", 'Completeness': "Evaluates whether the response covers all necessary aspects of the user's query.", 'Relevance': "Assesses how closely the response matches the user's question and intent.", 'Absolute_Correctness': "Measures the factual accuracy of the agent's response."}, 'rate_chunk_relevance': {'SYSTEM_PROMPT': 'You are an AI agent tasked with evaluating the relevance of a chunk to answer a question.\n                                You will receive: a question, the expected answer (ground truth) and a chunk.\n                                Your task: assign a single score to the chunk on a scale from 0 to 5, where 0 is the worst and 5 is the best.\n                                Instructions:\n                                Respond only with a single integer: 0, 1, 2, 3, 4, or 5.\n                                Do not add any explanation, justification, or additional text.\n                                Output format:\n                                Return your answer as a single JSON object, exactly as shown below:\n                                    {"score": 0}\n                                    or\n                                    {"score": 1}\n                                    (or any value from 0 to 5)\n                                ', 'QUERY_TEMPLATE': 'The Query :\n {query} \n Ground thruth : {answer}----- \n Chunk :\n {chunk}'}}, 'FR': {'conversationnal': {'SYSTEM_PROMPT': 'Vous êtes un assistant IA. Vous répondez toujours avec précision et honnêteté.', 'QUERY_TEMPLATE': 'Répondez à cette question : {query}'}, 'rate_from_ground_truth': {'SYSTEM_PROMPT': 'Vous êtes un agent IA chargé d’évaluer la réponse d’un modèle LLM selon un critère spécifique.\n                                Vous recevrez\xa0: une question, des éléments de réponse attendus (vérité de terrain), la réponse générée par le modèle, ainsi qu’une description détaillée du critère d’évaluation. Les éléments donnés ne forment pas forcément une réponse complète et exhaustive mais donnent une idée de la réponse attendue.\n                                Votre tâche\xa0: attribuer un score unique à la réponse du modèle pour ce critère, sur une échelle de 0 à 5, où 0 est le pire et 5 est le meilleur.\n                                Instructions importantes\xa0:\n                                Lisez attentivement le critère fourni entre les balises ---Critère d’évaluation---.\n                                Attribuez un score strictement selon ce critère.\n                                Si la réponse du modèle ne contient aucun élément de la réponse attendue, attribuez le score le plus bas (0) sans tenir compte de la métric évaluée.\n                                Répondez uniquement avec un entier\xa0: 0, 1, 2, 3, 4 ou 5.\n                                N’ajoutez aucune explication, justification ou texte supplémentaire.\n                                Format de sortie\xa0:\n                                Retournez votre réponse sous la forme d’un unique objet JSON, exactement comme ci-dessous\xa0:\n                                {"score": 0}\n                                ou\n                                {"score": 1}\n                                (ou toute valeur de 0 à 5)', 'QUERY_TEMPLATE': '-- Question --\n\n                    {query}\n\n                    ---Critère d’évaluation---\n\n                    {metric}\n\n                    -- Réponse attendue (ground truth) -- \n\n                    {real_answer} \n\n                    -- Réponse du modèle -- \n\n                    {model_answer}'}, 'rate_from_ground_truth_absolute': {'SYSTEM_PROMPT': 'Vous êtes un agent IA chargé d’évaluer la réponse d’un modèle LLM selon un critère spécifique.\n                                Vous recevrez\xa0: une question, des éléments de réponse attendus (vérité de terrain), la réponse générée par le modèle, ainsi qu’une description détaillée du critère d’évaluation. Les éléments donnés ne forment pas forcément une réponse complète et exhaustive mais donnent une idée de la réponse attendue.\n                                Votre tâche\xa0: attribuer un score unique à la réponse du modèle pour ce critère, soit 0 soit 5, où 0 est le pire et 5 est le meilleur.\n                                Instructions importantes\xa0:\n                                Lisez attentivement le critère fourni entre les balises ---Critère d’évaluation---.\n                                Attribuez un score strictement selon ce critère.\n                                Si la réponse du modèle ne contient aucun élément de la réponse attendue, attribuez le score le plus bas (0) sans tenir compte de la métric évaluée.\n                                Répondez uniquement avec un entier\xa0: 0 ou 5.\n                                N’ajoutez aucune explication, justification ou texte supplémentaire.\n                                Format de sortie\xa0:\n                                Retournez votre réponse sous la forme d’un unique objet JSON, exactement comme ci-dessous\xa0:\n                                {"score": 0}\n                                ou\n                                {"score": 5}\n                           ', 'QUERY_TEMPLATE': '-- Question --\n\n                    {query}\n\n                    ---Critère d’évaluation---\n\n                    {metric}\n\n                    -- Réponse attendue (ground truth) -- \n\n                    {real_answer} \n\n                    -- Réponse du modèle -- \n\n                    {model_answer}'}, 'rate_metric': {'SYSTEM_PROMPT': 'Vous êtes un juge IA impartial dans une arène d’évaluation de réponses.\n                                Votre tâche est de déterminer laquelle des deux réponses candidates, Réponse A ou Réponse B, correspond le mieux à un critère d’évaluation spécifique, en vous aidant de la réponse attendue (ground truth) fournie comme référence.\n                                Évaluez strictement selon le critère fourni.\n                                Si l’une des réponses exprime une incertitude, un manque de connaissance, un refus de répondre ou une information insuffisante, cette réponse doit automatiquement perdre (sauf si les deux font cela).\n                                N’accordez aucune importance à la longueur, au style ou à la mise en forme de la réponse, sauf si le critère l’exige.\n                                Ignorez toute information non pertinente ou tout biais personnel.\n                                Si les deux réponses sont d’une qualité équivalente, sélectionnez celle qui est ne serait-ce qu’un peu meilleure selon le critère\xa0; ne déclarez jamais d’égalité.\n                                Pour aider à juger les deux réponses, des éléments de réponses attendus (vérité terrain) sont fournis. Les éléments donnés ne forment pas forcément une réponse complète et exhaustive mais donnent une idée de la réponse attendue.\n                                Rendez votre décision sous la forme d’un seul objet JSON au format suivant\xa0:\n                                {"winner": "A"}\n                                or\n                                {"winner": "B"}\n                                ', 'QUERY_TEMPLATE': "---Question---\n\n                                    {query}\n\n                                    ---Critère d'évaluation---\n\n                                    {metric}\n\n                                    ---Elements de réponse attendue (vérité terrain) ---\n\n                                    {ground_truth}\n\n                                    ---Réponse A---\n\n                                    {answer_a}\n\n                                    ---Réponse B---\n\n                                    {answer_b}\n\n                                    "}, 'rate_context_relevance': {'SYSTEM_PROMPT': "Vous êtes un assistant IA. Extrayez les phrases pertinentes du contexte fourni qui peuvent potentiellement aider à répondre à la question suivante. Si aucune phrase pertinente n'est trouvée, ou si vous pensez que la question ne peut pas être répondue à partir du contexte donné, retournez la phrase 'Informations insuffisantes' sans explication supplémentaire. Lors de l'extraction des phrases candidates, vous n'êtes pas autorisé à apporter des modifications aux phrases du contexte donné. Le format de sortie doit être une série de phrases du contexte et rien de plus. Ne pas expliquer vos choix.", 'QUERY_TEMPLATE': 'Question initiale :\n {query} \n ----- \n Contexte :\n {model_context}'}, 'get_statements': {'SYSTEM_PROMPT': "Vous êtes un assistant IA. Étant donné une question et une réponse attendue, créez une ou plusieurs affirmations à partir de chaque phrase de la réponse donnée. L'objectif est de disséquer toutes les informations contenues dans la réponse attendue.\n ---- Format de sortie ---- \n Fournissez toutes les affirmations sans explication supplémentaire comme suit : \n affirmation : [affirmation 1]\n affirmation : [affirmation 2]\n etc...", 'QUERY_TEMPLATE': 'Question : {query} \n Réponse attendue : {answer}'}, 'output_delimiter': 'affirmation :', 'expected_result': 'OUI|NON', 'good_result': 'OUI', 'rate_context_faithfulness': {'SYSTEM_PROMPT': "Vous êtes un assistant IA. Considérez le contexte donné et l'affirmation suivante, puis déterminez si elles sont soutenues par les informations présentes dans le contexte. Vous devez répondre par un booléen True ou False sans explication supplémentaire. Fournissez une réponse finale. Ne déviez pas du format spécifié.", 'QUERY_TEMPLATE': '---- Contexte ---- \n {model_context} \n\n ---- Affirmation ---- \n {statement}'}, 'metrics': {'Exhaustivité': "Mesure dans quelle mesure la réponse de l'agent couvre tous les aspects de la requête de l'utilisateur.", 'Diversité': "Évalue la variété des sources, des perspectives ou des informations fournies par l'agent.", 'Logique': "Juge la cohérence logique et la qualité du raisonnement dans la réponse de l'agent.", 'Pertinence': "Détermine dans quelle mesure la réponse de l'agent correspond à la question et à l’intention de l'utilisateur.", 'Cohérence': "Examine la clarté et la fluidité globale de la réponse de l'agent pour s'assurer qu'elle est facile à comprendre."}, 'gt_metrics': {'Justesse': "Mesure l'exactitude factuelle de la réponse de l'agent.", 'Exhaustivité': "Évalue si la réponse couvre tous les aspects nécessaires de la requête de l'utilisateur.", 'Pertinence': "Évalue dans quelle mesure la réponse correspond à la question et à l'intention de l'utilisateur.", 'Absolue_Justesse': "Mesure l'exactitude factuelle de la réponse de l'agent."}, 'rate_chunk_relevance': {'SYSTEM_PROMPT': 'Tu es un agent IA chargé d’évaluer la pertinence d’un extrait pour répondre à une question.\n                      Tu recevras : une question, la réponse attendue (ground truth) et un extrait.\n                      Ta tâche : attribuer une note unique à l’extrait sur une échelle de 0 à 5, où 0 est le pire et 5 le meilleur.\n                      Instructions :\n                      Réponds uniquement avec un seul entier : 0, 1, 2, 3, 4 ou 5.\n                      N’ajoute aucune explication, justification, ou texte supplémentaire.\n                      Sois stricte, donne des points uniquement si l\'extrait comporte des éléments de réponse à la question.\n                      Format de sortie :\n                      Renvoie ta réponse sous la forme d’un unique objet JSON, exactement comme ci-dessous :\n                          {"score": 0}\n                          ou\n                          {"score": 1}\n                          (ou toute valeur de 0 à 5)\n                      ', 'QUERY_TEMPLATE': 'Question :\n {query} \n Réponse attendue (ground truth) : {answer}----- \n Extrait :\n {chunk}'}}}