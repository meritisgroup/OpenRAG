<html>
    <head>
        <meta charset="utf-8">
        
            <script src="lib/bindings/utils.js"></script>
            <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/vis-network/9.1.2/dist/dist/vis-network.min.css" integrity="sha512-WgxfT5LWjfszlPHXRmBWHkV2eceiWTOBvrKCNbdgDYTHrT2AeLCGbF4sZlZw3UMN3WtL0tGUoIAKsu8mllg/XA==" crossorigin="anonymous" referrerpolicy="no-referrer" />
            <script src="https://cdnjs.cloudflare.com/ajax/libs/vis-network/9.1.2/dist/vis-network.min.js" integrity="sha512-LnvoEWDFrqGHlHmDD2101OrLcbsfkrzoSpvtSQtxK3RMnRV0eOkhhBN2dXHKRrUU8p2DGRTk35n4O8nWSVe1mQ==" crossorigin="anonymous" referrerpolicy="no-referrer"></script>
            
        
<center>
<h1></h1>
</center>

<!-- <link rel="stylesheet" href="../node_modules/vis/dist/vis.min.css" type="text/css" />
<script type="text/javascript" src="../node_modules/vis/dist/vis.js"> </script>-->
        <link
          href="https://cdn.jsdelivr.net/npm/bootstrap@5.0.0-beta3/dist/css/bootstrap.min.css"
          rel="stylesheet"
          integrity="sha384-eOJMYsd53ii+scO/bJGFsiCZc+5NDVN2yr8+0RDqr0Ql0h+rP48ckxlpbzKgwra6"
          crossorigin="anonymous"
        />
        <script
          src="https://cdn.jsdelivr.net/npm/bootstrap@5.0.0-beta3/dist/js/bootstrap.bundle.min.js"
          integrity="sha384-JEW9xMcG8R+pH31jmWH6WWP0WintQrMb4s7ZOdauHnUtxwoG2vI5DkLtS3qm9Ekf"
          crossorigin="anonymous"
        ></script>


        <center>
          <h1></h1>
        </center>
        <style type="text/css">

             #mynetwork {
                 width: 100%;
                 height: 600px;
                 background-color: #222222;
                 border: 1px solid lightgray;
                 position: relative;
                 float: left;
             }

             
             #loadingBar {
                 position:absolute;
                 top:0px;
                 left:0px;
                 width: 100%;
                 height: 600px;
                 background-color:rgba(200,200,200,0.8);
                 -webkit-transition: all 0.5s ease;
                 -moz-transition: all 0.5s ease;
                 -ms-transition: all 0.5s ease;
                 -o-transition: all 0.5s ease;
                 transition: all 0.5s ease;
                 opacity:1;
             }

             #bar {
                 position:absolute;
                 top:0px;
                 left:0px;
                 width:20px;
                 height:20px;
                 margin:auto auto auto auto;
                 border-radius:11px;
                 border:2px solid rgba(30,30,30,0.05);
                 background: rgb(0, 173, 246); /* Old browsers */
                 box-shadow: 2px 0px 4px rgba(0,0,0,0.4);
             }

             #border {
                 position:absolute;
                 top:10px;
                 left:10px;
                 width:500px;
                 height:23px;
                 margin:auto auto auto auto;
                 box-shadow: 0px 0px 4px rgba(0,0,0,0.2);
                 border-radius:10px;
             }

             #text {
                 position:absolute;
                 top:8px;
                 left:530px;
                 width:30px;
                 height:50px;
                 margin:auto auto auto auto;
                 font-size:22px;
                 color: #000000;
             }

             div.outerBorder {
                 position:relative;
                 top:400px;
                 width:600px;
                 height:44px;
                 margin:auto auto auto auto;
                 border:8px solid rgba(0,0,0,0.1);
                 background: rgb(252,252,252); /* Old browsers */
                 background: -moz-linear-gradient(top,  rgba(252,252,252,1) 0%, rgba(237,237,237,1) 100%); /* FF3.6+ */
                 background: -webkit-gradient(linear, left top, left bottom, color-stop(0%,rgba(252,252,252,1)), color-stop(100%,rgba(237,237,237,1))); /* Chrome,Safari4+ */
                 background: -webkit-linear-gradient(top,  rgba(252,252,252,1) 0%,rgba(237,237,237,1) 100%); /* Chrome10+,Safari5.1+ */
                 background: -o-linear-gradient(top,  rgba(252,252,252,1) 0%,rgba(237,237,237,1) 100%); /* Opera 11.10+ */
                 background: -ms-linear-gradient(top,  rgba(252,252,252,1) 0%,rgba(237,237,237,1) 100%); /* IE10+ */
                 background: linear-gradient(to bottom,  rgba(252,252,252,1) 0%,rgba(237,237,237,1) 100%); /* W3C */
                 filter: progid:DXImageTransform.Microsoft.gradient( startColorstr='#fcfcfc', endColorstr='#ededed',GradientType=0 ); /* IE6-9 */
                 border-radius:72px;
                 box-shadow: 0px 0px 10px rgba(0,0,0,0.2);
             }
             

             
             #config {
                 float: left;
                 width: 400px;
                 height: 600px;
             }
             

             
        </style>
    </head>


    <body>
        <div class="card" style="width: 100%">
            
            
            <div id="mynetwork" class="card-body"></div>
        </div>

        
            <div id="loadingBar">
              <div class="outerBorder">
                <div id="text">0%</div>
                <div id="border">
                  <div id="bar"></div>
                </div>
              </div>
            </div>
        
        
            <div id="config"></div>
        

        <script type="text/javascript">

              // initialize global variables.
              var edges;
              var nodes;
              var allNodes;
              var allEdges;
              var nodeColors;
              var originalNodes;
              var network;
              var container;
              var options, data;
              var filter = {
                  item : '',
                  property : '',
                  value : []
              };

              

              

              // This method is responsible for drawing the graph, returns the drawn network
              function drawGraph() {
                  var container = document.getElementById('mynetwork');

                  

                  // parsing and collecting nodes and edges from the python
                  nodes = new vis.DataSet([{"color": "rgb(178, 204, 15)", "font": {"color": "white"}, "id": "DIEGO THOMAS", "label": "DIEGO THOMAS", "shape": "dot", "size": 10, "title": "Diego Thomas is a researcher cited as an author in the paper \u0027Activeneus: Neural signed\u0027"}, {"color": "rgb(178, 204, 15)", "font": {"color": "white"}, "id": "ACTIVENEUS: NEURAL SIGNED", "label": "ACTIVENEUS: NEURAL SIGNED", "shape": "dot", "size": 10, "title": "Activeneus: Neural signed refers to a neural signed distance function or related neural representation discussed in the cited work"}, {"color": "rgb(70, 100, 22)", "font": {"color": "white"}, "id": "STRUCTURED LIGHT", "label": "STRUCTURED LIGHT", "shape": "dot", "size": 10, "title": "A 3D scanning technique mentioned as part of the comparison set."}, {"color": "rgb(70, 100, 22)", "font": {"color": "white"}, "id": "MVPS", "label": "MVPS", "shape": "dot", "size": 10, "title": "Multi-View Photometric Stereo, a benchmark framework for evaluating 3D reconstruction methods."}, {"color": "rgb(70, 100, 22)", "font": {"color": "white"}, "id": "NEURAL INVERSE STRUCTURED LIGHT", "label": "NEURAL INVERSE STRUCTURED LIGHT", "shape": "dot", "size": 10, "title": "A technique combining structured light and neural networks for rapid 3D scanning"}, {"color": "rgb(70, 100, 22)", "font": {"color": "white"}, "id": "TURBOSL", "label": "TURBOSL", "shape": "dot", "size": 10, "title": "A structured light based method cited as [33] for 4D scene reconstruction.\nA 3D reconstruction approach cited as [33]\nA rapid surface learning method for 3D scene reconstruction.\nA fast neural surface learning technique used in the comparison\nA neural reconstruction method referenced as [33]\nAnother structured light-based method noted for improved shape recovery over MVPS methods\nA 3D reconstruction approach cited as [33]\nDense, accurate, and fast 3D reconstruction using neural inverse structured light"}, {"color": "rgb(243, 72, 250)", "font": {"color": "white"}, "id": "JOHANNES L SCHONBERGER", "label": "JOHANNES L SCHONBERGER", "shape": "dot", "size": 10, "title": "Researcher in computer vision and structure-from-motion"}, {"color": "rgb(243, 72, 250)", "font": {"color": "white"}, "id": "STRUCTURE-FROM-MOTION REVISITED", "label": "STRUCTURE-FROM-MOTION REVISITED", "shape": "dot", "size": 10, "title": "Paper discussing advances in structure-from-motion techniques"}, {"color": "rgb(57, 62, 17)", "font": {"color": "white"}, "id": "CHAOFENG CHEN", "label": "CHAOFENG CHEN", "shape": "dot", "size": 10, "title": "Chaofeng Chen is a researcher who co-authored the paper Ps-nerf: Neural inverse rendering for multi-view photometric stereo."}, {"color": "rgb(57, 62, 17)", "font": {"color": "white"}, "id": "PS-NERF", "label": "PS-NERF", "shape": "dot", "size": 10, "title": "a method that integrates photometric stereo with neural radiance fields for 3D scene reconstruction\nA NeRF-based method referenced as (f) in the study\nA variant of Neural Radiance Fields that predicts surface normals and uses them for improved rendering.\nPs-nerf is a neural inverse rendering technique for multi-view photometric stereo."}, {"color": "rgb(105, 132, 238)", "font": {"color": "white"}, "id": "WIDE-BASELINE STEREO", "label": "WIDE-BASELINE STEREO", "shape": "dot", "size": 10, "title": "A stereo vision method that uses cameras with large separation to achieve accurate depth estimation, requiring regularization for effective view interpolation"}, {"color": "rgb(105, 132, 238)", "font": {"color": "white"}, "id": "REGULARIZATION", "label": "REGULARIZATION", "shape": "dot", "size": 10, "title": "A technique to impose constraints or smoothness on a model to prevent overfitting or to improve interpolation quality\nA mathematical technique used to impose smoothness or prior knowledge on model parameters, reducing overfitting when data is scarce."}, {"color": "rgb(14, 125, 85)", "font": {"color": "white"}, "id": "TJ(P(T))", "label": "TJ(P(T))", "shape": "dot", "size": 10, "title": "The accumulated transmittance along the ray from light source\u202fj to point\u202fp(t), representing how much light reaches that point."}, {"color": "rgb(14, 125, 85)", "font": {"color": "white"}, "id": "IJ(P(T))", "label": "IJ(P(T))", "shape": "dot", "size": 10, "title": "The irradiance at point\u202fp(t) emitted from light source\u202fj, attenuated by distance and transmittance."}, {"color": "rgb(143, 201, 116)", "font": {"color": "white"}, "id": "DEPTH-SUPERVISED NERF", "label": "DEPTH-SUPERVISED NERF", "shape": "dot", "size": 10, "title": "A neural radiance field model trained with depth supervision for faster and fewer-view reconstruction"}, {"color": "rgb(143, 201, 116)", "font": {"color": "white"}, "id": "KANGLE DENG", "label": "KANGLE DENG", "shape": "dot", "size": 10, "title": "Researcher and co-author of the paper on depth-supervised NeRF\nResearcher and co-author of the 2023 paper on Total-recon."}, {"color": "rgb(143, 201, 116)", "font": {"color": "white"}, "id": "ANDREW LIU", "label": "ANDREW LIU", "shape": "dot", "size": 10, "title": "Researcher and co-author of the paper on depth-supervised NeRF"}, {"color": "rgb(143, 201, 116)", "font": {"color": "white"}, "id": "JUN-YAN ZHU", "label": "JUN-YAN ZHU", "shape": "dot", "size": 10, "title": "Researcher and co-author of the paper on depth-supervised NeRF"}, {"color": "rgb(143, 201, 116)", "font": {"color": "white"}, "id": "DEVA RAMAN", "label": "DEVA RAMAN", "shape": "dot", "size": 10, "title": "Researcher and co-author of the paper on depth-supervised NeRF"}, {"color": "rgb(143, 201, 116)", "font": {"color": "white"}, "id": "FREE TRAINING", "label": "FREE TRAINING", "shape": "dot", "size": 10, "title": "The ability to train depth-supervised NeRF models more quickly and with fewer views"}, {"color": "rgb(143, 201, 116)", "font": {"color": "white"}, "id": "NEURAL 4D SCENE RECONSTRUCTION", "label": "NEURAL 4D SCENE RECONSTRUCTION", "shape": "dot", "size": 10, "title": "A method for reconstructing dynamic 3D scenes using neural networks and limited hardware\nTitle of the PDF discussing 4D scene reconstruction using neural methods\nA deep learning framework for reconstructing 3\u2011D scenes over time from multi\u2011channel images, described in the cited PDF chapter.\nA method for reconstructing 4D scenes using neural networks, referenced by the document title.\nA method for reconstructing 3\u2011D scenes over time using neural networks, aiming to recover accurate geometry and appearance from multiple views.\nThe overall research area of reconstructing dynamic scenes in four dimensions using neural methods\nTitle of the PDF containing the cited works.\nTitle of the PDF document containing the cited works\nA research topic focused on reconstructing 3D scenes over time using neural rendering techniques.\nA research topic focused on reconstructing 3\u2011D scenes over time using neural network techniques.\nThe broader research area addressed by the document, involving 4D scene reconstruction using neural methods"}, {"color": "rgb(19, 52, 3)", "font": {"color": "white"}, "id": "OBJECTS", "label": "OBJECTS", "shape": "dot", "size": 10, "title": "Physical entities whose 3D geometry is reconstructed"}, {"color": "rgb(19, 52, 3)", "font": {"color": "white"}, "id": "GROUND TRUTHS (GT)", "label": "GROUND TRUTHS (GT)", "shape": "dot", "size": 10, "title": "Reference data representing the true positions and shapes of objects in a scene"}, {"color": "rgb(99, 106, 157)", "font": {"color": "white"}, "id": "IEEE/CVF CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION", "label": "IEEE/CVF CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION", "shape": "dot", "size": 10, "title": "An annual international conference organized by IEEE and CVF, focusing on advances in computer vision and pattern recognition, held in 2022 and 2024.\nAn annual conference where cutting\u2011edge computer vision research is presented\nVenue where the Turbosl paper was presented\nAn annual international conference organized jointly by the IEEE Computer Society and the CVF, focusing on advances in computer vision and pattern recognition, held in 2020 for this publication.\nAnnual conference where the paper on dense depth priors was presented.\nAnnual conference for computer vision research"}, {"color": "rgb(99, 106, 157)", "font": {"color": "white"}, "id": "ANTI-ALIASED NEURAL RADIANCE FIELDS", "label": "ANTI-ALIASED NEURAL RADIANCE FIELDS", "shape": "dot", "size": 10, "title": "A neural rendering technique that produces high\u2011quality, anti\u2011aliased 3D scene representations using neural radiance fields, described in a 2022 IEEE/CVF CVPR paper."}, {"color": "rgb(99, 106, 157)", "font": {"color": "white"}, "id": "RNB-NEUS", "label": "RNB-NEUS", "shape": "dot", "size": 10, "title": "A multi\u2011view 3D reconstruction method that leverages reflectance and normal information, introduced in a 2024 CVPR paper."}, {"color": "rgb(99, 106, 157)", "font": {"color": "white"}, "id": "SPARSE 3D RECONSTRUCTION VIA OBJECT-CENTRIC RAY SAMPLING", "label": "SPARSE 3D RECONSTRUCTION VIA OBJECT-CENTRIC RAY SAMPLING", "shape": "dot", "size": 10, "title": "A 2024 CVPR paper presenting a sparse 3D reconstruction technique that uses object\u2011centric ray sampling."}, {"color": "rgb(99, 106, 157)", "font": {"color": "white"}, "id": "INFONERF", "label": "INFONERF", "shape": "dot", "size": 10, "title": "A neural radiance field variant that minimizes ray entropy, assuming each ray intersects the surface once\nA method that applies ray entropy minimization to achieve few\u2011shot neural volume rendering"}, {"color": "rgb(99, 106, 157)", "font": {"color": "white"}, "id": "VGGT", "label": "VGGT", "shape": "dot", "size": 10, "title": "Visual geometry grounded transformer, a neural architecture for vision tasks"}, {"color": "rgb(178, 204, 15)", "font": {"color": "white"}, "id": "NEURAL FIELDS", "label": "NEURAL FIELDS", "shape": "dot", "size": 10, "title": "Learned representations that map spatial coordinates to scene properties, trained using synthesized images"}, {"color": "rgb(178, 204, 15)", "font": {"color": "white"}, "id": "IMAGES", "label": "IMAGES", "shape": "dot", "size": 10, "title": "Photographs or visual data captured from various viewpoints for analysis.\nDigital captures of the scene taken under different illumination conditions and viewpoints, used as input for MVPS algorithms.\nVisual data generated or captured for training neural fields\nDigital captures of a scene taken from different viewpoints and under different lighting conditions.\nCaptured visual data representing the scene under specific lighting conditions.\nSynthetic images produced by the simulation for testing."}, {"color": "rgb(105, 132, 238)", "font": {"color": "white"}, "id": "DEEP MULTI-VIEW PHOTOMETRIC STEREO", "label": "DEEP MULTI-VIEW PHOTOMETRIC STEREO", "shape": "dot", "size": 10, "title": "An imaging technique that reconstructs surface normals and reflectance from multiple calibrated views."}, {"color": "rgb(105, 132, 238)", "font": {"color": "white"}, "id": "NEURAL RADIANCE FIELDS", "label": "NEURAL RADIANCE FIELDS", "shape": "dot", "size": 10, "title": "A deep learning approach for representing 3D scenes as continuous volumetric fields, used for photometric stereo.\nA neural representation of scene radiance used for novel view synthesis\nA neural representation of 3D scenes used for rendering novel views.\nA class of models for representing 3D scenes using neural networks\nA neural network representation for 3D scenes that predicts color and density for any viewpoint.\nNeural radiance fields (NeRF) are deep learning models that synthesize novel views of a scene by predicting radiance and density.\nNeural radiance fields (NeRF) are a deep learning framework for representing 3D scenes as continuous volumetric functions that output color and density for any 3D point and viewing direction.\nA representation that maps 3\u2011D coordinates and viewing directions to emitted color and density, enabling novel view synthesis."}, {"color": "rgb(178, 204, 15)", "font": {"color": "white"}, "id": "HIROSHI KAWASAKI", "label": "HIROSHI KAWASAKI", "shape": "dot", "size": 10, "title": "Co-author and researcher at Kyushu University\nResearcher and co\u2011author of the Nerf\u2011based multi\u2011frame 3D integration paper.\nHiroshi Kawasaki is a researcher cited as an author in the paper \u0027Activeneus: Neural signed\u0027\nCo-author of the active lighting and computer vision publication.\nA researcher who co\u2011authored the paper \u201cDepth reconstruction with neural signed distance fields in structured light systems.\u201d\nA researcher in computer vision, co\u2011author of dense 3D reconstruction and active stereo papers."}, {"color": "rgb(178, 204, 15)", "font": {"color": "white"}, "id": "KYUSHU UNIVERSITY", "label": "KYUSHU UNIVERSITY", "shape": "dot", "size": 10, "title": "Japanese university hosting multiple authors of the study"}, {"color": "rgb(178, 204, 15)", "font": {"color": "white"}, "id": "NERF-BASED MULTI-FRAME 3D INTEGRATION", "label": "NERF-BASED MULTI-FRAME 3D INTEGRATION", "shape": "dot", "size": 10, "title": "A neural radiance field (NeRF) approach that fuses multiple frames for 3D endoscopy reconstruction using active stereo."}, {"color": "rgb(178, 204, 15)", "font": {"color": "white"}, "id": "ACTIVE LIGHTING", "label": "ACTIVE LIGHTING", "shape": "dot", "size": 10, "title": "A method of illuminating a scene with controlled light sources to enhance photometric measurements\nIllumination strategy that changes lighting conditions to infer surface normals\nA technique that uses controlled illumination patterns to enhance 3D reconstruction accuracy.\nillumination strategy that actively controls light sources to aid reconstruction\nIllumination technique that provides additional information to estimate 3D shape.\nA method of illumination that provides dynamic lighting information used to find correspondences between image points and 3D geometry.\nA technique that manipulates illumination to enhance visual perception in computer vision tasks."}, {"color": "rgb(70, 100, 22)", "font": {"color": "white"}, "id": "DEPTH RECONSTRUCTION WITH NEURAL SIGNED DISTANCE FIELDS IN STRUCTURED LIGHT SYSTEMS", "label": "DEPTH RECONSTRUCTION WITH NEURAL SIGNED DISTANCE FIELDS IN STRUCTURED LIGHT SYSTEMS", "shape": "dot", "size": 10, "title": "A 2024 research paper presenting a method for depth reconstruction using neural signed distance fields within structured light systems, presented at the 2024 International Conference on 3D Vision (3DV)."}, {"color": "rgb(178, 204, 15)", "font": {"color": "white"}, "id": "DENSE 3D RECONSTRUCTION METHOD USING A SINGLE PATTERN FOR FAST MOVING OBJECT", "label": "DENSE 3D RECONSTRUCTION METHOD USING A SINGLE PATTERN FOR FAST MOVING OBJECT", "shape": "dot", "size": 10, "title": "A computational technique that reconstructs dense 3D models of rapidly moving objects from a single patterned illumination, enabling fast acquisition."}, {"color": "rgb(178, 204, 15)", "font": {"color": "white"}, "id": "GRID-BASED ACTIVE STEREO WITH SINGLE-COLORED WAVE PATTERN FOR DENSE ONE-SHOT 3D SCAN", "label": "GRID-BASED ACTIVE STEREO WITH SINGLE-COLORED WAVE PATTERN FOR DENSE ONE-SHOT 3D SCAN", "shape": "dot", "size": 10, "title": "An active stereo system that uses a single-colored wave pattern to capture dense 3D scans in a single shot, improving speed and accuracy."}, {"color": "rgb(99, 215, 10)", "font": {"color": "white"}, "id": "CAMERA IMAGES", "label": "CAMERA IMAGES", "shape": "dot", "size": 10, "title": "Photographic captures from one or more cameras used as input for reconstruction.\nImages captured by cameras that contain visual information of a scene"}, {"color": "rgb(99, 215, 10)", "font": {"color": "white"}, "id": "PROJECTED PATTERNS", "label": "PROJECTED PATTERNS", "shape": "dot", "size": 10, "title": "Light patterns emitted by a projector onto a scene, used in structured light systems\nIllumination patterns projected onto a scene to provide additional geometric cues."}, {"color": "rgb(99, 215, 10)", "font": {"color": "white"}, "id": "CONTROLLED LIGHT SOURCES", "label": "CONTROLLED LIGHT SOURCES", "shape": "dot", "size": 10, "title": "Eight programmable light sources used to illuminate the target in a controlled manner\nLight sources whose power is regulated to illuminate a scene in a controlled manner"}, {"color": "rgb(228, 9, 85)", "font": {"color": "white"}, "id": "NARROW BASELINE", "label": "NARROW BASELINE", "shape": "dot", "size": 10, "title": "A configuration where the distance between Cam1 and Cam2 is small, affecting ray intersection accuracy."}, {"color": "rgb(228, 9, 85)", "font": {"color": "white"}, "id": "RAY FROM CAM1", "label": "RAY FROM CAM1", "shape": "dot", "size": 10, "title": "A line of sight emitted from camera 1 used in the reconstruction process."}, {"color": "rgb(84, 114, 219)", "font": {"color": "white"}, "id": "CLIP-BASED SIMILARITY CONSTRAINTS", "label": "CLIP-BASED SIMILARITY CONSTRAINTS", "shape": "dot", "size": 10, "title": "Regularization terms derived from the CLIP model that enforce semantic consistency between rendered images and textual descriptions."}, {"color": "rgb(84, 114, 219)", "font": {"color": "white"}, "id": "DIET-NERF", "label": "DIET-NERF", "shape": "dot", "size": 10, "title": "A NeRF variant that incorporates CLIP-based similarity constraints to improve reconstruction quality with fewer views."}, {"color": "rgb(70, 100, 22)", "font": {"color": "white"}, "id": "OURS", "label": "OURS", "shape": "dot", "size": 10, "title": "The proposed method presented in the paper, achieving competitive Chamfer distances.\nThe proposed neural 3D reconstruction method presented by the authors"}, {"color": "rgb(70, 100, 22)", "font": {"color": "white"}, "id": "CHAMFER DISTANCE", "label": "CHAMFER DISTANCE", "shape": "dot", "size": 10, "title": "A distance measure used to compare the accuracy of reconstructed shapes against ground truth\nA metric used to evaluate the similarity between two point sets, indicating reconstruction error\nA distance metric used to evaluate the similarity between reconstructed and ground truth point clouds."}, {"color": "rgb(70, 100, 22)", "font": {"color": "white"}, "id": "LEGO", "label": "LEGO", "shape": "dot", "size": 10, "title": "A building block toy used as a reference object in the simulation.\nA 3D object category used as a benchmark for reconstruction accuracy.\nA plastic building block toy used as a 3D shape in the reconstruction experiment"}, {"color": "rgb(70, 100, 22)", "font": {"color": "white"}, "id": "HOTDOG", "label": "HOTDOG", "shape": "dot", "size": 10, "title": "A food item used as a reference object in the simulation.\nAnother 3D object category used as a benchmark for reconstruction accuracy.\nA food item referenced as another 3D shape in the reconstruction experiment"}, {"color": "rgb(70, 100, 22)", "font": {"color": "white"}, "id": "NEUS", "label": "NEUS", "shape": "dot", "size": 10, "title": "Neural Surface Reconstruction, a neural method for implicit surface modeling in 3D scene reconstruction.\na neural implicit surface representation used to compute weights\nA neural implicit surface representation method used for 3D scene reconstruction, serving as the core framework in the described workflow.\nA neural implicit surface representation method for 3D scene reconstruction\nA neural implicit surface reconstruction method that uses frequency regularization to improve 3D scene reconstruction.\nNeural implicit surfaces for volume rendering in multi-view reconstruction"}, {"color": "rgb(70, 100, 22)", "font": {"color": "white"}, "id": "DUST3R", "label": "DUST3R", "shape": "dot", "size": 10, "title": "A 3DGS-based method referenced as (h) in the study\nA neural 3D reconstruction method referenced in the figure\nA geometric 3D vision tool that simplifies the creation of 3D models from images, presented at CVPR 2024."}, {"color": "rgb(70, 100, 22)", "font": {"color": "white"}, "id": "CHAMFER DISTANCES", "label": "CHAMFER DISTANCES", "shape": "dot", "size": 10, "title": "A quantitative measure of reconstruction accuracy used to compare methods\nA distance metric used to evaluate reconstruction accuracy\nQuantitative measure of shape similarity between reconstructed and reference models"}, {"color": "rgb(70, 100, 22)", "font": {"color": "white"}, "id": "NEURAL SIGNED DISTANCE FIELDS", "label": "NEURAL SIGNED DISTANCE FIELDS", "shape": "dot", "size": 10, "title": "A mathematical representation used in computer vision to model surfaces by their signed distance from a point, enabling efficient 3D reconstruction.\nA representation of 3D geometry using signed distance functions learned by neural networks, employed for depth reconstruction in structured light systems."}, {"color": "rgb(70, 100, 22)", "font": {"color": "white"}, "id": "ACTIVENEUS", "label": "ACTIVENEUS", "shape": "dot", "size": 10, "title": "A structured light based method cited as [18] for 4D scene reconstruction.\nA neural surface reconstruction method cited as [18]\nAn active learning variant of NeuS for improved surface reconstruction.\nAn active neural surface reconstruction method evaluated\nA neural surface reconstruction method referenced as [18]\nA structured light-based method that achieved comparatively better shape reconstruction but lost high-frequency details\nA neural surface reconstruction method cited as [18]\nA research paper titled \u0027Neural signed distance fields for active stereo\u0027 presented at a 3D vision conference in 2024."}, {"color": "rgb(4, 95, 99)", "font": {"color": "white"}, "id": "YEBIN LIU", "label": "YEBIN LIU", "shape": "dot", "size": 10, "title": "Yebin Liu is a researcher who co-authored a point-cloud-based multiview stereo algorithm for free-viewpoint video."}, {"color": "rgb(4, 95, 99)", "font": {"color": "white"}, "id": "POINT-CLOUD-BASED MULTIVIEW STEREO ALGORITHM", "label": "POINT-CLOUD-BASED MULTIVIEW STEREO ALGORITHM", "shape": "dot", "size": 10, "title": "An algorithm that reconstructs 3D scenes from multiple views using point clouds, aimed at free-viewpoint video."}, {"color": "rgb(32, 135, 241)", "font": {"color": "white"}, "id": "JAMES TOMPKIN", "label": "JAMES TOMPKIN", "shape": "dot", "size": 10, "title": "Researcher and co-author of the 2023 paper on neural fields for structured lighting."}, {"color": "rgb(32, 135, 241)", "font": {"color": "white"}, "id": "NEURAL FIELDS FOR STRUCTURED LIGHTING", "label": "NEURAL FIELDS FOR STRUCTURED LIGHTING", "shape": "dot", "size": 10, "title": "A 2023 IEEE/CVF International Conference on Computer Vision paper presenting neural field techniques for structured lighting applications."}, {"color": "rgb(228, 9, 85)", "font": {"color": "white"}, "id": "NOVEL APPROACH", "label": "NOVEL APPROACH", "shape": "dot", "size": 10, "title": "a new method that combines active lighting to recover 3D shapes"}, {"color": "rgb(228, 9, 85)", "font": {"color": "white"}, "id": "ACTIVE LIGHTING TECHNIQUES", "label": "ACTIVE LIGHTING TECHNIQUES", "shape": "dot", "size": 10, "title": "methods that use controlled illumination to improve 3D reconstruction from sparse views"}, {"color": "rgb(158, 206, 123)", "font": {"color": "white"}, "id": "2008", "label": "2008", "shape": "dot", "size": 10, "title": "Year 2008, the publication year of \u0027Multiview photometric stereo\u0027"}, {"color": "rgb(158, 206, 123)", "font": {"color": "white"}, "id": "MULTIVIEW PHOTOMETRIC STEREO", "label": "MULTIVIEW PHOTOMETRIC STEREO", "shape": "dot", "size": 10, "title": "Multiview photometric stereo is a computer vision technique for 3D reconstruction using multiple images and photometric cues"}, {"color": "rgb(105, 132, 238)", "font": {"color": "white"}, "id": "SURYANSH KUMAR", "label": "SURYANSH KUMAR", "shape": "dot", "size": 10, "title": "Co-author of the 2022 Neural Radiance Fields paper."}, {"color": "rgb(143, 201, 116)", "font": {"color": "white"}, "id": "EIGHT POINT LIGHT", "label": "EIGHT POINT LIGHT", "shape": "dot", "size": 10, "title": "Number of light sources used in the experiment"}, {"color": "rgb(143, 201, 116)", "font": {"color": "white"}, "id": "DYNAMIC SCENE", "label": "DYNAMIC SCENE", "shape": "dot", "size": 10, "title": "A scene that changes over time, e.g., a jumping human\na scene that changes over time, used as a test case for the proposed method"}, {"color": "rgb(70, 100, 22)", "font": {"color": "white"}, "id": "SUPPLEMENTAL VIDEO", "label": "SUPPLEMENTAL VIDEO", "shape": "dot", "size": 10, "title": "An additional video file accompanying the document that shows 4D scene reconstruction results\nA video provided alongside the document to demonstrate 4D scene reconstruction results."}, {"color": "rgb(70, 100, 22)", "font": {"color": "white"}, "id": "4D SCENE RECONSTRUCTION", "label": "4D SCENE RECONSTRUCTION", "shape": "dot", "size": 10, "title": "The process of reconstructing a dynamic scene over time, combining spatial and temporal dimensions, as addressed in the referenced PDF.\nThe process of reconstructing three-dimensional shapes over time, as discussed in the document.\nThe process of reconstructing a three\u2011dimensional scene over time, producing a four\u2011dimensional representation"}, {"color": "rgb(178, 204, 15)", "font": {"color": "white"}, "id": "CORRESPONDENCES", "label": "CORRESPONDENCES", "shape": "dot", "size": 10, "title": "Pairs of matching points or features identified between images taken from different angles.\nEstablished mappings between points across different views or representations, crucial for accurate reconstruction\nPairs of image points that match across different views, enabling the estimation of 3D structure."}, {"color": "rgb(178, 204, 15)", "font": {"color": "white"}, "id": "MULTIVIEW STEREO (MVS)", "label": "MULTIVIEW STEREO (MVS)", "shape": "dot", "size": 10, "title": "A computer vision technique that reconstructs 3D geometry by matching corresponding points across multiple camera views.\nA 3\u2011D reconstruction approach that fuses multiple images from different viewpoints to recover depth\nA computer vision technique that uses multiple images taken from different viewpoints to recover dense 3D geometry of a scene or object."}, {"color": "rgb(178, 204, 15)", "font": {"color": "white"}, "id": "NEURAL REPRESENTATION", "label": "NEURAL REPRESENTATION", "shape": "dot", "size": 10, "title": "A specific instance of a neural representation that is optimized during reconstruction.\nA learned mapping that encodes 3D scene information into a high\u2011dimensional vector space, used to facilitate convergence of the reconstruction process.\nA learned 3D model generated by a neural network, representing the scene or object in a compact form\nA learned, high\u2011dimensional encoding of visual scenes that enables efficient processing and synthesis in neural rendering pipelines."}, {"color": "rgb(130, 83, 45)", "font": {"color": "white"}, "id": "3DGS-BASED METHODS", "label": "3DGS-BASED METHODS", "shape": "dot", "size": 10, "title": "A class of reconstruction algorithms that utilize 3D Gaussian Splatting to generate point clouds"}, {"color": "rgb(149, 32, 107)", "font": {"color": "white"}, "id": "NEURAL GEOMETRY", "label": "NEURAL GEOMETRY", "shape": "dot", "size": 10, "title": "The process of modeling 3D shapes using neural networks to capture geometric details"}, {"color": "rgb(149, 32, 107)", "font": {"color": "white"}, "id": "BRDF RECONSTRUCTION", "label": "BRDF RECONSTRUCTION", "shape": "dot", "size": 10, "title": "The estimation of bidirectional reflectance distribution functions to model how surfaces reflect light"}, {"color": "rgb(149, 32, 107)", "font": {"color": "white"}, "id": "REFLECTIVE OBJECTS", "label": "REFLECTIVE OBJECTS", "shape": "dot", "size": 10, "title": "Objects that exhibit significant specular reflection, often challenging for 3D reconstruction"}, {"color": "rgb(149, 32, 107)", "font": {"color": "white"}, "id": "MULTIVIEW IMAGES", "label": "MULTIVIEW IMAGES", "shape": "dot", "size": 10, "title": "A set of images captured from multiple viewpoints used for 3D reconstruction"}, {"color": "rgb(70, 100, 22)", "font": {"color": "white"}, "id": "RUKUN QIAO", "label": "RUKUN QIAO", "shape": "dot", "size": 10, "title": "A researcher who co\u2011authored the paper \u201cDepth reconstruction with neural signed distance fields in structured light systems.\u201d"}, {"color": "rgb(70, 100, 22)", "font": {"color": "white"}, "id": "HONGBIN ZHA", "label": "HONGBIN ZHA", "shape": "dot", "size": 10, "title": "A researcher who co\u2011authored the paper \u201cDepth reconstruction with neural signed distance fields in structured light systems.\u201d"}, {"color": "rgb(70, 100, 22)", "font": {"color": "white"}, "id": "2024 INTERNATIONAL CONFERENCE ON 3D VISION (3DV)", "label": "2024 INTERNATIONAL CONFERENCE ON 3D VISION (3DV)", "shape": "dot", "size": 10, "title": "An annual academic conference focused on advances in 3D computer vision, where the Activeneus paper was published.\nAn international conference dedicated to 3D vision technologies, where the aforementioned paper was presented in 2024."}, {"color": "rgb(70, 100, 22)", "font": {"color": "white"}, "id": "STRUCTURED LIGHT SYSTEMS", "label": "STRUCTURED LIGHT SYSTEMS", "shape": "dot", "size": 10, "title": "A class of depth sensing systems that project known patterns onto a scene to infer depth information."}, {"color": "rgb(14, 125, 85)", "font": {"color": "white"}, "id": "RAY", "label": "RAY", "shape": "dot", "size": 10, "title": "A straight line along which light travels from a source to a point in space.\nA line of sight along which light propagates from the light source to the camera, parameterized by t."}, {"color": "rgb(14, 125, 85)", "font": {"color": "white"}, "id": "IRRADIANCE", "label": "IRRADIANCE", "shape": "dot", "size": 10, "title": "The amount of light energy received per unit area on a surface\nThe amount of light energy received at a point, used to constrain reconstruction.\nThe power per unit area received by a surface from a light source, constrained in the reconstruction model for projected lights.\nThe power per unit area received from a light source at a given point.\nthe amount of light energy received per unit area from a light source\nThe scalar quantity representing the power of electromagnetic radiation per unit area incident on a surface, used here as a feature in the neural rendering pipeline.\nThe measure of light intensity that decreases monotonically along the channel.\nThe amount of light energy received per unit area, varying along a light ray.\nMeasure of light intensity that influences shadow characteristics\nThe power of light received from the light source per unit area."}, {"color": "rgb(14, 125, 85)", "font": {"color": "white"}, "id": "LIGHT SOURCE", "label": "LIGHT SOURCE", "shape": "dot", "size": 10, "title": "The origin of illumination in a scene, such as a projector or lamp\nAn object or device that emits light used to illuminate the scene during photometric stereo or MVPS measurements.\nAn individual illumination device (e.g., LED, projector) that illuminates the scene during image capture.\nThe source of illumination used in the imaging setup.\nAn emitter of light in the scene, indexed by j\nAn illumination device whose rays interact with the scene and cameras; multiple light sources may be considered.\nThe origin of photons emitting light into the scene.\nAn origin of illumination that emits light to illuminate the scene.\nThe origin of illumination in the scene whose emitted light is attenuated by transmittance.\nA point light source used in the paper, geometrically regarded as a perspective camera for the purposes of calculating Lg.\nThe origin of illumination used in active lighting."}, {"color": "rgb(34, 33, 152)", "font": {"color": "white"}, "id": "MACHINE LEARNING AND SYSTEMS", "label": "MACHINE LEARNING AND SYSTEMS", "shape": "dot", "size": 10, "title": "An annual conference where the implicit geometric regularization paper was presented in 2020."}, {"color": "rgb(34, 33, 152)", "font": {"color": "white"}, "id": "IMPLICIT GEOMETRIC REGULARIZATION", "label": "IMPLICIT GEOMETRIC REGULARIZATION", "shape": "dot", "size": 10, "title": "A technique that imposes smoothness constraints on learned 3D shapes."}, {"color": "rgb(228, 9, 85)", "font": {"color": "white"}, "id": "RAY FROM CAM2", "label": "RAY FROM CAM2", "shape": "dot", "size": 10, "title": "A line of sight emitted from camera 2 used in the reconstruction process."}, {"color": "rgb(228, 9, 85)", "font": {"color": "white"}, "id": "P1", "label": "P1", "shape": "dot", "size": 10, "title": "A specific 3D point where rays from Cam1 and Cam2 intersect or are evaluated.\nA specific projected point used as a reference in the figure.\nA point in the channel that appears with high intensity in the camera image.\nA point where light intensity is unexpectedly high despite attenuation."}, {"color": "rgb(228, 9, 85)", "font": {"color": "white"}, "id": "P2", "label": "P2", "shape": "dot", "size": 10, "title": "A specific 3D point used in the figure to illustrate ray projection differences.\nA second projected point used as a reference in the figure.\nA point in the channel that appears with high intensity in the camera image.\nA point whose observed intensity from the camera is compared to the calculated intensity."}, {"color": "rgb(14, 125, 85)", "font": {"color": "white"}, "id": "PRACTICAL SETUP", "label": "PRACTICAL SETUP", "shape": "dot", "size": 10, "title": "The experimental configuration where light sources are positioned sufficiently far from the object to minimize inter\u2011reflections."}, {"color": "rgb(14, 125, 85)", "font": {"color": "white"}, "id": "INTER\u2011REFLECTIONS", "label": "INTER\u2011REFLECTIONS", "shape": "dot", "size": 10, "title": "Secondary light paths that can violate the monotonic transmittance assumption when sources are close to the object."}, {"color": "rgb(41, 138, 133)", "font": {"color": "white"}, "id": "FIXED CAMERAS", "label": "FIXED CAMERAS", "shape": "dot", "size": 10, "title": "Static camera setups positioned to capture multiple views of a scene, often used to obtain dense viewpoints."}, {"color": "rgb(41, 138, 133)", "font": {"color": "white"}, "id": "DENSE VIEWPOINTS", "label": "DENSE VIEWPOINTS", "shape": "dot", "size": 10, "title": "A large number of camera positions surrounding a scene, providing many perspectives for accurate reconstruction."}, {"color": "rgb(99, 106, 157)", "font": {"color": "white"}, "id": "YVAIN QU\u00b4EAU", "label": "YVAIN QU\u00b4EAU", "shape": "dot", "size": 10, "title": "Author of the 2024 CVPR paper on Rnb-neus."}, {"color": "rgb(73, 36, 85)", "font": {"color": "white"}, "id": "HIGH-PASS FILTERING", "label": "HIGH-PASS FILTERING", "shape": "dot", "size": 10, "title": "A signal processing technique applied to images to reduce low-frequency components, used here to mitigate artifacts."}, {"color": "rgb(73, 36, 85)", "font": {"color": "white"}, "id": "ARTIFACT", "label": "ARTIFACT", "shape": "dot", "size": 10, "title": "Unwanted distortion or error in the demultiplexed images caused by motion."}, {"color": "rgb(73, 36, 85)", "font": {"color": "white"}, "id": "MOTION", "label": "MOTION", "shape": "dot", "size": 10, "title": "Movement occurring during the acquisition of images, violating static scene assumptions."}, {"color": "rgb(73, 36, 85)", "font": {"color": "white"}, "id": "ACQUISITION", "label": "ACQUISITION", "shape": "dot", "size": 10, "title": "The act of capturing images of the scene."}, {"color": "rgb(178, 204, 15)", "font": {"color": "white"}, "id": "SHIRO OKA", "label": "SHIRO OKA", "shape": "dot", "size": 10, "title": "Researcher and co\u2011author of the Nerf\u2011based multi\u2011frame 3D integration paper."}, {"color": "rgb(178, 204, 15)", "font": {"color": "white"}, "id": "TAKAKI IKEDA", "label": "TAKAKI IKEDA", "shape": "dot", "size": 10, "title": "Takaki Ikeda is a researcher cited as an author in the paper \u0027Activeneus: Neural signed\u0027"}, {"color": "rgb(99, 106, 157)", "font": {"color": "white"}, "id": "LILIAN CALVET", "label": "LILIAN CALVET", "shape": "dot", "size": 10, "title": "Author of the 2024 CVPR paper on Rnb-neus."}, {"color": "rgb(178, 204, 15)", "font": {"color": "white"}, "id": "MULTIPLEXED ILLUMINATION TECHNIQUES", "label": "MULTIPLEXED ILLUMINATION TECHNIQUES", "shape": "dot", "size": 10, "title": "Illumination methods that combine multiple light patterns or sources to capture richer spatial information."}, {"color": "rgb(178, 204, 15)", "font": {"color": "white"}, "id": "3D RECONSTRUCTION", "label": "3D RECONSTRUCTION", "shape": "dot", "size": 10, "title": "The process of generating a three\u2011dimensional model of a scene from multiple images or viewpoints.\nThe process of generating a three\u2011dimensional model of a scene from multiple camera images and additional data.\nThe process of generating a three\u2011dimensional model of a scene or object from multiple images or viewpoints.\nThe process of generating a three\u2011dimensional model from captured images\nThe process of generating three-dimensional models from data\nThe process of generating a three\u2011dimensional model from captured images\nThe process of generating a three\u2011dimensional model of an object or scene from image data\nThe output of the reconstruction process that represents spatial geometry in three dimensions.\nThe overall task of recovering a three\u2011dimensional model of a scene or object from image data."}, {"color": "rgb(57, 107, 249)", "font": {"color": "white"}, "id": "CHEN SUN", "label": "CHEN SUN", "shape": "dot", "size": 10, "title": "Author of the paper \u201cRevisiting unreasonable effectiveness of data in deep learning era.\u201d"}, {"color": "rgb(57, 107, 249)", "font": {"color": "white"}, "id": "REVISITING UNREASONABLE EFFECTIVENESS OF DATA IN DEEP LEARNING ERA", "label": "REVISITING UNREASONABLE EFFECTIVENESS OF DATA IN DEEP LEARNING ERA", "shape": "dot", "size": 10, "title": "A 2017 IEEE International Conference on Computer Vision paper exploring the impact of large datasets on deep learning performance."}, {"color": "rgb(32, 135, 241)", "font": {"color": "white"}, "id": "STEREO RADIANCE FIELDS (SRF)", "label": "STEREO RADIANCE FIELDS (SRF)", "shape": "dot", "size": 10, "title": "A method for learning view synthesis from sparse views of novel scenes, presented in a CVPR paper."}, {"color": "rgb(32, 135, 241)", "font": {"color": "white"}, "id": "JULIAN CHIBANE", "label": "JULIAN CHIBANE", "shape": "dot", "size": 10, "title": "A researcher who authored the Stereo Radiance Fields (SRF) paper."}, {"color": "rgb(32, 135, 241)", "font": {"color": "white"}, "id": "AAYUSH BANSAL", "label": "AAYUSH BANSAL", "shape": "dot", "size": 10, "title": "A researcher who authored the SRF paper."}, {"color": "rgb(32, 135, 241)", "font": {"color": "white"}, "id": "VERICA LAZOVA", "label": "VERICA LAZOVA", "shape": "dot", "size": 10, "title": "A researcher who authored the SRF paper."}, {"color": "rgb(32, 135, 241)", "font": {"color": "white"}, "id": "GERARD PONS\u2011MOLL", "label": "GERARD PONS\u2011MOLL", "shape": "dot", "size": 10, "title": "A researcher who authored the SRF paper."}, {"color": "rgb(32, 135, 241)", "font": {"color": "white"}, "id": "IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION", "label": "IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION", "shape": "dot", "size": 10, "title": "Venue where Mip-nerf was presented in 2021\nAn annual conference where the Mvsnerf paper was presented.\nInternational conference on computer vision organized by IEEE and CVF, held annually, venue for many CV research papers\nAnnual computer vision conference where the 2023 neural fields paper was presented.\nThe IEEE/CVF International Conference on Computer Vision (ICCV) is a premier annual conference in computer vision where the Nerfingmvs paper was presented.\nAn annual international conference on computer vision where the Mvpsnet paper was presented in 2023."}, {"color": "rgb(32, 135, 241)", "font": {"color": "white"}, "id": "MVSNERF", "label": "MVSNERF", "shape": "dot", "size": 10, "title": "A fast, generalizable radiance field reconstruction method derived from multi\u2011view stereo data."}, {"color": "rgb(144, 248, 153)", "font": {"color": "white"}, "id": "OBJECT SHAPE ESTIMATION", "label": "OBJECT SHAPE ESTIMATION", "shape": "dot", "size": 10, "title": "The process of determining the 3D geometry of objects within a scene using the neural SDF"}, {"color": "rgb(144, 248, 153)", "font": {"color": "white"}, "id": "NEURAL SDF", "label": "NEURAL SDF", "shape": "dot", "size": 10, "title": "A neural signed distance function model that benefits from the light power prior loss.\nA neural signed distance function used for 3D scene reconstruction, trained with a loss function to estimate object shapes"}, {"color": "rgb(213, 22, 26)", "font": {"color": "white"}, "id": "LIHE YANG", "label": "LIHE YANG", "shape": "dot", "size": 10, "title": "Lihe Yang is a researcher who co-authored the paper Depth anything: Unleashing the power of large-scale unlabeled data."}, {"color": "rgb(213, 22, 26)", "font": {"color": "white"}, "id": "DEPTH ANYTHING", "label": "DEPTH ANYTHING", "shape": "dot", "size": 10, "title": "Depth anything is a method that leverages large-scale unlabeled data to improve depth estimation in computer vision."}, {"color": "rgb(224, 235, 197)", "font": {"color": "white"}, "id": "INTENSITY OF AN IMAGE POINT", "label": "INTENSITY OF AN IMAGE POINT", "shape": "dot", "size": 10, "title": "the brightness value computed from integrating view directions of the camera and light source"}, {"color": "rgb(224, 235, 197)", "font": {"color": "white"}, "id": "VIEW DIRECTIONS", "label": "VIEW DIRECTIONS", "shape": "dot", "size": 10, "title": "the directions from which the camera vk and light source vj observe the point p(t)"}, {"color": "rgb(224, 235, 197)", "font": {"color": "white"}, "id": "CAMERA VK", "label": "CAMERA VK", "shape": "dot", "size": 10, "title": "the camera positioned at direction vk used to capture the image"}, {"color": "rgb(224, 235, 197)", "font": {"color": "white"}, "id": "LIGHT SOURCE VJ", "label": "LIGHT SOURCE VJ", "shape": "dot", "size": 10, "title": "the light source positioned at direction vj emitting illumination"}, {"color": "rgb(154, 161, 89)", "font": {"color": "white"}, "id": "LASER LIGHT SOURCES", "label": "LASER LIGHT SOURCES", "shape": "dot", "size": 10, "title": "Devices that emit laser light used in the experiment.\nSources of laser light used in the experiment, positioned next to cameras, with weaker power than room lights"}, {"color": "rgb(154, 161, 89)", "font": {"color": "white"}, "id": "DIFFRACTIVE OPTICAL ELEMENTS (DOE)", "label": "DIFFRACTIVE OPTICAL ELEMENTS (DOE)", "shape": "dot", "size": 10, "title": "Optical components attached to laser light sources to produce a fixed pattern."}, {"color": "rgb(154, 161, 89)", "font": {"color": "white"}, "id": "EXPERIMENT", "label": "EXPERIMENT", "shape": "dot", "size": 10, "title": "The conducted study involving object illumination and image capture\nThe practical test setup described in the document."}, {"color": "rgb(154, 161, 89)", "font": {"color": "white"}, "id": "ROOM LIGHTS", "label": "ROOM LIGHTS", "shape": "dot", "size": 10, "title": "Sources of illumination that brighten the objects in the scene.\nAmbient lighting in the room, providing stronger illumination than the laser light sources"}, {"color": "rgb(154, 161, 89)", "font": {"color": "white"}, "id": "CHANNELS OF THE LASER LIGHTS", "label": "CHANNELS OF THE LASER LIGHTS", "shape": "dot", "size": 10, "title": "The optical paths or signal channels through which the laser light travels, noted to exhibit noise"}, {"color": "rgb(105, 132, 238)", "font": {"color": "white"}, "id": "PHOTOMETRY CONSTRAINTS", "label": "PHOTOMETRY CONSTRAINTS", "shape": "dot", "size": 10, "title": "Regularization terms that enforce realistic lighting and reflectance properties in the reconstructed scene."}, {"color": "rgb(105, 132, 238)", "font": {"color": "white"}, "id": "REGNERF", "label": "REGNERF", "shape": "dot", "size": 10, "title": "A NeRF variant that introduces geometry and photometry constraints, trained on the JFT-300M dataset.\nRegularized neural radiance fields for view synthesis."}, {"color": "rgb(14, 125, 85)", "font": {"color": "white"}, "id": "PHOTOMETRIC STEREO", "label": "PHOTOMETRIC STEREO", "shape": "dot", "size": 10, "title": "A technique that estimates surface normals by analyzing images taken under different lighting conditions."}, {"color": "rgb(70, 100, 22)", "font": {"color": "white"}, "id": "IRON", "label": "IRON", "shape": "dot", "size": 10, "title": "A specific MVPS method cited as a comparison baseline.\nA 3D scene reconstruction method referenced as [74]\nAn implicit surface reconstruction method focusing on high-resolution details.\nA neural rendering technique included in the comparison set\nA state\u2011of\u2011the\u2011art neural 4D scene reconstruction method referenced as [74]\nA specific MVPS-based method referenced in the study, noted for its failure to recover meaningful shapes under limited views\nA 3D reconstruction method referenced in the study, cited as [74]\nInverse rendering technique that optimizes neural signed distance functions (SDFs) and materials from photometric images"}, {"color": "rgb(70, 100, 22)", "font": {"color": "white"}, "id": "SUPERNORMAL", "label": "SUPERNORMAL", "shape": "dot", "size": 10, "title": "Another MVPS method used for comparison.\nA method referenced as [67] for 4D scene reconstruction using supernormal techniques.\nA 3D reconstruction technique cited as [67]\nA technique that enhances normal estimation in neural rendering.\nA method for normal estimation in 3D reconstruction\nA neural reconstruction method referenced as [67]\nAnother MVPS-based method cited, also struggling with shape recovery due to limited viewpoints and lighting\nA 3D reconstruction technique cited as [67]\nA neural surface reconstruction method that integrates multi-view normals, presented in 2024 at CVPR."}, {"color": "rgb(105, 132, 238)", "font": {"color": "white"}, "id": "JONATHAN T BARRON", "label": "JONATHAN T BARRON", "shape": "dot", "size": 10, "title": "Researcher and co-author of Mip-nerf and Mip-nerf 360\nA researcher mentioned in the document, likely a co\u2011author of a related work.\nResearcher and co\u2011author of the paper \u201cDense depth priors for neural radiance fields from sparse input views.\u201d"}, {"color": "rgb(105, 132, 238)", "font": {"color": "white"}, "id": "MIP-NERF", "label": "MIP-NERF", "shape": "dot", "size": 10, "title": "Multiscale representation for anti-aliasing neural radiance fields"}, {"color": "rgb(105, 132, 238)", "font": {"color": "white"}, "id": "MIP-NERF 360", "label": "MIP-NERF 360", "shape": "dot", "size": 10, "title": "Unbounded anti-aliased neural radiance fields extending Mip-nerf"}, {"color": "rgb(152, 180, 111)", "font": {"color": "white"}, "id": "SHAPE-CONDITIONED RADIANCE FIELDS", "label": "SHAPE-CONDITIONED RADIANCE FIELDS", "shape": "dot", "size": 10, "title": "A neural rendering technique that generates radiance fields conditioned on 3D shape information from a single view."}, {"color": "rgb(152, 180, 111)", "font": {"color": "white"}, "id": "SHARF: SHAPE-CONDITIONED RADIANCE FIELDS FROM A SINGLE VIEW", "label": "SHARF: SHAPE-CONDITIONED RADIANCE FIELDS FROM A SINGLE VIEW", "shape": "dot", "size": 10, "title": "A 2021 ICML paper introducing a method for generating shape\u2011conditioned radiance fields from a single view."}, {"color": "rgb(228, 9, 85)", "font": {"color": "white"}, "id": "LOW-FREQUENCY SHAPE", "label": "LOW-FREQUENCY SHAPE", "shape": "dot", "size": 10, "title": "A coarse geometric representation generated first to improve reconstruction quality when view coverage is sparse."}, {"color": "rgb(228, 9, 85)", "font": {"color": "white"}, "id": "SPARSE VIEWS", "label": "SPARSE VIEWS", "shape": "dot", "size": 10, "title": "few camera angles that make reconstruction difficult\nLimited number of viewpoints used by mv-dust3r+"}, {"color": "rgb(4, 95, 99)", "font": {"color": "white"}, "id": "QIONGHAI DAI", "label": "QIONGHAI DAI", "shape": "dot", "size": 10, "title": "Qionghai Dai is a researcher who co-authored a point-cloud-based multiview stereo algorithm for free-viewpoint video."}, {"color": "rgb(57, 107, 249)", "font": {"color": "white"}, "id": "Y.Y. SCHECHNER", "label": "Y.Y. SCHECHNER", "shape": "dot", "size": 10, "title": "Researcher in computer vision and photometry"}, {"color": "rgb(57, 107, 249)", "font": {"color": "white"}, "id": "IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION", "label": "IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION", "shape": "dot", "size": 10, "title": "ICCV is an annual computer vision conference where the Barf paper was presented.\nAnnual conference on computer vision where the paper on multiplexed illumination was presented\nICCV, the venue for the 2017 paper."}, {"color": "rgb(14, 125, 85)", "font": {"color": "white"}, "id": "SIX CALIBRATED LIGHT SOURCES", "label": "SIX CALIBRATED LIGHT SOURCES", "shape": "dot", "size": 10, "title": "Six light sources with known calibration used to illuminate the scene"}, {"color": "rgb(14, 125, 85)", "font": {"color": "white"}, "id": "EXTERNAL LIGHT", "label": "EXTERNAL LIGHT", "shape": "dot", "size": 10, "title": "Additional illumination source outside the controlled setup"}, {"color": "rgb(103, 156, 25)", "font": {"color": "white"}, "id": "SIGGRAPH 2024 CONFERENCE PAPERS", "label": "SIGGRAPH 2024 CONFERENCE PAPERS", "shape": "dot", "size": 10, "title": "SIGGRAPH 2024 Conference Papers is the proceedings where the paper \u00272d gaussian splatting for geometrically accurate radiance fields\u0027 was presented"}, {"color": "rgb(103, 156, 25)", "font": {"color": "white"}, "id": "2D GAUSSIAN SPLATTING FOR GEOMETRICALLY ACCURATE RADIANCE FIELDS", "label": "2D GAUSSIAN SPLATTING FOR GEOMETRICALLY ACCURATE RADIANCE FIELDS", "shape": "dot", "size": 10, "title": "2d gaussian splatting for geometrically accurate radiance fields is a method for representing radiance fields with Gaussian splats in 2D space"}, {"color": "rgb(103, 156, 25)", "font": {"color": "white"}, "id": "ASSOCIATION FOR COMPUTING MACHINERY", "label": "ASSOCIATION FOR COMPUTING MACHINERY", "shape": "dot", "size": 10, "title": "Association for Computing Machinery (ACM) is the publisher of the SIGGRAPH 2024 Conference Papers"}, {"color": "rgb(103, 156, 25)", "font": {"color": "white"}, "id": "2024", "label": "2024", "shape": "dot", "size": 10, "title": "Year 2024, the publication year of the SIGGRAPH conference proceedings"}, {"color": "rgb(57, 62, 17)", "font": {"color": "white"}, "id": "SZYMON RUSINKIEWICZ", "label": "SZYMON RUSINKIEWICZ", "shape": "dot", "size": 10, "title": "Szymon Rusinkiewicz is a co\u2011author of the dynamic shape capture paper."}, {"color": "rgb(57, 62, 17)", "font": {"color": "white"}, "id": "DYNAMIC SHAPE CAPTURE USING MULTI-VIEW PHOTOMETRIC STEREO", "label": "DYNAMIC SHAPE CAPTURE USING MULTI-VIEW PHOTOMETRIC STEREO", "shape": "dot", "size": 10, "title": "This is a technique for capturing changing 3D shapes by analyzing multiple photometric views."}, {"color": "rgb(70, 100, 22)", "font": {"color": "white"}, "id": "CHAMFER DISTANCE METRIC", "label": "CHAMFER DISTANCE METRIC", "shape": "dot", "size": 10, "title": "A metric used to evaluate the similarity between two point sets, commonly applied in 3D reconstruction to measure the average distance from points in one set to the nearest points in another set."}, {"color": "rgb(70, 100, 22)", "font": {"color": "white"}, "id": "PROPOSED METHOD", "label": "PROPOSED METHOD", "shape": "dot", "size": 10, "title": "A novel reconstruction technique referenced as the baseline for comparison in the document\nA novel algorithm for 4D scene reconstruction that establishes correspondences across wide-baseline views and incorporates irradiance loss to improve accuracy.\nA computational approach for 4D scene reconstruction that incorporates image data captured under controlled illumination and uses an irradiance loss term Lg.\nThe new 3D reconstruction method introduced in the document, evaluated against IRON, SuperNormal, ActiveNeuS, and TurboSL"}, {"color": "rgb(134, 111, 37)", "font": {"color": "white"}, "id": "PRADYUMNA CHARI", "label": "PRADYUMNA CHARI", "shape": "dot", "size": 10, "title": "Pradyumna Chari is an author of the 2023 Arxiv paper Sparsegs."}, {"color": "rgb(134, 111, 37)", "font": {"color": "white"}, "id": "HAOLIN XIONG", "label": "HAOLIN XIONG", "shape": "dot", "size": 10, "title": "Haolin Xiong is an author of the 2023 Arxiv paper Sparsegs: Real-time 360\u00b0 sparse view synthesis using gaussian splatting."}, {"color": "rgb(243, 72, 250)", "font": {"color": "white"}, "id": "JAN-MICHAEL FRAHM", "label": "JAN-MICHAEL FRAHM", "shape": "dot", "size": 10, "title": "Researcher in computer vision and pattern recognition\nJan-Michael Frahm is a researcher who co-authored the paper \"Mvpsnet: Fast generalizable multi-view photometric stereo\" in 2023."}, {"color": "rgb(243, 72, 250)", "font": {"color": "white"}, "id": "IEEE CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION", "label": "IEEE CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION", "shape": "dot", "size": 10, "title": "Conference where the paper on illuminant\u2011camera communication was presented.\nAn international conference organized by the IEEE that focuses on advances in computer vision and pattern recognition, where research papers are presented and peer\u2011reviewed.\nAnnual conference where the structure-from-motion paper was presented\nAnnual conference where the 2017 benchmark paper was presented"}, {"color": "rgb(243, 72, 250)", "font": {"color": "white"}, "id": "3DIMPVT", "label": "3DIMPVT", "shape": "dot", "size": 10, "title": "A conference dedicated to 3D imaging and perception, where works on 3D reconstruction and scanning are presented.\nA 3D image processing method referenced in the text"}, {"color": "rgb(243, 72, 250)", "font": {"color": "white"}, "id": "THOMAS SCHOPS", "label": "THOMAS SCHOPS", "shape": "dot", "size": 10, "title": "Researcher in computer vision and photogrammetry"}, {"color": "rgb(243, 72, 250)", "font": {"color": "white"}, "id": "SILVANO GALLIANI", "label": "SILVANO GALLIANI", "shape": "dot", "size": 10, "title": "Researcher in computer vision and photogrammetry"}, {"color": "rgb(243, 72, 250)", "font": {"color": "white"}, "id": "TORSTEN SATTLER", "label": "TORSTEN SATTLER", "shape": "dot", "size": 10, "title": "Researcher in computer vision and photogrammetry\nResearcher and author of the 2017 multi-view stereo benchmark paper"}, {"color": "rgb(243, 72, 250)", "font": {"color": "white"}, "id": "KONRAD SCHINDLER", "label": "KONRAD SCHINDLER", "shape": "dot", "size": 10, "title": "Researcher in computer vision and photogrammetry\nResearcher and author of the 2017 multi-view stereo benchmark paper"}, {"color": "rgb(243, 72, 250)", "font": {"color": "white"}, "id": "MARC POLLEFEYS", "label": "MARC POLLEFEYS", "shape": "dot", "size": 10, "title": "Researcher in computer vision and photogrammetry\nResearcher and author of the 2017 multi-view stereo benchmark paper"}, {"color": "rgb(70, 100, 22)", "font": {"color": "white"}, "id": "OTHER APPROACHES", "label": "OTHER APPROACHES", "shape": "dot", "size": 10, "title": "Existing techniques for 4D scene reconstruction against which the proposed method is compared."}, {"color": "rgb(99, 215, 10)", "font": {"color": "white"}, "id": "SURFACE TEXTURE", "label": "SURFACE TEXTURE", "shape": "dot", "size": 10, "title": "The fine visual details and micro-structure of an object\u0027s surface that can be enhanced by illumination techniques"}, {"color": "rgb(99, 215, 10)", "font": {"color": "white"}, "id": "STRUCTURED LIGHT (SL)", "label": "STRUCTURED LIGHT (SL)", "shape": "dot", "size": 10, "title": "A specific active-light technique that projects high-frequency patterns onto a scene to capture depth information.\nA method of projecting known light patterns onto a scene to capture depth and surface details"}, {"color": "rgb(136, 133, 36)", "font": {"color": "white"}, "id": "6 LIGHTS", "label": "6 LIGHTS", "shape": "dot", "size": 10, "title": "Number of light sources used in a reconstruction experiment\nSix light sources accompanying the camera setup to illuminate the scene"}, {"color": "rgb(136, 133, 36)", "font": {"color": "white"}, "id": "ABLATION STUDY", "label": "ABLATION STUDY", "shape": "dot", "size": 10, "title": "An analysis removing components to assess their impact"}, {"color": "rgb(136, 133, 36)", "font": {"color": "white"}, "id": "4 CAMERAS", "label": "4 CAMERAS", "shape": "dot", "size": 10, "title": "A set of four cameras used in the 3D scene reconstruction experiment"}, {"color": "rgb(187, 58, 56)", "font": {"color": "white"}, "id": "KAIRUN WEN", "label": "KAIRUN WEN", "shape": "dot", "size": 10, "title": "Researcher and co\u2011author of the Instantsplat paper."}, {"color": "rgb(187, 58, 56)", "font": {"color": "white"}, "id": "INSTANTSPLAT", "label": "INSTANTSPLAT", "shape": "dot", "size": 10, "title": "A 3DGS-based method referenced as (g) in the study\nA fast neural rendering technique that represents scenes with splats for real-time performance.\nA real\u2011time, unbounded, sparse\u2011view, pose\u2011free Gaussian splatting method that renders 3D scenes in approximately 40\u202fseconds."}, {"color": "rgb(70, 100, 22)", "font": {"color": "white"}, "id": "IEEE CONF. COMPUT. VIS. PATTERN RECOG.", "label": "IEEE CONF. COMPUT. VIS. PATTERN RECOG.", "shape": "dot", "size": 10, "title": "Conference where the paper on Iron was presented in 2022"}, {"color": "rgb(213, 22, 26)", "font": {"color": "white"}, "id": "ZILONG HUANG", "label": "ZILONG HUANG", "shape": "dot", "size": 10, "title": "Zilong Huang is a researcher who co-authored the paper Depth anything: Unleashing the power of large-scale unlabeled data."}, {"color": "rgb(99, 215, 10)", "font": {"color": "white"}, "id": "LIGHT SOURCE POSITIONS", "label": "LIGHT SOURCE POSITIONS", "shape": "dot", "size": 10, "title": "The spatial coordinates of each light source, assumed to coincide with the positions of the cameras"}, {"color": "rgb(99, 215, 10)", "font": {"color": "white"}, "id": "CAMERAS", "label": "CAMERAS", "shape": "dot", "size": 10, "title": "Optical devices used to capture visual information; in this context, multiple cameras are employed to reconstruct 4D scenes.\nImaging devices that capture multiple viewpoints of a scene, whose relative positions define the baseline.\nDevices that capture images of the scene, positioned densely to satisfy the minimization requirements.\nThe imaging devices positioned around the scene, noted to be sparsely located.\nEight digital cameras positioned around a target object to capture images from different viewpoints for reconstruction.\nImaging devices placed next to each light source to capture scenes for reconstruction\nNumber of cameras employed in the reconstruction setups\nEight cameras employed to capture images of the mannequin\nDevices used to capture images of the scene simultaneously for reconstruction."}, {"color": "rgb(14, 125, 85)", "font": {"color": "white"}, "id": "LIGHT RAY", "label": "LIGHT RAY", "shape": "dot", "size": 10, "title": "Ray emitted from a light source towards the scene\nA ray emitted from a light source into the scene, used to compute illumination contributions.\nA path along which light travels, used here to describe the direction of illumination.\nThe path along which image data is sampled and processed in the reconstruction pipeline, with additional components added to impose constraints."}, {"color": "rgb(14, 125, 85)", "font": {"color": "white"}, "id": "POSITION", "label": "POSITION", "shape": "dot", "size": 10, "title": "3D coordinate of a point along a ray, denoted p(t)"}, {"color": "rgb(14, 125, 85)", "font": {"color": "white"}, "id": "MLP", "label": "MLP", "shape": "dot", "size": 10, "title": "multi\u2011layer perceptron, a type of neural network used for training\nA multilayer perceptron neural network used to map input features to output colors.\nMulti-Layer Perceptron, a type of neural network used to parameterize the NeRF model.\nMulti\u2011Layer Perceptron used to predict parameters such as intensity ck, position, and transmittance\nMulti\u2011Layer Perceptron, a feed\u2011forward neural network used to compute SDF values and irradiance intensities."}, {"color": "rgb(32, 135, 241)", "font": {"color": "white"}, "id": "IEEE CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION (CVPR)", "label": "IEEE CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION (CVPR)", "shape": "dot", "size": 10, "title": "International annual conference on computer vision and pattern recognition where research papers are presented"}, {"color": "rgb(70, 100, 22)", "font": {"color": "white"}, "id": "IEEE", "label": "IEEE", "shape": "dot", "size": 10, "title": "American Institute of Electrical and Electronics Engineers, publisher of many technical conferences and journals\nInstitute of Electrical and Electronics Engineers, a professional association that publishes conference proceedings."}, {"color": "rgb(32, 135, 241)", "font": {"color": "white"}, "id": "2021", "label": "2021", "shape": "dot", "size": 10, "title": "Year of the Mip-nerf conference presentation\nYear the CVPR conference was held and the thesis was presented\nYear in which the proceedings of the IEEE/CVF International Conference on Computer Vision were published\nYear of publication for the referenced article in the International Journal of Computer Vision.\nYear the Barf paper was presented at ICCV.\nThe year the \u201cpixelnerf\u201d paper was presented at CVPR."}, {"color": "rgb(88, 245, 88)", "font": {"color": "white"}, "id": "SIX CAMERAS", "label": "SIX CAMERAS", "shape": "dot", "size": 10, "title": "Six cameras positioned around the object to capture input images"}, {"color": "rgb(88, 245, 88)", "font": {"color": "white"}, "id": "INPUT IMAGE", "label": "INPUT IMAGE", "shape": "dot", "size": 10, "title": "The composite image captured from a single viewpoint, consisting of nine channels corresponding to different light sources\nCaptured image from a camera, containing seven channels due to lighting conditions"}, {"color": "rgb(154, 161, 89)", "font": {"color": "white"}, "id": "INPUT IMAGES", "label": "INPUT IMAGES", "shape": "dot", "size": 10, "title": "Captured visual data from the experiment, consisting of nine channels."}, {"color": "rgb(154, 161, 89)", "font": {"color": "white"}, "id": "NINE CHANNELS", "label": "NINE CHANNELS", "shape": "dot", "size": 10, "title": "The number of distinct image channels used in the input data."}, {"color": "rgb(105, 132, 238)", "font": {"color": "white"}, "id": "JONATHAN T. BARRON", "label": "JONATHAN T. BARRON", "shape": "dot", "size": 10, "title": "Researcher and author of the Nerf paper\nCo\u2011author of the Regnerf paper, known for work on neural rendering.\nResearcher and author of the Hypernerf paper"}, {"color": "rgb(105, 132, 238)", "font": {"color": "white"}, "id": "NERF", "label": "NERF", "shape": "dot", "size": 10, "title": "Neural Radiance Fields, a neural rendering method that represents scenes as continuous volumetric functions for high\u2011fidelity view synthesis.\nA neural radiance field model used for view synthesis and 3D scene reconstruction, originally designed to generate novel views from sparse inputs\nA neural radiance field model used for 3D scene representation and rendering\nNeural Radiance Fields, a deep learning model for novel view synthesis.\nNeural Radiance Fields, a method for representing scenes as neural radiance fields for view synthesis"}, {"color": "rgb(105, 132, 238)", "font": {"color": "white"}, "id": "HYPERNERF", "label": "HYPERNERF", "shape": "dot", "size": 10, "title": "A higher-dimensional representation for topologically varying neural radiance fields, presented in 2021"}, {"color": "rgb(70, 100, 22)", "font": {"color": "white"}, "id": "SIMULATION MODELS", "label": "SIMULATION MODELS", "shape": "dot", "size": 10, "title": "The reference models used to generate ground truth data for comparison"}, {"color": "rgb(70, 100, 22)", "font": {"color": "white"}, "id": "GROUND TRUTH", "label": "GROUND TRUTH", "shape": "dot", "size": 10, "title": "The reference data against which the accuracy of reconstructed shapes is compared.\nThe true simulation models against which reconstructed shapes are evaluated\nThe accurate 3D model against which reconstructions are compared.\nthe accurate, real-world 3D data against which reconstructions are compared\nThe reference shape obtained from the 3D scanner, used as a benchmark for evaluating reconstruction accuracy"}, {"color": "rgb(158, 206, 123)", "font": {"color": "white"}, "id": "GEORGE VOGIATZIS", "label": "GEORGE VOGIATZIS", "shape": "dot", "size": 10, "title": "Researcher cited in the document.\nGeorge Vogiatzis is a researcher cited as an author in the paper \u0027Multiview photometric stereo\u0027"}, {"color": "rgb(185, 21, 66)", "font": {"color": "white"}, "id": "NEURAL IMPLICIT REPRESENTATION METHODS", "label": "NEURAL IMPLICIT REPRESENTATION METHODS", "shape": "dot", "size": 10, "title": "Approaches that use neural networks to learn continuous 3D representations such as voxel densities or signed distance fields."}, {"color": "rgb(178, 204, 15)", "font": {"color": "white"}, "id": "4D SCENE", "label": "4D SCENE", "shape": "dot", "size": 10, "title": "A spatial scene with three spatial dimensions plus time or another parameter, enabling dynamic reconstruction\nA spatio\u2011temporal representation that extends 3D reconstruction by incorporating the time dimension, capturing dynamic changes."}, {"color": "rgb(178, 204, 15)", "font": {"color": "white"}, "id": "SURFACE POINTS", "label": "SURFACE POINTS", "shape": "dot", "size": 10, "title": "Discrete 3D coordinates on the object\u0027s surface derived through triangulation."}, {"color": "rgb(178, 204, 15)", "font": {"color": "white"}, "id": "TRIANGULATION", "label": "TRIANGULATION", "shape": "dot", "size": 10, "title": "A mathematical process that computes 3D points by intersecting lines of sight from multiple images."}, {"color": "rgb(178, 204, 15)", "font": {"color": "white"}, "id": "RECONSTRUCTION ERRORS", "label": "RECONSTRUCTION ERRORS", "shape": "dot", "size": 10, "title": "Inaccuracies or deviations that occur during the 3D reconstruction process."}, {"color": "rgb(178, 204, 15)", "font": {"color": "white"}, "id": "CORRESPONDENCE", "label": "CORRESPONDENCE", "shape": "dot", "size": 10, "title": "The matching of points or features across multiple images in multi-view stereo (MVS) processing."}, {"color": "rgb(178, 204, 15)", "font": {"color": "white"}, "id": "FEATURES", "label": "FEATURES", "shape": "dot", "size": 10, "title": "Attributes or characteristics extracted from data, used to identify correspondences between views."}, {"color": "rgb(222, 222, 124)", "font": {"color": "white"}, "id": "\u03a1(P)", "label": "\u03a1(P)", "shape": "dot", "size": 10, "title": "The density at point p derived from the SDF via the sigmoid-based \u03c6."}, {"color": "rgb(222, 222, 124)", "font": {"color": "white"}, "id": "F(P)", "label": "F(P)", "shape": "dot", "size": 10, "title": "A signed distance function (SDF) value at point p, where f(p)=0 defines the surface."}, {"color": "rgb(32, 135, 241)", "font": {"color": "white"}, "id": "INDOOR MULTI\u2011VIEW STEREO", "label": "INDOOR MULTI\u2011VIEW STEREO", "shape": "dot", "size": 10, "title": "Indoor multi\u2011view stereo refers to reconstructing 3D geometry of indoor scenes from multiple camera views."}, {"color": "rgb(32, 135, 241)", "font": {"color": "white"}, "id": "NERFINGMVS", "label": "NERFINGMVS", "shape": "dot", "size": 10, "title": "Nerfingmvs is a method that guides the optimization of neural radiance fields for indoor multi\u2011view stereo reconstruction."}, {"color": "rgb(14, 125, 85)", "font": {"color": "white"}, "id": "OPAQUE DENSITY FUNCTION", "label": "OPAQUE DENSITY FUNCTION", "shape": "dot", "size": 10, "title": "\u03c1(p)=\u03c6(f(p)), mapping SDF values to density values."}, {"color": "rgb(14, 125, 85)", "font": {"color": "white"}, "id": "SDF", "label": "SDF", "shape": "dot", "size": 10, "title": "Signed Distance Function, a mathematical representation used to define surfaces in 3D space\nSigned Distance Function, a mathematical representation of surface distance used to compute density.\nSigned Distance Function, a scalar field that gives the signed distance from any point in space to the nearest surface, used for geometry representation.\nSigned Distance Function, a scalar field whose zero level set represents the surface geometry."}, {"color": "rgb(14, 125, 85)", "font": {"color": "white"}, "id": "GJ", "label": "GJ", "shape": "dot", "size": 10, "title": "Per\u2011ray irradiance loss term, computed as cj(p(t), vjp) times the Euclidean distance between predicted and ground\u2011truth positions"}, {"color": "rgb(14, 125, 85)", "font": {"color": "white"}, "id": "LG", "label": "LG", "shape": "dot", "size": 10, "title": "Loss function to be minimized in neural 4D scene reconstruction, defined as an integral over light sources and rays\nA loss term (e.g., gradient or regularization loss) weighted by \u03b1.\nLg is a luminance or light gain value calculated only for the channel along the ray from the light source that is the sole illuminator of the channel.\nAbbreviation for a lighting configuration or system used in the experiments"}, {"color": "rgb(14, 125, 85)", "font": {"color": "white"}, "id": "SG", "label": "SG", "shape": "dot", "size": 10, "title": "Stop\u2011gradient operation that prevents gradients from flowing through its argument"}, {"color": "rgb(14, 125, 85)", "font": {"color": "white"}, "id": "TJ", "label": "TJ", "shape": "dot", "size": 10, "title": "transmittance term associated with the light source direction\nTransmittance function for the j\u2011th light source, representing how much light reaches a point along a ray"}, {"color": "rgb(105, 132, 238)", "font": {"color": "white"}, "id": "VITTORIO FERARI", "label": "VITTORIO FERARI", "shape": "dot", "size": 10, "title": "Co-author of the 2022 Neural Radiance Fields paper."}, {"color": "rgb(92, 98, 3)", "font": {"color": "white"}, "id": "SIGMOID FUNCTION", "label": "SIGMOID FUNCTION", "shape": "dot", "size": 10, "title": "The logistic function \u03c6(x)=se^{-sx}/(1+se^{-sx})^2 used to convert SDF values into density."}, {"color": "rgb(92, 98, 3)", "font": {"color": "white"}, "id": "\u03a6(X)", "label": "\u03a6(X)", "shape": "dot", "size": 10, "title": "The specific sigmoid-based function applied to f(p) to obtain \u03c1(p).\nA sigmoid-like function defined as \u03c6(x) = se^(\u2212sx)/(1 + se^(\u2212sx))\u00b2, where s is a scale parameter."}, {"color": "rgb(92, 98, 3)", "font": {"color": "white"}, "id": "SCALE PARAMETER", "label": "SCALE PARAMETER", "shape": "dot", "size": 10, "title": "The parameter s in the sigmoid function that controls the steepness of the transition."}, {"color": "rgb(236, 30, 243)", "font": {"color": "white"}, "id": "CHUNYU LI", "label": "CHUNYU LI", "shape": "dot", "size": 10, "title": "Researcher and author contributing to the paper on multi\u2011view neural surface reconstruction with structured light."}, {"color": "rgb(236, 30, 243)", "font": {"color": "white"}, "id": "MULTI-VIEW NEURAL SURFACE RECONSTRUCTION WITH STRUCTURED LIGHT", "label": "MULTI-VIEW NEURAL SURFACE RECONSTRUCTION WITH STRUCTURED LIGHT", "shape": "dot", "size": 10, "title": "A study presenting a neural network approach for reconstructing 3D surfaces from multiple views using structured light illumination."}, {"color": "rgb(105, 132, 238)", "font": {"color": "white"}, "id": "REFLECTION PARAMETER ESTIMATION", "label": "REFLECTION PARAMETER ESTIMATION", "shape": "dot", "size": 10, "title": "The process of estimating parameters that describe how surfaces reflect light"}, {"color": "rgb(143, 201, 116)", "font": {"color": "white"}, "id": "NERS", "label": "NERS", "shape": "dot", "size": 10, "title": "A neural reconstruction method that assumes spherical topology for objects\nA NeRF-based method referenced as (e) in the study\nA neural radiance field-based approach for 3D scene reconstruction that incorporates surface normals.\nNeural reflectance surfaces for sparse-view 3D reconstruction in the wild, a method for reconstructing 3D scenes from limited viewpoints"}, {"color": "rgb(143, 201, 116)", "font": {"color": "white"}, "id": "SPHERICAL TOPOLOGY", "label": "SPHERICAL TOPOLOGY", "shape": "dot", "size": 10, "title": "A geometric assumption that objects are roughly spherical, simplifying shape modeling"}, {"color": "rgb(105, 132, 238)", "font": {"color": "white"}, "id": "RECONSTRUCTED SHAPES", "label": "RECONSTRUCTED SHAPES", "shape": "dot", "size": 10, "title": "The 3D geometries produced by the methods under evaluation"}, {"color": "rgb(143, 201, 116)", "font": {"color": "white"}, "id": "JASON ZHANG", "label": "JASON ZHANG", "shape": "dot", "size": 10, "title": "Author of the paper \"Ners: Neural reflectance surfaces for sparse-view 3D reconstruction in the wild\""}, {"color": "rgb(143, 201, 116)", "font": {"color": "white"}, "id": "GENGSHAN YANG", "label": "GENGSHAN YANG", "shape": "dot", "size": 10, "title": "Researcher and co-author of the 2023 paper on Total-recon.\nCo-author of the paper \"Ners: Neural reflectance surfaces for sparse-view 3D reconstruction in the wild\""}, {"color": "rgb(143, 201, 116)", "font": {"color": "white"}, "id": "SHUBHAM TULSIANI", "label": "SHUBHAM TULSIANI", "shape": "dot", "size": 10, "title": "Co-author of the paper \"Ners: Neural reflectance surfaces for sparse-view 3D reconstruction in the wild\""}, {"color": "rgb(143, 201, 116)", "font": {"color": "white"}, "id": "DEVA RAMANAN", "label": "DEVA RAMANAN", "shape": "dot", "size": 10, "title": "Researcher and co-author of the 2023 paper on Total-recon.\nCo-author of the paper \"Ners: Neural reflectance surfaces for sparse-view 3D reconstruction in the wild\""}, {"color": "rgb(70, 100, 22)", "font": {"color": "white"}, "id": "ANALYSIS AND MACHINE INTELLIGENCE", "label": "ANALYSIS AND MACHINE INTELLIGENCE", "shape": "dot", "size": 10, "title": "Publication venue cited in the document, likely a conference or journal in 2022"}, {"color": "rgb(239, 230, 71)", "font": {"color": "white"}, "id": "TAKETOMI TAKAFUMI", "label": "TAKETOMI TAKAFUMI", "shape": "dot", "size": 10, "title": "Taketomi Takafumi is an author of the 2024 CVPR paper Supernormal."}, {"color": "rgb(239, 230, 71)", "font": {"color": "white"}, "id": "CAO XU", "label": "CAO XU", "shape": "dot", "size": 10, "title": "Cao Xu is an author of the 2024 CVPR paper Supernormal: Neural surface reconstruction via multi-view normal integration."}, {"color": "rgb(105, 132, 238)", "font": {"color": "white"}, "id": "BERK KAYA", "label": "BERK KAYA", "shape": "dot", "size": 10, "title": "Researcher and author of the 2022 paper on Neural Radiance Fields for deep multi-view photometric stereo."}, {"color": "rgb(63, 203, 156)", "font": {"color": "white"}, "id": "MESH AND VOLUME REPRESENTATIONS", "label": "MESH AND VOLUME REPRESENTATIONS", "shape": "dot", "size": 10, "title": "Hybrid 3D representations combining discrete meshes with continuous volumetric data"}, {"color": "rgb(63, 203, 156)", "font": {"color": "white"}, "id": "CERKEZI AND FAVARO", "label": "CERKEZI AND FAVARO", "shape": "dot", "size": 10, "title": "Researchers who proposed object\u2011centric ray sampling techniques"}, {"color": "rgb(63, 203, 156)", "font": {"color": "white"}, "id": "OBJECT\u2011CENTRIC RAY SAMPLING", "label": "OBJECT\u2011CENTRIC RAY SAMPLING", "shape": "dot", "size": 10, "title": "A sampling strategy that focuses rays around object regions to improve efficiency"}, {"color": "rgb(65, 49, 93)", "font": {"color": "white"}, "id": "22H00545", "label": "22H00545", "shape": "dot", "size": 10, "title": "Another grant identifier supporting the research.\nJapanese research grant identifier 22H00545 supporting the study"}, {"color": "rgb(65, 49, 93)", "font": {"color": "white"}, "id": "JAPAN", "label": "JAPAN", "shape": "dot", "size": 10, "title": "Country where the funding agencies are based and where the research was conducted.\nCountry where the grants are awarded"}, {"color": "rgb(14, 125, 85)", "font": {"color": "white"}, "id": "INTENSITY CK", "label": "INTENSITY CK", "shape": "dot", "size": 10, "title": "Scalar value representing the intensity of the j\u2011th light source"}, {"color": "rgb(178, 204, 15)", "font": {"color": "white"}, "id": "KAZUTO ICHIMARU", "label": "KAZUTO ICHIMARU", "shape": "dot", "size": 10, "title": "Kazuto Ichimaru is a researcher cited as an author in the paper \u0027Activeneus: Neural signed\u0027"}, {"color": "rgb(178, 204, 15)", "font": {"color": "white"}, "id": "TAKAFUMI IWAGUCHI", "label": "TAKAFUMI IWAGUCHI", "shape": "dot", "size": 10, "title": "Co-author and researcher at Kyushu University\nTakafumi Iwaguchi is a researcher cited as an author in the paper \u0027Activeneus: Neural signed\u0027"}, {"color": "rgb(105, 132, 238)", "font": {"color": "white"}, "id": "LUC VAN GOOL", "label": "LUC VAN GOOL", "shape": "dot", "size": 10, "title": "Co-author of the 2022 Neural Radiance Fields paper, prominent computer vision researcher."}, {"color": "rgb(70, 100, 22)", "font": {"color": "white"}, "id": "ARXIV", "label": "ARXIV", "shape": "dot", "size": 10, "title": "An open-access repository for scholarly articles in physics, mathematics, computer science, and related fields.\nAn open-access preprint repository where the 2023 Sparsegs paper was published.\nAn open-access repository for preprints in science and engineering where the Open3D paper was published in 2018."}, {"color": "rgb(70, 100, 22)", "font": {"color": "white"}, "id": "IBRNET", "label": "IBRNET", "shape": "dot", "size": 10, "title": "A deep learning framework for learning multi-view image-based rendering, presented at CVPR 2021."}, {"color": "rgb(70, 100, 22)", "font": {"color": "white"}, "id": "SPARSEGS", "label": "SPARSEGS", "shape": "dot", "size": 10, "title": "A sparse Gaussian splatting method referenced in the paper for generating mesh surfaces from point clouds.\nA sparse geometry splatting approach for neural scene representation.\nA sparse geometry synthesis approach used for comparison\nA real-time 360\u00b0 sparse view synthesis method using Gaussian splatting, presented in 2023 on Arxiv."}, {"color": "rgb(70, 100, 22)", "font": {"color": "white"}, "id": "OPEN3D", "label": "OPEN3D", "shape": "dot", "size": 10, "title": "An open-source library for 3D data processing, used here to implement TSDF fusion.\nOpen3D is a modern open-source library for processing 3D data, released in 2018 and available on arXiv."}, {"color": "rgb(57, 62, 17)", "font": {"color": "white"}, "id": "NEURAL RADIANCE FIELD", "label": "NEURAL RADIANCE FIELD", "shape": "dot", "size": 10, "title": "a neural network model that predicts color and density for 3D points\nA neural network model that represents 3D scenes by predicting radiance values for any point and viewing direction."}, {"color": "rgb(175, 51, 143)", "font": {"color": "white"}, "id": "SNR", "label": "SNR", "shape": "dot", "size": 10, "title": "Signal\u2011to\u2011Noise Ratio, a measure of signal quality relative to background noise."}, {"color": "rgb(175, 51, 143)", "font": {"color": "white"}, "id": "HADAMARD-BASED MULTIPLEXING", "label": "HADAMARD-BASED MULTIPLEXING", "shape": "dot", "size": 10, "title": "A signal encoding technique that uses Hadamard matrices to combine multiple illumination sources, enabling improved signal\u2011to\u2011noise ratio in imaging systems."}, {"color": "rgb(144, 248, 153)", "font": {"color": "white"}, "id": "LIGHT POWER PRIOR", "label": "LIGHT POWER PRIOR", "shape": "dot", "size": 10, "title": "A prior assumption about the distribution of light power used as a loss function."}, {"color": "rgb(144, 248, 153)", "font": {"color": "white"}, "id": "SURFACE REFLECTION", "label": "SURFACE REFLECTION", "shape": "dot", "size": 10, "title": "The reflection of light from a surface, used here as a basis for loss calculation."}, {"color": "rgb(178, 204, 15)", "font": {"color": "white"}, "id": "STRONG EXTERNAL LIGHT", "label": "STRONG EXTERNAL LIGHT", "shape": "dot", "size": 10, "title": "High intensity ambient lighting that can interfere with visual perception and imaging."}, {"color": "rgb(178, 204, 15)", "font": {"color": "white"}, "id": "ILLUMINANT\u2011CAMERA COMMUNICATION", "label": "ILLUMINANT\u2011CAMERA COMMUNICATION", "shape": "dot", "size": 10, "title": "A technique that uses communication between light sources and cameras to track moving objects under strong external illumination."}, {"color": "rgb(65, 49, 93)", "font": {"color": "white"}, "id": "JST PARKS2025 04", "label": "JST PARKS2025 04", "shape": "dot", "size": 10, "title": "A Japanese research funding program that supported this work."}, {"color": "rgb(134, 111, 37)", "font": {"color": "white"}, "id": "SAIRISHEEK MUTTUKURU", "label": "SAIRISHEEK MUTTUKURU", "shape": "dot", "size": 10, "title": "Sairisheek Muttukuru is an author of the 2023 Arxiv paper Sparsegs."}, {"color": "rgb(134, 111, 37)", "font": {"color": "white"}, "id": "RISHI UPADHYAY", "label": "RISHI UPADHYAY", "shape": "dot", "size": 10, "title": "Rishi Upadhyay is an author of the 2023 Arxiv paper Sparsegs."}, {"color": "rgb(134, 111, 37)", "font": {"color": "white"}, "id": "ACHUTA KADAMBI", "label": "ACHUTA KADAMBI", "shape": "dot", "size": 10, "title": "Achuta Kadambi is an author of the 2023 Arxiv paper Sparsegs."}, {"color": "rgb(34, 33, 152)", "font": {"color": "white"}, "id": "LEARNING SHAPES", "label": "LEARNING SHAPES", "shape": "dot", "size": 10, "title": "The process of training models to predict or generate 3D geometry."}, {"color": "rgb(14, 125, 85)", "font": {"color": "white"}, "id": "DENSITY", "label": "DENSITY", "shape": "dot", "size": 10, "title": "Spatial occupancy value predicted for a 3D point\nA scalar value at each 3D point indicating material opacity or occupancy in volumetric representation.\nThe concentration of matter or particles that can absorb or scatter light, affecting intensity."}, {"color": "rgb(14, 125, 85)", "font": {"color": "white"}, "id": "VOLUME RENDERING", "label": "VOLUME RENDERING", "shape": "dot", "size": 10, "title": "A rendering technique that integrates density along rays to produce images from volumetric data.\nthe process of generating images from volumetric data\nA rendering technique that integrates color and opacity along rays through a volumetric representation.\nThe rendering technique that accumulates transmittance along rays to produce images, used here to compute Lg.\nTechnique to render 3D volumes by integrating along rays"}, {"color": "rgb(14, 125, 85)", "font": {"color": "white"}, "id": "3D POINT", "label": "3D POINT", "shape": "dot", "size": 10, "title": "A specific location in three\u2011dimensional space\nA specific location in three-dimensional space within the volumetric model.\na spatial coordinate in three dimensions\nA point in three\u2011dimensional space denoted as p(t)."}, {"color": "rgb(178, 204, 15)", "font": {"color": "white"}, "id": "VOLUMETRIC REPRESENTATION", "label": "VOLUMETRIC REPRESENTATION", "shape": "dot", "size": 10, "title": "A way to encode 3D scenes using density values over a volume, enabling rendering and reconstruction."}, {"color": "rgb(14, 125, 85)", "font": {"color": "white"}, "id": "P3", "label": "P3", "shape": "dot", "size": 10, "title": "A specific 3D point associated with irradiance measurement in the figure.\nA specific point in the scene where density and cj(P3, vjP3) are evaluated."}, {"color": "rgb(32, 135, 241)", "font": {"color": "white"}, "id": "WANG ZHAO", "label": "WANG ZHAO", "shape": "dot", "size": 10, "title": "Wang Zhao is a researcher who co\u2011authored the paper \u201cNerfingmvs: Guided optimization of neural radiance fields for indoor multi\u2011view stereo.\u201d"}, {"color": "rgb(99, 106, 157)", "font": {"color": "white"}, "id": "LLUKMAN CERKEZI", "label": "LLUKMAN CERKEZI", "shape": "dot", "size": 10, "title": "Author of the 2024 CVPR paper on sparse 3D reconstruction."}, {"color": "rgb(57, 92, 92)", "font": {"color": "white"}, "id": "Y. YAGI", "label": "Y. YAGI", "shape": "dot", "size": 10, "title": "Co-author of the 2007 paper on multiplexed illumination for BRDF measurement."}, {"color": "rgb(57, 92, 92)", "font": {"color": "white"}, "id": "MULTIPLEXED ILLUMINATION FOR MEASURING BRDF USING AN ELLIPSOIDAL MIRROR AND A PROJECTOR", "label": "MULTIPLEXED ILLUMINATION FOR MEASURING BRDF USING AN ELLIPSOIDAL MIRROR AND A PROJECTOR", "shape": "dot", "size": 10, "title": "A photometric technique that uses multiple light sources and an ellipsoidal mirror to capture bidirectional reflectance distribution functions (BRDF) efficiently."}, {"color": "rgb(105, 132, 238)", "font": {"color": "white"}, "id": "VICKIE YE", "label": "VICKIE YE", "shape": "dot", "size": 10, "title": "Co\u2011author of the \u201cpixelnerf\u201d paper."}, {"color": "rgb(105, 132, 238)", "font": {"color": "white"}, "id": "PIXELNERF", "label": "PIXELNERF", "shape": "dot", "size": 10, "title": "A neural radiance field (NeRF) method that reconstructs scenes from a single or few images."}, {"color": "rgb(57, 92, 92)", "font": {"color": "white"}, "id": "ASIAN CONFERENCE ON COMPUTER VISION", "label": "ASIAN CONFERENCE ON COMPUTER VISION", "shape": "dot", "size": 10, "title": "Regional academic conference where the 2007 multiplexed illumination paper was presented."}, {"color": "rgb(14, 125, 85)", "font": {"color": "white"}, "id": "NORMAL VECTOR", "label": "NORMAL VECTOR", "shape": "dot", "size": 10, "title": "A unit vector perpendicular to a surface at a point, indicating surface orientation, used as an input feature for shading."}, {"color": "rgb(143, 201, 116)", "font": {"color": "white"}, "id": "DEMULTIPLEXING ILLUMINATION", "label": "DEMULTIPLEXING ILLUMINATION", "shape": "dot", "size": 10, "title": "Technique to separate multiple illumination channels for imaging"}, {"color": "rgb(143, 201, 116)", "font": {"color": "white"}, "id": "OBSERVATION OF WHOLE SCENE WITH SPARSE CAMERAS AND LIGHT SOURCES", "label": "OBSERVATION OF WHOLE SCENE WITH SPARSE CAMERAS AND LIGHT SOURCES", "shape": "dot", "size": 10, "title": "Approach to capture a full scene using few cameras and light points"}, {"color": "rgb(143, 201, 116)", "font": {"color": "white"}, "id": "ILLUMINATING A TARGET SIMULTANEOUSLY WITH LIGHTS", "label": "ILLUMINATING A TARGET SIMULTANEOUSLY WITH LIGHTS", "shape": "dot", "size": 10, "title": "Method of using multiple light sources concurrently to capture images"}, {"color": "rgb(143, 201, 116)", "font": {"color": "white"}, "id": "MULTI-CHANNEL IMAGES", "label": "MULTI-CHANNEL IMAGES", "shape": "dot", "size": 10, "title": "Images captured under different illumination conditions for reconstruction"}, {"color": "rgb(143, 201, 116)", "font": {"color": "white"}, "id": "DEPTH-REGULARIZED OPTIMIZATION", "label": "DEPTH-REGULARIZED OPTIMIZATION", "shape": "dot", "size": 10, "title": "A technique that incorporates depth information to improve 3D Gaussian splatting from few-shot images"}, {"color": "rgb(143, 201, 116)", "font": {"color": "white"}, "id": "DYNAMIC SHAPE CAPTURE", "label": "DYNAMIC SHAPE CAPTURE", "shape": "dot", "size": 10, "title": "The process of capturing changing shapes of objects using imaging techniques"}, {"color": "rgb(105, 132, 238)", "font": {"color": "white"}, "id": "COMPUTER VISION", "label": "COMPUTER VISION", "shape": "dot", "size": 10, "title": "The field of study that enables computers to interpret and process visual information.\nThe field of enabling computers to interpret and understand visual information from images and videos."}, {"color": "rgb(70, 100, 22)", "font": {"color": "white"}, "id": "QIANQIAN WANG", "label": "QIANQIAN WANG", "shape": "dot", "size": 10, "title": "Co\u2011author of the Ibrnet paper, contributing to multi\u2011view rendering research."}, {"color": "rgb(57, 62, 17)", "font": {"color": "white"}, "id": "WENQI YANG", "label": "WENQI YANG", "shape": "dot", "size": 10, "title": "Wenqi Yang is a researcher who co-authored the paper Ps-nerf: Neural inverse rendering for multi-view photometric stereo."}, {"color": "rgb(228, 9, 85)", "font": {"color": "white"}, "id": "CAMERA", "label": "CAMERA", "shape": "dot", "size": 10, "title": "The imaging device that captures rays for 3D reconstruction.\nThe imaging device capturing the scene, used to observe intensity at P2.\nA device that captures images from different viewpoints in the simulation."}, {"color": "rgb(228, 9, 85)", "font": {"color": "white"}, "id": "LIGHT", "label": "LIGHT", "shape": "dot", "size": 10, "title": "The illumination source used in conjunction with the camera to capture scene information.\nPhotons traveling from the light source, subject to attenuation by density."}, {"color": "rgb(243, 72, 250)", "font": {"color": "white"}, "id": "MULTI-VIEW STEREO RECONSTRUCTION ALGORITHMS", "label": "MULTI-VIEW STEREO RECONSTRUCTION ALGORITHMS", "shape": "dot", "size": 10, "title": "Algorithms that reconstruct 3D scenes from multiple images or videos"}, {"color": "rgb(243, 72, 250)", "font": {"color": "white"}, "id": "STEVEN M SEITZ", "label": "STEVEN M SEITZ", "shape": "dot", "size": 10, "title": "Researcher and author of the 2006 multi-view stereo reconstruction algorithms comparison paper"}, {"color": "rgb(243, 72, 250)", "font": {"color": "white"}, "id": "BRIAN CURLESS", "label": "BRIAN CURLESS", "shape": "dot", "size": 10, "title": "Researcher and author of the 2006 multi-view stereo reconstruction algorithms comparison paper"}, {"color": "rgb(243, 72, 250)", "font": {"color": "white"}, "id": "JAMES DIEBEL", "label": "JAMES DIEBEL", "shape": "dot", "size": 10, "title": "Researcher and author of the 2006 multi-view stereo reconstruction algorithms comparison paper"}, {"color": "rgb(243, 72, 250)", "font": {"color": "white"}, "id": "DANIEL SCHARSTEIN", "label": "DANIEL SCHARSTEIN", "shape": "dot", "size": 10, "title": "Researcher and author of the 2006 multi-view stereo reconstruction algorithms comparison paper"}, {"color": "rgb(243, 72, 250)", "font": {"color": "white"}, "id": "RICHARD SZELISKI", "label": "RICHARD SZELISKI", "shape": "dot", "size": 10, "title": "Researcher and author of the 2006 multi-view stereo reconstruction algorithms comparison paper"}, {"color": "rgb(243, 72, 250)", "font": {"color": "white"}, "id": "CVPR\u002706", "label": "CVPR\u002706", "shape": "dot", "size": 10, "title": "2006 IEEE Computer Society conference on computer vision and pattern recognition where the comparison paper was presented"}, {"color": "rgb(243, 72, 250)", "font": {"color": "white"}, "id": "MULTI-VIEW STEREO BENCHMARK", "label": "MULTI-VIEW STEREO BENCHMARK", "shape": "dot", "size": 10, "title": "A dataset and evaluation framework for assessing multi-view stereo algorithms using high-resolution images and multi-camera videos"}, {"color": "rgb(178, 204, 15)", "font": {"color": "white"}, "id": "FIG. 2", "label": "FIG. 2", "shape": "dot", "size": 10, "title": "Illustration in the paper showing an example scene illuminated by seven light sources.\nIllustration depicting the seven-channel input images"}, {"color": "rgb(178, 204, 15)", "font": {"color": "white"}, "id": "LIGHT SOURCES", "label": "LIGHT SOURCES", "shape": "dot", "size": 10, "title": "Devices or points that emit illumination used to capture visual information in imaging and reconstruction tasks\nSources of illumination used to capture images of the scene, including multiple point lights\nSources of illumination used to illuminate a scene, each providing a distinct lighting condition for image capture.\nSeven distinct light emitters positioned around a scene to provide varied illumination for image capture.\nSources of light that illuminate a scene, potentially multiple in number.\nThe limited number of illumination points used in the reconstruction methods.\nEight controllable lights placed around the target object to provide active illumination during image capture."}, {"color": "rgb(105, 132, 238)", "font": {"color": "white"}, "id": "MATTHEW TANCIK", "label": "MATTHEW TANCIK", "shape": "dot", "size": 10, "title": "Researcher and co-author of Mip-nerf\nResearcher involved in developing few-shot view synthesis methods.\nResearcher and author of the Nerf paper\nCo\u2011author of the \u201cpixelnerf\u201d paper."}, {"color": "rgb(105, 132, 238)", "font": {"color": "white"}, "id": "FEW-SHOT VIEW SYNTHESIS", "label": "FEW-SHOT VIEW SYNTHESIS", "shape": "dot", "size": 10, "title": "A computer vision task that generates new views of a scene from a limited number of input images."}, {"color": "rgb(99, 106, 157)", "font": {"color": "white"}, "id": "BAPTISTE BRUMENT", "label": "BAPTISTE BRUMENT", "shape": "dot", "size": 10, "title": "Author of the 2024 CVPR paper on Rnb-neus."}, {"color": "rgb(99, 106, 157)", "font": {"color": "white"}, "id": "ROBIN BRUNEAU", "label": "ROBIN BRUNEAU", "shape": "dot", "size": 10, "title": "Author of the 2024 CVPR paper on Rnb-neus."}, {"color": "rgb(99, 106, 157)", "font": {"color": "white"}, "id": "JEAN M\u00b4ELOU", "label": "JEAN M\u00b4ELOU", "shape": "dot", "size": 10, "title": "Author of the 2024 CVPR paper on Rnb-neus."}, {"color": "rgb(99, 106, 157)", "font": {"color": "white"}, "id": "FRAN\u00c7OIS LAUZE", "label": "FRAN\u00c7OIS LAUZE", "shape": "dot", "size": 10, "title": "Author of the 2024 CVPR paper on Rnb-neus."}, {"color": "rgb(99, 106, 157)", "font": {"color": "white"}, "id": "JEAN-DENIS DUROU", "label": "JEAN-DENIS DUROU", "shape": "dot", "size": 10, "title": "Author of the 2024 CVPR paper on Rnb-neus."}, {"color": "rgb(57, 107, 249)", "font": {"color": "white"}, "id": "ABHINAV GUPTA", "label": "ABHINAV GUPTA", "shape": "dot", "size": 10, "title": "Co\u2011author of the same paper."}, {"color": "rgb(99, 106, 157)", "font": {"color": "white"}, "id": "SEONGUK SEO", "label": "SEONGUK SEO", "shape": "dot", "size": 10, "title": "A researcher who co\u2011authored the paper \u201cInfonerf\u201d on neural volume rendering"}, {"color": "rgb(178, 204, 15)", "font": {"color": "white"}, "id": "RYUSUKE SAGAWA", "label": "RYUSUKE SAGAWA", "shape": "dot", "size": 10, "title": "Lead author and researcher involved in Neural 4D Scene Reconstruction\nResearcher and co\u2011author of the Nerf\u2011based multi\u2011frame 3D integration paper.\nCo-author of the active lighting and computer vision publication.\nResearcher and co\u2011author of the paper \u201cIlluminant\u2011camera communication to observe moving objects under strong external light by spread spectrum modulation.\u201d\nA researcher in computer vision, co\u2011author of multiple papers on dense 3D reconstruction and active stereo techniques."}, {"color": "rgb(178, 204, 15)", "font": {"color": "white"}, "id": "AIST", "label": "AIST", "shape": "dot", "size": 10, "title": "National Institute of Advanced Industrial Science and Technology, Japan, where Ryusuke Sagawa is affiliated"}, {"color": "rgb(178, 204, 15)", "font": {"color": "white"}, "id": "RECONSTRUCTION", "label": "RECONSTRUCTION", "shape": "dot", "size": 10, "title": "The process of generating a 3D model or scene from 2D image data, often involving depth estimation and surface modeling."}, {"color": "rgb(178, 204, 15)", "font": {"color": "white"}, "id": "PERSON", "label": "PERSON", "shape": "dot", "size": 10, "title": "A human subject standing in the scene being reconstructed\nAn individual human subject captured in the video frames, exhibiting motion such as walking or sitting."}, {"color": "rgb(178, 204, 15)", "font": {"color": "white"}, "id": "EIGHT CAMERAS", "label": "EIGHT CAMERAS", "shape": "dot", "size": 10, "title": "Number of cameras employed in the reconstruction setup\nA set of eight imaging devices positioned to capture multiple viewpoints of a scene.\nThe number of cameras used to capture the scene for reconstruction\nA set of eight digital cameras used to capture multiple viewpoints for 4D scene reconstruction"}, {"color": "rgb(178, 204, 15)", "font": {"color": "white"}, "id": "NVIDIA V100 GPU", "label": "NVIDIA V100 GPU", "shape": "dot", "size": 10, "title": "A high-performance graphics processing unit used to accelerate the reconstruction process"}, {"color": "rgb(178, 204, 15)", "font": {"color": "white"}, "id": "90 FRAMES", "label": "90 FRAMES", "shape": "dot", "size": 10, "title": "The number of video frames processed for reconstruction"}, {"color": "rgb(178, 204, 15)", "font": {"color": "white"}, "id": "512 RAYS", "label": "512 RAYS", "shape": "dot", "size": 10, "title": "The number of rays sampled per iteration during reconstruction"}, {"color": "rgb(178, 204, 15)", "font": {"color": "white"}, "id": "210K ITERATIONS", "label": "210K ITERATIONS", "shape": "dot", "size": 10, "title": "The total number of iterations performed in the reconstruction algorithm"}, {"color": "rgb(178, 204, 15)", "font": {"color": "white"}, "id": "12 HOURS", "label": "12 HOURS", "shape": "dot", "size": 10, "title": "The approximate time required to reconstruct each frame on the NVIDIA V100 GPU"}, {"color": "rgb(178, 204, 15)", "font": {"color": "white"}, "id": "MANNEQUIN", "label": "MANNEQUIN", "shape": "dot", "size": 10, "title": "A 1.7\u202fm tall mannequin placed at the center of the scene\nA mannequin used as a reference case in the experimental setup\nA 1.7\u202fm\u2011tall mannequin used as a test subject for reconstruction"}, {"color": "rgb(178, 204, 15)", "font": {"color": "white"}, "id": "ILLUMINATION", "label": "ILLUMINATION", "shape": "dot", "size": 10, "title": "The variation of light intensity and direction used to capture different views of a scene.\nThe quality and distribution of light in a scene, affecting visibility and image characteristics.\nThe overall lighting condition under which the scene is captured, affecting visibility and shading.\nThe process of providing light to a scene, often involving multiple light sources simultaneously.\nThe light intensity and distribution produced by the light sources that reaches the target\nThe lighting conditions under which the images were captured"}, {"color": "rgb(178, 204, 15)", "font": {"color": "white"}, "id": "MULTIVIEW STEREO", "label": "MULTIVIEW STEREO", "shape": "dot", "size": 10, "title": "An algorithmic approach that reconstructs 3D geometry from multiple 2D images taken from different viewpoints\nA computer vision technique that uses multiple images from different viewpoints to estimate depth and reconstruct 3D geometry."}, {"color": "rgb(178, 204, 15)", "font": {"color": "white"}, "id": "MOVING OBJECTS", "label": "MOVING OBJECTS", "shape": "dot", "size": 10, "title": "objects that change position over time\nElements within a scene that change position over time, challenging for static\u2011scene assumptions.\nObjects that change position over time within a scene, requiring dynamic reconstruction techniques.\nObjects that change position or orientation over time, presenting challenges for static reconstruction methods.\nObjects that change position over time within the captured scene.\nDynamic entities whose positions change over time, observed in the context of illumination studies."}, {"color": "rgb(178, 204, 15)", "font": {"color": "white"}, "id": "REAL-WORLD EXPERIMENTS", "label": "REAL-WORLD EXPERIMENTS", "shape": "dot", "size": 10, "title": "Empirical tests conducted in practical settings to validate the proposed method"}, {"color": "rgb(178, 204, 15)", "font": {"color": "white"}, "id": "NEURAL REPRESENTATIONS", "label": "NEURAL REPRESENTATIONS", "shape": "dot", "size": 10, "title": "A machine\u2011learning based model that encodes 3\u2011D geometry and appearance in a neural network\nLearned, high-dimensional encodings of visual or spatial data generated by neural networks, enabling efficient integration and inference\nMathematical models (e.g., neural networks) that encode 3D scene geometry and appearance for reconstruction tasks."}, {"color": "rgb(178, 204, 15)", "font": {"color": "white"}, "id": "EIGHT POINT LIGHT SOURCES", "label": "EIGHT POINT LIGHT SOURCES", "shape": "dot", "size": 10, "title": "Eight discrete light emitters used to illuminate the scene for imaging."}, {"color": "rgb(178, 204, 15)", "font": {"color": "white"}, "id": "OUR METHOD", "label": "OUR METHOD", "shape": "dot", "size": 10, "title": "The proposed approach described in the document that reconstructs a dynamic scene using limited cameras and light sources.\na novel 3D scene reconstruction approach that addresses detail loss and calibration issues"}, {"color": "rgb(14, 125, 85)", "font": {"color": "white"}, "id": "LOSS FUNCTION", "label": "LOSS FUNCTION", "shape": "dot", "size": 10, "title": "A mathematical objective used to train the neural SDF, guiding optimization toward accurate shape estimation\nThe objective function L that is minimized during training, composed of multiple terms."}, {"color": "rgb(99, 215, 10)", "font": {"color": "white"}, "id": "ITERATION", "label": "ITERATION", "shape": "dot", "size": 10, "title": "The iterative process in which the high-density area is expected to be offset from the surface at the beginning.\nA repeated computational step in the algorithm where correspondences are refined."}, {"color": "rgb(222, 222, 124)", "font": {"color": "white"}, "id": "HIGH-DENSITY AREA", "label": "HIGH-DENSITY AREA", "shape": "dot", "size": 10, "title": "A region in the scene where the density function \u03c1(p) is high, indicating a dense or opaque part of the surface.\nA region within the volumetric data where the density of sampled points is high, influencing the accuracy of surface estimation.\nA region of the scene with high density that is expected to be offset from the surface during the initial iteration."}, {"color": "rgb(99, 215, 10)", "font": {"color": "white"}, "id": "WEIGHT \u0391", "label": "WEIGHT \u0391", "shape": "dot", "size": 10, "title": "A scalar parameter that controls the influence of LC, gradually reduced during iteration."}, {"color": "rgb(14, 125, 85)", "font": {"color": "white"}, "id": "LC", "label": "LC", "shape": "dot", "size": 10, "title": "the loss function measuring discrepancy between rendered and captured images\nThe loss term computed for all channels along the input image rays, typically a reconstruction or photometric loss.\nThe loss criterion (likely \u201cLeast Correspondence\u201d) minimized to find precise correspondences."}, {"color": "rgb(14, 125, 85)", "font": {"color": "white"}, "id": "SCENES ILLUMINATED VIRTUALLY", "label": "SCENES ILLUMINATED VIRTUALLY", "shape": "dot", "size": 10, "title": "Virtual illumination of a scene where only one light source is considered at a time to simplify analysis"}, {"color": "rgb(228, 9, 85)", "font": {"color": "white"}, "id": "YUCHEN FAN", "label": "YUCHEN FAN", "shape": "dot", "size": 10, "title": "Researcher mentioned in the text (no further context provided).\nCo-author of the mv-dust3r+ paper"}, {"color": "rgb(228, 9, 85)", "font": {"color": "white"}, "id": "MV-DUST3R+", "label": "MV-DUST3R+", "shape": "dot", "size": 10, "title": "A single-stage scene reconstruction method that operates from sparse views in 2 seconds"}, {"color": "rgb(228, 9, 85)", "font": {"color": "white"}, "id": "WIDE BASELINE", "label": "WIDE BASELINE", "shape": "dot", "size": 10, "title": "large distance between camera positions, increasing reconstruction challenge\nA configuration where the distance between Cam1 and Cam2 is large, affecting ray intersection accuracy."}, {"color": "rgb(32, 135, 241)", "font": {"color": "white"}, "id": "SHAOHUI LIU", "label": "SHAOHUI LIU", "shape": "dot", "size": 10, "title": "Shaohui Liu is a researcher who co\u2011authored the paper \u201cNerfingmvs: Guided optimization of neural radiance fields for indoor multi\u2011view stereo.\u201d"}, {"color": "rgb(65, 49, 93)", "font": {"color": "white"}, "id": "JSPS/KAKENHI JP25K22821", "label": "JSPS/KAKENHI JP25K22821", "shape": "dot", "size": 10, "title": "A specific grant from the Japan Society for the Promotion of Science.\nJapanese research grant identifier JP25K22821 supporting the study"}, {"color": "rgb(14, 125, 85)", "font": {"color": "white"}, "id": "TRAINING", "label": "TRAINING", "shape": "dot", "size": 10, "title": "the phase where models learn to interpret and reconstruct 3D shapes from visual data\nThe procedure of learning or optimizing a model, referenced in the context of enforcing constraints"}, {"color": "rgb(14, 125, 85)", "font": {"color": "white"}, "id": "EQ. (7)", "label": "EQ. (7)", "shape": "dot", "size": 10, "title": "A regularization equation applied to the transmittance function to enforce desired monotonicity.\nA mathematical equation that provides a constraint applicable to any relative angle between rays from a camera and a light source, referenced as Eq. (7).\nA specific equation that defines a constraint on intensity and transmittance\nAnother mathematical expression referenced for similar computations."}, {"color": "rgb(105, 132, 238)", "font": {"color": "white"}, "id": "VICTOR ADRIAN PRISACARIU", "label": "VICTOR ADRIAN PRISACARIU", "shape": "dot", "size": 10, "title": "Victor Adrian Prisacariu is a researcher who co\u2011authored the paper \u201cNeRF\u2212\u2212: Neural radiance fields without known camera parameters.\u201d"}, {"color": "rgb(105, 132, 238)", "font": {"color": "white"}, "id": "NERF\u2212\u2212", "label": "NERF\u2212\u2212", "shape": "dot", "size": 10, "title": "NeRF\u2212\u2212 is a neural radiance field model that reconstructs 3D scenes without known camera parameters, presented as an arXiv preprint."}, {"color": "rgb(105, 132, 238)", "font": {"color": "white"}, "id": "REF-NERF", "label": "REF-NERF", "shape": "dot", "size": 10, "title": "Ref-NeRF is a neural radiance field model that captures structured, view\u2011dependent appearance for more realistic rendering."}, {"color": "rgb(70, 100, 22)", "font": {"color": "white"}, "id": "CVPR", "label": "CVPR", "shape": "dot", "size": 10, "title": "Abbreviation for the IEEE/CVF Conference on Computer Vision and Pattern Recognition.\nIEEE Conference on Computer Vision and Pattern Recognition, 2022.\nComputer Vision and Pattern Recognition conference, 2006, where the pattern recognition paper was presented.\nCVPR is the IEEE Conference on Computer Vision and Pattern Recognition, a venue where Ref-NeRF was presented.\nThe IEEE Conference on Computer Vision and Pattern Recognition, a premier annual venue for computer vision research.\nThe IEEE Conference on Computer Vision and Pattern Recognition, where the 2024 paper was presented.\nThe IEEE Conference on Computer Vision and Pattern Recognition, a major venue for computer vision research.\nThe IEEE/CVF Conference on Computer Vision and Pattern Recognition, a major annual conference in computer vision."}, {"color": "rgb(99, 106, 157)", "font": {"color": "white"}, "id": "ANDREA VEDALDI", "label": "ANDREA VEDALDI", "shape": "dot", "size": 10, "title": "Researcher and author of the cited paper"}, {"color": "rgb(236, 30, 243)", "font": {"color": "white"}, "id": "EIICHI MATSUMOTO", "label": "EIICHI MATSUMOTO", "shape": "dot", "size": 10, "title": "Researcher and co\u2011author of the multi\u2011view neural surface reconstruction paper."}, {"color": "rgb(70, 100, 22)", "font": {"color": "white"}, "id": "NEURAL-BASED METHODS", "label": "NEURAL-BASED METHODS", "shape": "dot", "size": 10, "title": "A set of machine learning approaches used to reconstruct 4D scenes, producing smooth point clouds"}, {"color": "rgb(105, 132, 238)", "font": {"color": "white"}, "id": "SPARSE INPUTS", "label": "SPARSE INPUTS", "shape": "dot", "size": 10, "title": "A small number of input images or viewpoints used for reconstruction."}, {"color": "rgb(32, 135, 241)", "font": {"color": "white"}, "id": "ZEXIANG XU", "label": "ZEXIANG XU", "shape": "dot", "size": 10, "title": "A researcher who contributed to the Mvsnerf paper."}, {"color": "rgb(178, 204, 15)", "font": {"color": "white"}, "id": "SIMULATION", "label": "SIMULATION", "shape": "dot", "size": 10, "title": "A synthetic data generation process used to evaluate the proposed reconstruction method.\nA computational process that combines six images into seven channels for each viewpoint to reconstruct a 4D scene."}, {"color": "rgb(14, 125, 85)", "font": {"color": "white"}, "id": "VIEWPOINT", "label": "VIEWPOINT", "shape": "dot", "size": 10, "title": "The position from which the camera observes the scene\nThe camera position from which the input image is captured"}, {"color": "rgb(14, 125, 85)", "font": {"color": "white"}, "id": "CAMERA RAY", "label": "CAMERA RAY", "shape": "dot", "size": 10, "title": "Ray cast from the camera viewpoint towards the scene\nA ray cast from the camera into the scene, used to sample geometry and lighting for rendering."}, {"color": "rgb(70, 100, 22)", "font": {"color": "white"}, "id": "NUMBER OF LIGHTS", "label": "NUMBER OF LIGHTS", "shape": "dot", "size": 10, "title": "The count of light sources used during image capture; experiments reduce this number to evaluate robustness."}, {"color": "rgb(57, 62, 17)", "font": {"color": "white"}, "id": "NORMAL VECTORS", "label": "NORMAL VECTORS", "shape": "dot", "size": 10, "title": "vectors perpendicular to a surface, estimated by photometric stereo"}, {"color": "rgb(57, 62, 17)", "font": {"color": "white"}, "id": "GUANYING CHEN", "label": "GUANYING CHEN", "shape": "dot", "size": 10, "title": "Guanying Chen is a researcher who co-authored the paper Ps-nerf: Neural inverse rendering for multi-view photometric stereo."}, {"color": "rgb(57, 62, 17)", "font": {"color": "white"}, "id": "ZHENFANG CHEN", "label": "ZHENFANG CHEN", "shape": "dot", "size": 10, "title": "Zhenfang Chen is a researcher who co-authored the paper Ps-nerf: Neural inverse rendering for multi-view photometric stereo."}, {"color": "rgb(57, 62, 17)", "font": {"color": "white"}, "id": "KWAN-YEE K. WONG", "label": "KWAN-YEE K. WONG", "shape": "dot", "size": 10, "title": "Kwan-Yee K. Wong is a researcher who co-authored the paper Ps-nerf: Neural inverse rendering for multi-view photometric stereo."}, {"color": "rgb(57, 62, 17)", "font": {"color": "white"}, "id": "MULTI-VIEW PHOTOMETRIC STEREO", "label": "MULTI-VIEW PHOTOMETRIC STEREO", "shape": "dot", "size": 10, "title": "A research work proposing a robust solution and benchmark dataset for spatially varying isotropic materials using multi\u2011view photometric stereo techniques.\nA method that estimates surface normals and albedo from images taken under varying lighting conditions.\nA method that uses multiple viewpoints and photometric cues to recover surface geometry"}, {"color": "rgb(105, 132, 238)", "font": {"color": "white"}, "id": "ECCV", "label": "ECCV", "shape": "dot", "size": 10, "title": "European Conference on Computer Vision, venue where the Nerf paper was presented in 2020\nThe European Conference on Computer Vision, a biennial academic conference in the field of computer vision."}, {"color": "rgb(99, 106, 157)", "font": {"color": "white"}, "id": "JIANYUAN WANG", "label": "JIANYUAN WANG", "shape": "dot", "size": 10, "title": "Researcher and author of the cited paper"}, {"color": "rgb(99, 106, 157)", "font": {"color": "white"}, "id": "MINGHAO CHEN", "label": "MINGHAO CHEN", "shape": "dot", "size": 10, "title": "Researcher and author of the cited paper"}, {"color": "rgb(99, 106, 157)", "font": {"color": "white"}, "id": "NIKITA KARAEV", "label": "NIKITA KARAEV", "shape": "dot", "size": 10, "title": "Researcher and author of the cited paper"}, {"color": "rgb(99, 106, 157)", "font": {"color": "white"}, "id": "CHRISTIAN RUPPRECHT", "label": "CHRISTIAN RUPPRECHT", "shape": "dot", "size": 10, "title": "Researcher and author of the cited paper"}, {"color": "rgb(99, 106, 157)", "font": {"color": "white"}, "id": "DAVID NOVOTNY", "label": "DAVID NOVOTNY", "shape": "dot", "size": 10, "title": "Researcher and author of the cited paper"}, {"color": "rgb(99, 106, 157)", "font": {"color": "white"}, "id": "2025", "label": "2025", "shape": "dot", "size": 10, "title": "Year in which the Anysplat paper was published\nYear of publication of the Vggt paper"}, {"color": "rgb(178, 204, 15)", "font": {"color": "white"}, "id": "SIMPLE POINT LIGHT SOURCES", "label": "SIMPLE POINT LIGHT SOURCES", "shape": "dot", "size": 10, "title": "basic light emitters used as active lighting in photometric stereo"}, {"color": "rgb(105, 132, 238)", "font": {"color": "white"}, "id": "PETER HEDMAN", "label": "PETER HEDMAN", "shape": "dot", "size": 10, "title": "Researcher and co-author of Mip-nerf and Mip-nerf 360\nResearcher and author of the Hypernerf paper"}, {"color": "rgb(178, 204, 15)", "font": {"color": "white"}, "id": "BACKGROUND MASKS", "label": "BACKGROUND MASKS", "shape": "dot", "size": 10, "title": "Binary masks used to isolate foreground objects, not required by the proposed method"}, {"color": "rgb(178, 204, 15)", "font": {"color": "white"}, "id": "NERF-BASED METHODS", "label": "NERF-BASED METHODS", "shape": "dot", "size": 10, "title": "A class of neural radiance field techniques used for 3D scene reconstruction and visualization, often requiring multiple frames to regularize deformation.\nA family of neural rendering techniques that model scenes as continuous volumetric fields, enabling photorealistic view synthesis from sparse inputs.\nNeural Radiance Fields approaches that model scenes using neural networks for volumetric rendering."}, {"color": "rgb(178, 204, 15)", "font": {"color": "white"}, "id": "RYO FURUKAWA", "label": "RYO FURUKAWA", "shape": "dot", "size": 10, "title": "Researcher and co\u2011author of the Nerf\u2011based multi\u2011frame 3D integration paper.\nCo-author of the active lighting and computer vision publication.\nA researcher in computer vision, co\u2011author of dense 3D reconstruction methods and active stereo techniques."}, {"color": "rgb(178, 204, 15)", "font": {"color": "white"}, "id": "3D ENDOSCOPY", "label": "3D ENDOSCOPY", "shape": "dot", "size": 10, "title": "Medical imaging technique that captures three\u2011dimensional endoscopic views for diagnostic and surgical purposes."}, {"color": "rgb(70, 100, 22)", "font": {"color": "white"}, "id": "ACTIVE STEREO", "label": "ACTIVE STEREO", "shape": "dot", "size": 10, "title": "Stereo vision system that actively controls illumination or camera parameters to improve depth estimation.\nA stereo vision technique that actively controls lighting or camera parameters to improve depth estimation."}, {"color": "rgb(187, 58, 56)", "font": {"color": "white"}, "id": "COMPUTER VISION AND PATTERN RECOGNITION", "label": "COMPUTER VISION AND PATTERN RECOGNITION", "shape": "dot", "size": 10, "title": "A peer\u2011reviewed academic conference proceedings series covering research in computer vision and pattern recognition."}, {"color": "rgb(174, 144, 190)", "font": {"color": "white"}, "id": "ANDREW ZISSERMAN", "label": "ANDREW ZISSERMAN", "shape": "dot", "size": 10, "title": "Co\u2011author of a foundational text on multiple view geometry."}, {"color": "rgb(174, 144, 190)", "font": {"color": "white"}, "id": "MULTIPLE VIEW GEOMETRY", "label": "MULTIPLE VIEW GEOMETRY", "shape": "dot", "size": 10, "title": "Mathematical framework for reconstructing 3D structure from multiple images."}, {"color": "rgb(228, 9, 85)", "font": {"color": "white"}, "id": "ZHENGGANG TANG", "label": "ZHENGGANG TANG", "shape": "dot", "size": 10, "title": "Researcher mentioned in the text (no further context provided).\nAuthor of the paper on mv-dust3r+"}, {"color": "rgb(228, 9, 85)", "font": {"color": "white"}, "id": "DILIN WANG", "label": "DILIN WANG", "shape": "dot", "size": 10, "title": "Researcher mentioned in the text (no further context provided).\nCo-author of the mv-dust3r+ paper"}, {"color": "rgb(228, 9, 85)", "font": {"color": "white"}, "id": "HONGYU XU", "label": "HONGYU XU", "shape": "dot", "size": 10, "title": "Researcher mentioned in the text (no further context provided).\nCo-author of the mv-dust3r+ paper"}, {"color": "rgb(228, 9, 85)", "font": {"color": "white"}, "id": "RAKESH RANJAN", "label": "RAKESH RANJAN", "shape": "dot", "size": 10, "title": "Co-author of the mv-dust3r+ paper"}, {"color": "rgb(228, 9, 85)", "font": {"color": "white"}, "id": "ALEXANDER SCHWING", "label": "ALEXANDER SCHWING", "shape": "dot", "size": 10, "title": "Co-author of the mv-dust3r+ paper"}, {"color": "rgb(228, 9, 85)", "font": {"color": "white"}, "id": "ZHICHENG YAN", "label": "ZHICHENG YAN", "shape": "dot", "size": 10, "title": "Co-author of the mv-dust3r+ paper"}, {"color": "rgb(228, 9, 85)", "font": {"color": "white"}, "id": "ARXIV PREPRINT ARXIV:2412.06974", "label": "ARXIV PREPRINT ARXIV:2412.06974", "shape": "dot", "size": 10, "title": "The preprint where mv-dust3r+ is presented, dated 2024.2"}, {"color": "rgb(228, 9, 85)", "font": {"color": "white"}, "id": "SINGLE-STAGE SCENE RECONSTRUCTION", "label": "SINGLE-STAGE SCENE RECONSTRUCTION", "shape": "dot", "size": 10, "title": "Technique used by mv-dust3r+ to reconstruct scenes in a single pass"}, {"color": "rgb(228, 9, 85)", "font": {"color": "white"}, "id": "2 SECONDS", "label": "2 SECONDS", "shape": "dot", "size": 10, "title": "Processing time claimed by mv-dust3r+"}, {"color": "rgb(222, 222, 124)", "font": {"color": "white"}, "id": "\u03a1(P(T))", "label": "\u03a1(P(T))", "shape": "dot", "size": 10, "title": "the density value at point p(t) used in weight calculation"}, {"color": "rgb(222, 222, 124)", "font": {"color": "white"}, "id": "W(T)", "label": "W(T)", "shape": "dot", "size": 10, "title": "Weight at point px(t) along the ray during integration.\nthe weight assigned to a point along a ray in the reconstruction model"}, {"color": "rgb(146, 198, 108)", "font": {"color": "white"}, "id": "MULTI\u2011VIEW SHAPE RECONSTRUCTION", "label": "MULTI\u2011VIEW SHAPE RECONSTRUCTION", "shape": "dot", "size": 10, "title": "The process of building 3D models from images taken from multiple angles"}, {"color": "rgb(146, 198, 108)", "font": {"color": "white"}, "id": "VOLUME SWEEPING", "label": "VOLUME SWEEPING", "shape": "dot", "size": 10, "title": "A learning approach that uses photoconsistency to reconstruct 3D shapes from multiple views"}, {"color": "rgb(57, 62, 17)", "font": {"color": "white"}, "id": "IEEE TRANSACTIONS ON IMAGE PROCESSING", "label": "IEEE TRANSACTIONS ON IMAGE PROCESSING", "shape": "dot", "size": 10, "title": "A leading IEEE journal publishing research on image processing techniques."}, {"color": "rgb(99, 215, 10)", "font": {"color": "white"}, "id": "COMPUTATIONAL OVERHEAD", "label": "COMPUTATIONAL OVERHEAD", "shape": "dot", "size": 10, "title": "The additional processing resources required to handle data from many cameras during 4D scene reconstruction."}, {"color": "rgb(99, 215, 10)", "font": {"color": "white"}, "id": "RAY PROJECTION", "label": "RAY PROJECTION", "shape": "dot", "size": 10, "title": "The process of projecting rays onto a surface to analyze geometric relationships in 3D reconstruction."}, {"color": "rgb(99, 215, 10)", "font": {"color": "white"}, "id": "TRUE SURFACE", "label": "TRUE SURFACE", "shape": "dot", "size": 10, "title": "The actual geometric surface of an object in a 3D scene, against which ray projections are compared."}, {"color": "rgb(99, 215, 10)", "font": {"color": "white"}, "id": "BASELINE", "label": "BASELINE", "shape": "dot", "size": 10, "title": "The distance between two camera positions; can be narrow or wide, affecting ray projection accuracy.\nThe spatial separation between two or more cameras in a multi-view setup, affecting the disparity of projected points."}, {"color": "rgb(143, 201, 116)", "font": {"color": "white"}, "id": "JAEYOUNG CHUNG", "label": "JAEYOUNG CHUNG", "shape": "dot", "size": 10, "title": "Researcher and co-author of the paper on depth-regularized optimization for 3D Gaussian splatting"}, {"color": "rgb(143, 201, 116)", "font": {"color": "white"}, "id": "JEONGTAEK OH", "label": "JEONGTAEK OH", "shape": "dot", "size": 10, "title": "Researcher and co-author of the paper on depth-regularized optimization for 3D Gaussian splatting"}, {"color": "rgb(143, 201, 116)", "font": {"color": "white"}, "id": "KYOUNG MU LEE", "label": "KYOUNG MU LEE", "shape": "dot", "size": 10, "title": "Researcher and co-author of the paper on depth-regularized optimization for 3D Gaussian splatting"}, {"color": "rgb(213, 59, 55)", "font": {"color": "white"}, "id": "3D GAUSSIAN SPLATTING", "label": "3D GAUSSIAN SPLATTING", "shape": "dot", "size": 10, "title": "A rendering method that represents scenes using Gaussian functions in three dimensions\nA rendering technique that represents 3D points as Gaussian blobs for efficient real-time visualization.\nA rendering technique that represents 3D scenes using Gaussian distributions for efficient, real\u2011time radiance field synthesis"}, {"color": "rgb(143, 201, 116)", "font": {"color": "white"}, "id": "FEW-SHOT IMAGES", "label": "FEW-SHOT IMAGES", "shape": "dot", "size": 10, "title": "A small number of images used as input for training 3D reconstruction models"}, {"color": "rgb(143, 201, 116)", "font": {"color": "white"}, "id": "ARXIV PREPRINT ARXIV:2311.13398", "label": "ARXIV PREPRINT ARXIV:2311.13398", "shape": "dot", "size": 10, "title": "Online preprint of the paper on depth-regularized optimization for 3D Gaussian splatting, published in 2023"}, {"color": "rgb(57, 107, 249)", "font": {"color": "white"}, "id": "P.N. BELHUMEUR", "label": "P.N. BELHUMEUR", "shape": "dot", "size": 10, "title": "Researcher in computer vision and pattern recognition"}, {"color": "rgb(32, 135, 241)", "font": {"color": "white"}, "id": "XIAOSHUAI ZHANG", "label": "XIAOSHUAI ZHANG", "shape": "dot", "size": 10, "title": "A researcher who contributed to the Mvsnerf paper."}, {"color": "rgb(32, 135, 241)", "font": {"color": "white"}, "id": "MVPSNET", "label": "MVPSNET", "shape": "dot", "size": 10, "title": "Mvpsnet is a fast, generalizable multi-view photometric stereo method presented at the IEEE/CVF International Conference on Computer Vision in 2023."}, {"color": "rgb(136, 57, 198)", "font": {"color": "white"}, "id": "SOOHWAN SONG", "label": "SOOHWAN SONG", "shape": "dot", "size": 10, "title": "Researcher and author of the paper \u201cActive 3d modeling via online multi-view stereo.\u201d"}, {"color": "rgb(136, 57, 198)", "font": {"color": "white"}, "id": "ACTIVE 3D MODELING VIA ONLINE MULTI-VIEW STEREO", "label": "ACTIVE 3D MODELING VIA ONLINE MULTI-VIEW STEREO", "shape": "dot", "size": 10, "title": "A 2020 IEEE International Conference on Robotics and Automation (ICRA) publication describing a method for real\u2011time 3D model construction using multi\u2011view stereo."}, {"color": "rgb(70, 100, 22)", "font": {"color": "white"}, "id": "LIGHTS", "label": "LIGHTS", "shape": "dot", "size": 10, "title": "Number of light sources used in the reconstruction setups"}, {"color": "rgb(70, 100, 22)", "font": {"color": "white"}, "id": "FIGURE\u202f8", "label": "FIGURE\u202f8", "shape": "dot", "size": 10, "title": "Illustration showing reconstruction results under different camera and light configurations"}, {"color": "rgb(105, 132, 238)", "font": {"color": "white"}, "id": "DAN B GOLDMAN", "label": "DAN B GOLDMAN", "shape": "dot", "size": 10, "title": "Researcher and author of the Hypernerf paper"}, {"color": "rgb(178, 204, 15)", "font": {"color": "white"}, "id": "MULTIPLEXED ILLUMINATION TECHNIQUE", "label": "MULTIPLEXED ILLUMINATION TECHNIQUE", "shape": "dot", "size": 10, "title": "An illumination strategy that simultaneously projects multiple light patterns to capture the full shape of an object in a single shot\nA method that increases the number of independent light sources while preventing mutual interference, allowing more efficient lighting configurations"}, {"color": "rgb(178, 204, 15)", "font": {"color": "white"}, "id": "SCENE", "label": "SCENE", "shape": "dot", "size": 10, "title": "The physical environment or object being captured and reconstructed in the photometric stereo process.\nThe 3D environment being reconstructed and illuminated in the study\nThe spatial environment or setting that is being captured and reconstructed.\nThe physical environment or arrangement of objects that is being photographed and reconstructed.\nThe 3D environment or object being reconstructed or rendered.\nThe visual environment or set of objects being captured or reconstructed in a 3D reconstruction task.\nThe visual environment being observed and reconstructed.\nThe overall 5\u202fm diameter environment in which the mannequin and plaster object are positioned\nThe visual environment containing the person and background being reconstructed"}, {"color": "rgb(178, 204, 15)", "font": {"color": "white"}, "id": "MULTIPLEXED ILLUMINATION", "label": "MULTIPLEXED ILLUMINATION", "shape": "dot", "size": 10, "title": "A technique that separates images captured under multiple light sources into individual source images\nA lighting technique where multiple light sources are used simultaneously or in rapid sequence to illuminate a scene, enabling better capture of dynamic targets."}, {"color": "rgb(187, 58, 56)", "font": {"color": "white"}, "id": "JIAWEI YANG", "label": "JIAWEI YANG", "shape": "dot", "size": 10, "title": "Jiawei Yang is an author of an unspecified paper cited in the document.\nJiawei Yang is a researcher who co-authored the paper Freenerf: Improving few-shot neural rendering with free frequency regularization."}, {"color": "rgb(187, 58, 56)", "font": {"color": "white"}, "id": "FREENERF", "label": "FREENERF", "shape": "dot", "size": 10, "title": "a neural radiance field variant that incorporates frequency and occlusion regularization during training\nA neural radiance field approach that introduces frequency regularization for encoding positions and directions, cited for its effectiveness in sparse view scenarios.\nA neural radiance field framework that provides a positional encoding implementation used for regularization\nFreenerf is a neural rendering technique that improves few-shot performance through free frequency regularization."}, {"color": "rgb(222, 222, 124)", "font": {"color": "white"}, "id": "VKX", "label": "VKX", "shape": "dot", "size": 10, "title": "View direction of point x in the k\u2011th image."}, {"color": "rgb(222, 222, 124)", "font": {"color": "white"}, "id": "C(OK, VKX)", "label": "C(OK, VKX)", "shape": "dot", "size": 10, "title": "Intensity of a point x in the k\u2011th camera image, dependent on camera position ok and view direction vkx.\nA mathematical representation of the true scene radiance at pixel x for channel k, parameterized by observation index o and view vector vkx.\nThe mathematical function representing the rendering equation\u2019s contribution from a specific sample point (ok) and view direction (vkx)."}, {"color": "rgb(213, 22, 26)", "font": {"color": "white"}, "id": "BINGYI KANG", "label": "BINGYI KANG", "shape": "dot", "size": 10, "title": "Bingyi Kang is a researcher who co-authored the paper Depth anything: Unleashing the power of large-scale unlabeled data."}, {"color": "rgb(213, 22, 26)", "font": {"color": "white"}, "id": "XIAOGANG XU", "label": "XIAOGANG XU", "shape": "dot", "size": 10, "title": "Xiaogang Xu is a researcher who co-authored the paper Depth anything: Unleashing the power of large-scale unlabeled data."}, {"color": "rgb(213, 22, 26)", "font": {"color": "white"}, "id": "JIASHI FENG", "label": "JIASHI FENG", "shape": "dot", "size": 10, "title": "Jiashi Feng is a researcher who co-authored the paper Depth anything: Unleashing the power of large-scale unlabeled data."}, {"color": "rgb(213, 22, 26)", "font": {"color": "white"}, "id": "HENGSHUANG ZHAO", "label": "HENGSHUANG ZHAO", "shape": "dot", "size": 10, "title": "Hengshuang Zhao is a researcher who co-authored the paper Depth anything: Unleashing the power of large-scale unlabeled data."}, {"color": "rgb(178, 204, 15)", "font": {"color": "white"}, "id": "YASUHIRO MUKAIGAWA", "label": "YASUHIRO MUKAIGAWA", "shape": "dot", "size": 10, "title": "Co-author of the active lighting and computer vision publication."}, {"color": "rgb(70, 100, 22)", "font": {"color": "white"}, "id": "KIRIAKOS N. KUTULAKOS", "label": "KIRIAKOS N. KUTULAKOS", "shape": "dot", "size": 10, "title": "Researcher and author of the Turbosl paper"}, {"color": "rgb(70, 100, 22)", "font": {"color": "white"}, "id": "XIAOXIAO LONG", "label": "XIAOXIAO LONG", "shape": "dot", "size": 10, "title": "Xiaoxiao Long is a researcher who co-authored the paper \u201cNero: Neural geometry and brdf reconstruction of reflective objects.\u201d"}, {"color": "rgb(70, 100, 22)", "font": {"color": "white"}, "id": "NERO", "label": "NERO", "shape": "dot", "size": 10, "title": "Nero is a method for neural geometry and BRDF reconstruction of reflective objects."}, {"color": "rgb(70, 100, 22)", "font": {"color": "white"}, "id": "FIG. 8", "label": "FIG. 8", "shape": "dot", "size": 10, "title": "Illustration of reconstruction results under various experimental conditions, including with and without the irradiance loss."}, {"color": "rgb(32, 135, 241)", "font": {"color": "white"}, "id": "HANBYUL JOO", "label": "HANBYUL JOO", "shape": "dot", "size": 10, "title": "Researcher and author of the Panoptic studio paper"}, {"color": "rgb(32, 135, 241)", "font": {"color": "white"}, "id": "PANOPTIC STUDIO", "label": "PANOPTIC STUDIO", "shape": "dot", "size": 10, "title": "Massively multiview system for capturing and reconstructing 3D scenes from many camera views\nA massively multiview system designed for social motion capture, presented at the IEEE International Conference on Computer Vision (ICCV) in 2015."}, {"color": "rgb(178, 204, 15)", "font": {"color": "white"}, "id": "DYNAMIC SCENE (JUMPING HUMAN)", "label": "DYNAMIC SCENE (JUMPING HUMAN)", "shape": "dot", "size": 10, "title": "A moving 3D scene featuring a human performing a jumping motion."}, {"color": "rgb(178, 204, 15)", "font": {"color": "white"}, "id": "SPREAD SPECTRUM MODULATION", "label": "SPREAD SPECTRUM MODULATION", "shape": "dot", "size": 10, "title": "A communication method that spreads signal energy over a wide frequency band to reduce interference."}, {"color": "rgb(70, 100, 22)", "font": {"color": "white"}, "id": "MESH SURFACES", "label": "MESH SURFACES", "shape": "dot", "size": 10, "title": "Triangulated surfaces derived from point clouds, intended to represent continuous geometry"}, {"color": "rgb(70, 100, 22)", "font": {"color": "white"}, "id": "2DGS", "label": "2DGS", "shape": "dot", "size": 10, "title": "A 2D Gaussian Splatting method used in combination with DUSt3R\nA 2D Gaussian splatting implementation that first renders depth maps and then integrates them into 3D meshes.\nA 2D-guided geometry synthesis method compared in the study"}, {"color": "rgb(70, 100, 22)", "font": {"color": "white"}, "id": "POINT CLOUD", "label": "POINT CLOUD", "shape": "dot", "size": 10, "title": "A set of 3D points serving as an initial input for mesh generation"}, {"color": "rgb(130, 83, 45)", "font": {"color": "white"}, "id": "POINT CLOUDS", "label": "POINT CLOUDS", "shape": "dot", "size": 10, "title": "Collections of 3D points representing the reconstructed shapes\nSets of 3D points representing the geometry of objects, extracted by various methods"}, {"color": "rgb(178, 204, 15)", "font": {"color": "white"}, "id": "OUR WORK", "label": "OUR WORK", "shape": "dot", "size": 10, "title": "The research described in the text, which focuses on reconstructing shapes from only a few images taken at a single moment."}, {"color": "rgb(57, 107, 249)", "font": {"color": "white"}, "id": "BARF", "label": "BARF", "shape": "dot", "size": 10, "title": "Barf is a method called Bundle-adjusting neural radiance fields, presented in a 2021 ICCV paper."}, {"color": "rgb(158, 206, 123)", "font": {"color": "white"}, "id": "CARLOS HERNANDEZ", "label": "CARLOS HERNANDEZ", "shape": "dot", "size": 10, "title": "Researcher cited in the document.\nCarlos Hernandez is a researcher cited as an author in the paper \u0027Multiview photometric stereo\u0027"}, {"color": "rgb(136, 57, 198)", "font": {"color": "white"}, "id": "2020", "label": "2020", "shape": "dot", "size": 10, "title": "Year the multi\u2011view photometric stereo paper was published in IEEE Transactions on Image Processing.\nYear of publication for the Nerf paper"}, {"color": "rgb(136, 57, 198)", "font": {"color": "white"}, "id": "IEEE INTERNATIONAL CONFERENCE ON ROBOTICS AND AUTOMATION", "label": "IEEE INTERNATIONAL CONFERENCE ON ROBOTICS AND AUTOMATION", "shape": "dot", "size": 10, "title": "ICRA, a leading robotics and automation conference where the above paper was presented in 2020."}, {"color": "rgb(14, 125, 85)", "font": {"color": "white"}, "id": "FEATURE VECTOR", "label": "FEATURE VECTOR", "shape": "dot", "size": 10, "title": "A multi-dimensional numerical representation of input data (e.g., position, direction) fed into neural networks."}, {"color": "rgb(146, 198, 108)", "font": {"color": "white"}, "id": "PHOTOCONSISTENCY", "label": "PHOTOCONSISTENCY", "shape": "dot", "size": 10, "title": "The property that ensures consistency of pixel intensities across different viewpoints in multi\u2011view reconstruction"}, {"color": "rgb(146, 198, 108)", "font": {"color": "white"}, "id": "VINCENT LEROY", "label": "VINCENT LEROY", "shape": "dot", "size": 10, "title": "A researcher involved in the \u201cVolume sweeping\u201d study"}, {"color": "rgb(146, 198, 108)", "font": {"color": "white"}, "id": "JEAN\u2011SEBASTIEN FRANCO", "label": "JEAN\u2011SEBASTIEN FRANCO", "shape": "dot", "size": 10, "title": "A researcher involved in the \u201cVolume sweeping\u201d study"}, {"color": "rgb(146, 198, 108)", "font": {"color": "white"}, "id": "EDMOND BOYER", "label": "EDMOND BOYER", "shape": "dot", "size": 10, "title": "A researcher involved in the \u201cVolume sweeping\u201d study"}, {"color": "rgb(200, 199, 161)", "font": {"color": "white"}, "id": "SIGGRAPH", "label": "SIGGRAPH", "shape": "dot", "size": 10, "title": "An annual conference on computer graphics and interactive techniques, where the 2005 paper was presented."}, {"color": "rgb(200, 199, 161)", "font": {"color": "white"}, "id": "PERFORMANCE RELIGHTING", "label": "PERFORMANCE RELIGHTING", "shape": "dot", "size": 10, "title": "The process of adjusting lighting conditions in a rendered scene to achieve desired visual effects."}, {"color": "rgb(213, 59, 55)", "font": {"color": "white"}, "id": "ALEXANDER KELLER", "label": "ALEXANDER KELLER", "shape": "dot", "size": 10, "title": "Co-author of the 2022 paper on instant neural graphics primitives."}, {"color": "rgb(213, 59, 55)", "font": {"color": "white"}, "id": "INSTANT NEURAL GRAPHICS PRIMITIVES WITH A MULTIRESOLUTION HASH ENCODING", "label": "INSTANT NEURAL GRAPHICS PRIMITIVES WITH A MULTIRESOLUTION HASH ENCODING", "shape": "dot", "size": 10, "title": "A neural rendering method that uses a multiresolution hash table to encode 3D geometry and appearance for real\u2011time rendering."}, {"color": "rgb(105, 132, 238)", "font": {"color": "white"}, "id": "KEUNHONG PARK", "label": "KEUNHONG PARK", "shape": "dot", "size": 10, "title": "Researcher and author of the Hypernerf paper"}, {"color": "rgb(105, 132, 238)", "font": {"color": "white"}, "id": "UTKARSH SINHA", "label": "UTKARSH SINHA", "shape": "dot", "size": 10, "title": "Researcher and author of the Hypernerf paper"}, {"color": "rgb(105, 132, 238)", "font": {"color": "white"}, "id": "SOFIEN BOUAZIZ", "label": "SOFIEN BOUAZIZ", "shape": "dot", "size": 10, "title": "Researcher and author of the Hypernerf paper"}, {"color": "rgb(105, 132, 238)", "font": {"color": "white"}, "id": "RICARDO MARTIN-BRUALLA", "label": "RICARDO MARTIN-BRUALLA", "shape": "dot", "size": 10, "title": "Researcher and co-author of Mip-nerf\nResearcher and author of the Hypernerf paper\nA researcher who co\u2011authored the paper \u201cSharf: Shape-conditioned radiance fields from a single view.\u201d"}, {"color": "rgb(105, 132, 238)", "font": {"color": "white"}, "id": "STEVEN M. SEITZ", "label": "STEVEN M. SEITZ", "shape": "dot", "size": 10, "title": "Researcher and author of the Hypernerf paper"}, {"color": "rgb(105, 132, 238)", "font": {"color": "white"}, "id": "D-NERF", "label": "D-NERF", "shape": "dot", "size": 10, "title": "Neural Radiance Fields for Dynamic Scenes, presented in 2021"}, {"color": "rgb(105, 132, 238)", "font": {"color": "white"}, "id": "ACM TRANS. GRAPH.", "label": "ACM TRANS. GRAPH.", "shape": "dot", "size": 10, "title": "Association for Computing Machinery Transactions on Graphics, a peer\u2011reviewed journal"}, {"color": "rgb(213, 59, 55)", "font": {"color": "white"}, "id": "ALEX EVANS", "label": "ALEX EVANS", "shape": "dot", "size": 10, "title": "Co-author of the 2022 paper on instant neural graphics primitives."}, {"color": "rgb(34, 33, 152)", "font": {"color": "white"}, "id": "AMOS GROPP", "label": "AMOS GROPP", "shape": "dot", "size": 10, "title": "Researcher contributing to implicit geometric regularization for shape learning."}, {"color": "rgb(34, 33, 152)", "font": {"color": "white"}, "id": "LIOR YARIV", "label": "LIOR YARIV", "shape": "dot", "size": 10, "title": "Researcher contributing to implicit geometric regularization for shape learning."}, {"color": "rgb(34, 33, 152)", "font": {"color": "white"}, "id": "NIV HAIM", "label": "NIV HAIM", "shape": "dot", "size": 10, "title": "Researcher contributing to implicit geometric regularization for shape learning."}, {"color": "rgb(34, 33, 152)", "font": {"color": "white"}, "id": "MATAN ATZMON", "label": "MATAN ATZMON", "shape": "dot", "size": 10, "title": "Researcher contributing to implicit geometric regularization for shape learning."}, {"color": "rgb(34, 33, 152)", "font": {"color": "white"}, "id": "YARON LIPMAN", "label": "YARON LIPMAN", "shape": "dot", "size": 10, "title": "Researcher contributing to implicit geometric regularization for shape learning."}, {"color": "rgb(99, 215, 10)", "font": {"color": "white"}, "id": "BASELINES", "label": "BASELINES", "shape": "dot", "size": 10, "title": "The distances between camera pairs, described as very wide in the experiment."}, {"color": "rgb(99, 106, 157)", "font": {"color": "white"}, "id": "PAOLO FAVARO", "label": "PAOLO FAVARO", "shape": "dot", "size": 10, "title": "Author of the 2024 CVPR paper on sparse 3D reconstruction."}, {"color": "rgb(130, 83, 45)", "font": {"color": "white"}, "id": "SL BASED METHOD", "label": "SL BASED METHOD", "shape": "dot", "size": 10, "title": "Structure\u2011from\u2011Motion based reconstruction approach that failed to recover fine details"}, {"color": "rgb(105, 132, 238)", "font": {"color": "white"}, "id": "PRATUL P. SRINIVASAN", "label": "PRATUL P. SRINIVASAN", "shape": "dot", "size": 10, "title": "Researcher and author of the Nerf paper"}, {"color": "rgb(143, 201, 116)", "font": {"color": "white"}, "id": "SHAPE RECONSTRUCTION", "label": "SHAPE RECONSTRUCTION", "shape": "dot", "size": 10, "title": "Process of deriving 3D shape from multi-channel images\nThe process of determining the 3D shape of an object from visual data."}, {"color": "rgb(14, 125, 85)", "font": {"color": "white"}, "id": "PHOTOMETRIC STEREO (PS)", "label": "PHOTOMETRIC STEREO (PS)", "shape": "dot", "size": 10, "title": "A 3\u2011D reconstruction technique that uses variations in lighting to infer surface normals and depth\nA technique that reconstructs surface normals and albedo by observing a scene under varying illumination from a single viewpoint, using the irradiance of the light source.\na technique that estimates surface normals from multiple images taken under different lighting"}, {"color": "rgb(14, 125, 85)", "font": {"color": "white"}, "id": "MULTI-VIEW PHOTOMETRIC STEREO (MVPS)", "label": "MULTI-VIEW PHOTOMETRIC STEREO (MVPS)", "shape": "dot", "size": 10, "title": "An extension of photometric stereo that captures a scene from multiple viewpoints under varying illumination, providing richer information about each surface point.\nAn extension of photometric stereo that uses multiple images from a single camera under varying light positions."}, {"color": "rgb(14, 125, 85)", "font": {"color": "white"}, "id": "RECONSTRUCTION ACCURACY", "label": "RECONSTRUCTION ACCURACY", "shape": "dot", "size": 10, "title": "the degree to which a reconstructed 3D scene matches the original"}, {"color": "rgb(178, 204, 15)", "font": {"color": "white"}, "id": "PHOTOMETRIC STEREO (PS) TECHNIQUE", "label": "PHOTOMETRIC STEREO (PS) TECHNIQUE", "shape": "dot", "size": 10, "title": "A method that recovers surface normals by observing changes in shading under different lighting"}, {"color": "rgb(14, 125, 85)", "font": {"color": "white"}, "id": "SHADOW", "label": "SHADOW", "shape": "dot", "size": 10, "title": "Shadow cast by the object, providing information about irradiance"}, {"color": "rgb(158, 206, 123)", "font": {"color": "white"}, "id": "ROBERTO CIPOLLA", "label": "ROBERTO CIPOLLA", "shape": "dot", "size": 10, "title": "Researcher cited in the document.\nRoberto Cipolla is a researcher cited as an author in the paper \u0027Multiview photometric stereo\u0027"}, {"color": "rgb(178, 204, 15)", "font": {"color": "white"}, "id": "TARGET OBJECT", "label": "TARGET OBJECT", "shape": "dot", "size": 10, "title": "An object within the scene that is moving and is the focus of observation.\nThe physical item being scanned and reconstructed in the experiments."}, {"color": "rgb(178, 204, 15)", "font": {"color": "white"}, "id": "OBSERVING MOVING TARGETS", "label": "OBSERVING MOVING TARGETS", "shape": "dot", "size": 10, "title": "The process of tracking and capturing data about objects that change position over time within a scene."}, {"color": "rgb(236, 30, 243)", "font": {"color": "white"}, "id": "TAISUKE HASHIMOTO", "label": "TAISUKE HASHIMOTO", "shape": "dot", "size": 10, "title": "Researcher and co\u2011author of the multi\u2011view neural surface reconstruction paper."}, {"color": "rgb(105, 132, 238)", "font": {"color": "white"}, "id": "WEIDI XIE", "label": "WEIDI XIE", "shape": "dot", "size": 10, "title": "Weidi Xie is a researcher who co\u2011authored the paper \u201cNeRF\u2212\u2212: Neural radiance fields without known camera parameters.\u201d"}, {"color": "rgb(14, 125, 85)", "font": {"color": "white"}, "id": "CJ(P(T), VJP)", "label": "CJ(P(T), VJP)", "shape": "dot", "size": 10, "title": "The intensity measured at point\u202fp(t) for a viewing direction\u202fvjp, derived from irradiance and reflectance.\na constraint function proportional to irradiance at point p(t) from light source vj"}, {"color": "rgb(14, 125, 85)", "font": {"color": "white"}, "id": "OBJECT SURFACE", "label": "OBJECT SURFACE", "shape": "dot", "size": 10, "title": "The geometric surface that may exist at point\u202fp(t), potentially reflecting or absorbing light."}, {"color": "rgb(14, 125, 85)", "font": {"color": "white"}, "id": "TRANSMITTANCE", "label": "TRANSMITTANCE", "shape": "dot", "size": 10, "title": "the fraction of light that passes through a medium without being absorbed or scattered\nThe attenuation of light as it travels through space, modeled as a monotonically decreasing function of distance from the light source.\nThe amount of light that passes through a medium, accumulated during volume rendering to calculate Lg."}, {"color": "rgb(14, 125, 85)", "font": {"color": "white"}, "id": "ATTENUATION", "label": "ATTENUATION", "shape": "dot", "size": 10, "title": "the reduction in light intensity due to distance or medium absorption"}, {"color": "rgb(105, 132, 238)", "font": {"color": "white"}, "id": "TRAINING/RENDERING SPEED", "label": "TRAINING/RENDERING SPEED", "shape": "dot", "size": 10, "title": "The efficiency of training a model and rendering images from it"}, {"color": "rgb(213, 59, 55)", "font": {"color": "white"}, "id": "REAL-TIME RADIANCE FIELD RENDERING", "label": "REAL-TIME RADIANCE FIELD RENDERING", "shape": "dot", "size": 10, "title": "The process of generating high\u2011fidelity images from neural radiance fields with minimal latency, enabling interactive applications"}, {"color": "rgb(178, 204, 15)", "font": {"color": "white"}, "id": "SEQUENTIAL ACQUISITION", "label": "SEQUENTIAL ACQUISITION", "shape": "dot", "size": 10, "title": "The method of capturing images or data one after another, typically under different lighting conditions."}, {"color": "rgb(187, 58, 56)", "font": {"color": "white"}, "id": "KEVIN WANG", "label": "KEVIN WANG", "shape": "dot", "size": 10, "title": "Researcher and co\u2011author of the Instantsplat paper."}, {"color": "rgb(57, 62, 17)", "font": {"color": "white"}, "id": "DANIEL VLASIC", "label": "DANIEL VLASIC", "shape": "dot", "size": 10, "title": "Daniel Vlasic is a co\u2011author of the dynamic shape capture paper."}, {"color": "rgb(187, 58, 56)", "font": {"color": "white"}, "id": "JIAN ZHANG", "label": "JIAN ZHANG", "shape": "dot", "size": 10, "title": "Researcher and co\u2011author of the Instantsplat paper."}, {"color": "rgb(105, 132, 238)", "font": {"color": "white"}, "id": "BEN MILDENHALL", "label": "BEN MILDENHALL", "shape": "dot", "size": 10, "title": "Researcher and co-author of Mip-nerf and Mip-nerf 360\nResearcher and author of the Nerf paper\nCo\u2011author of the Regnerf paper, contributor to neural radiance fields research.\nA researcher mentioned in the document, likely a co\u2011author of a related work.\nResearcher and co\u2011author of the paper \u201cDense depth priors for neural radiance fields from sparse input views.\u201d"}, {"color": "rgb(105, 132, 238)", "font": {"color": "white"}, "id": "PRATUL P SRINIVASAN", "label": "PRATUL P SRINIVASAN", "shape": "dot", "size": 10, "title": "Researcher and co-author of Mip-nerf and Mip-nerf 360\nResearcher and co\u2011author of the paper \u201cDense depth priors for neural radiance fields from sparse input views.\u201d"}, {"color": "rgb(136, 133, 36)", "font": {"color": "white"}, "id": "4 CAMS", "label": "4 CAMS", "shape": "dot", "size": 10, "title": "Number of cameras used in a reconstruction experiment"}, {"color": "rgb(32, 135, 241)", "font": {"color": "white"}, "id": "MATTHEW O\u2019TOOLE", "label": "MATTHEW O\u2019TOOLE", "shape": "dot", "size": 10, "title": "Researcher and co-author of the 2023 paper on neural fields for structured lighting."}, {"color": "rgb(14, 125, 85)", "font": {"color": "white"}, "id": "CHANNEL", "label": "CHANNEL", "shape": "dot", "size": 10, "title": "A spectral or illumination\u2011specific slice of a multi\u2011channel image, e.g., one captured under a particular light source.\nThe specific portion of the scene illuminated by one of the light sources, analyzed in Fig. 3(c).\nThe optical path or ray along which light travels from a light source to a point in the scene, considered for Lg calculation."}, {"color": "rgb(70, 100, 22)", "font": {"color": "white"}, "id": "DEMULTIPLEXED IMAGES", "label": "DEMULTIPLEXED IMAGES", "shape": "dot", "size": 10, "title": "Images separated by a demultiplexing process, each illuminated by a single light source.\nImages obtained by separating a set of 256 high\u2011speed camera frames into individual frames to increase frame rate\nProcessed images where individual channels or views are separated\nA set of images separated from a composite capture, used to analyze individual views of a scene"}, {"color": "rgb(178, 204, 15)", "font": {"color": "white"}, "id": "POINT LIGHT SOURCES", "label": "POINT LIGHT SOURCES", "shape": "dot", "size": 10, "title": "Individual light emitters positioned at specific locations to illuminate the scene\nAssumed light sources in the paper that act as idealized point emitters, enabling geometric treatment as perspective cameras."}, {"color": "rgb(14, 125, 85)", "font": {"color": "white"}, "id": "PERSPECTIVE CAMERAS", "label": "PERSPECTIVE CAMERAS", "shape": "dot", "size": 10, "title": "The geometric model used to represent point light sources when calculating Lg."}, {"color": "rgb(240, 197, 127)", "font": {"color": "white"}, "id": "BLENDER DATASET", "label": "BLENDER DATASET", "shape": "dot", "size": 10, "title": "A collection of 3D models used as samples in the experiment"}, {"color": "rgb(240, 197, 127)", "font": {"color": "white"}, "id": "3D MODELS", "label": "3D MODELS", "shape": "dot", "size": 10, "title": "Three-dimensional representations of objects provided by the Blender dataset"}, {"color": "rgb(178, 204, 15)", "font": {"color": "white"}, "id": "VIEWPOINTS", "label": "VIEWPOINTS", "shape": "dot", "size": 10, "title": "Different camera positions from which a scene is observed, enabling multiview analysis.\nThe limited number of observation angles available for reconstruction."}, {"color": "rgb(187, 58, 56)", "font": {"color": "white"}, "id": "DUST3R+2DGS", "label": "DUST3R+2DGS", "shape": "dot", "size": 10, "title": "A hybrid method combining DUSt3R with 2D Geometry Splatting for efficient 3D reconstruction."}, {"color": "rgb(187, 58, 56)", "font": {"color": "white"}, "id": "SINUSOIDAL FUNCTIONS", "label": "SINUSOIDAL FUNCTIONS", "shape": "dot", "size": 10, "title": "Mathematical functions used to encode spatial positions and viewing directions, facilitating smooth interpolation across frequencies."}, {"color": "rgb(187, 58, 56)", "font": {"color": "white"}, "id": "FREQUENCY REGULARIZATION", "label": "FREQUENCY REGULARIZATION", "shape": "dot", "size": 10, "title": "a constraint applied to neural network weights to limit high-frequency components\nA constraint technique that penalizes high-frequency components in the encoding, proposed in FreeNeRF to stabilize learning.\nA technique that penalizes high-frequency components in the positional encoding to improve stability"}, {"color": "rgb(70, 100, 22)", "font": {"color": "white"}, "id": "NOAH SNAVELY", "label": "NOAH SNAVELY", "shape": "dot", "size": 10, "title": "Co-author of the paper \"Iron: Inverse rendering by optimizing neural sdfs and materials from photometric images\""}, {"color": "rgb(174, 144, 190)", "font": {"color": "white"}, "id": "RICHARD HARTLEY", "label": "RICHARD HARTLEY", "shape": "dot", "size": 10, "title": "Co\u2011author of a foundational text on multiple view geometry."}, {"color": "rgb(174, 144, 190)", "font": {"color": "white"}, "id": "CAMBRIDGE UNIVERSITY PRESS", "label": "CAMBRIDGE UNIVERSITY PRESS", "shape": "dot", "size": 10, "title": "Publisher of the Multiple View Geometry book."}, {"color": "rgb(178, 204, 15)", "font": {"color": "white"}, "id": "YASUSHI YAGI", "label": "YASUSHI YAGI", "shape": "dot", "size": 10, "title": "A researcher in computer vision, co\u2011author of several works on dense 3D reconstruction and active stereo."}, {"color": "rgb(32, 135, 241)", "font": {"color": "white"}, "id": "DONGXU ZHAO", "label": "DONGXU ZHAO", "shape": "dot", "size": 10, "title": "Dongxu Zhao is a researcher who co-authored the paper \"Mvpsnet: Fast generalizable multi-view photometric stereo\" in 2023."}, {"color": "rgb(32, 135, 241)", "font": {"color": "white"}, "id": "DANIEL LICHY", "label": "DANIEL LICHY", "shape": "dot", "size": 10, "title": "Daniel Lichy is a researcher who co-authored the paper \"Mvpsnet: Fast generalizable multi-view photometric stereo\" in 2023."}, {"color": "rgb(32, 135, 241)", "font": {"color": "white"}, "id": "PIERRE-NICOLAS PERRIN", "label": "PIERRE-NICOLAS PERRIN", "shape": "dot", "size": 10, "title": "Pierre-Nicolas Perrin is a researcher who co-authored the paper \"Mvpsnet: Fast generalizable multi-view photometric stereo\" in 2023."}, {"color": "rgb(32, 135, 241)", "font": {"color": "white"}, "id": "SOUMYADIP SENGUPTA", "label": "SOUMYADIP SENGUPTA", "shape": "dot", "size": 10, "title": "Soumyadip Sengupta is a researcher who co-authored the paper \"Mvpsnet: Fast generalizable multi-view photometric stereo\" in 2023."}, {"color": "rgb(32, 135, 241)", "font": {"color": "white"}, "id": "FAST GENERALIZABLE MULTI-VIEW PHOTOMETRIC STEREO", "label": "FAST GENERALIZABLE MULTI-VIEW PHOTOMETRIC STEREO", "shape": "dot", "size": 10, "title": "A computer vision technique that estimates surface normals and depth from multiple images using photometric cues, optimized for speed and generalization."}, {"color": "rgb(70, 100, 22)", "font": {"color": "white"}, "id": "NEURAL GEOMETRY AND BRDF RECONSTRUCTION OF REFLECTIVE OBJECTS", "label": "NEURAL GEOMETRY AND BRDF RECONSTRUCTION OF REFLECTIVE OBJECTS", "shape": "dot", "size": 10, "title": "A technique to recover geometry and reflectance properties of reflective surfaces using neural networks."}, {"color": "rgb(178, 204, 15)", "font": {"color": "white"}, "id": "DAISUKE MIYAZAKI", "label": "DAISUKE MIYAZAKI", "shape": "dot", "size": 10, "title": "Co-author of the active lighting and computer vision publication."}, {"color": "rgb(178, 204, 15)", "font": {"color": "white"}, "id": "PS", "label": "PS", "shape": "dot", "size": 10, "title": "An abbreviation for a specific pattern\u2011sensing or pattern\u2011shaping method used alongside active lighting in 3D reconstruction."}, {"color": "rgb(70, 100, 22)", "font": {"color": "white"}, "id": "CAPTURED IMAGES", "label": "CAPTURED IMAGES", "shape": "dot", "size": 10, "title": "Images obtained from real-world viewpoints used as input for reconstruction.\nImages that have been filtered and demodulated from the raw sensor data\nImages taken by the cameras during the experiment, used as input for 3D reconstruction\nImages taken by cameras used for 3D reconstruction\nImages taken by the camera setup before demultiplexing"}, {"color": "rgb(178, 204, 15)", "font": {"color": "white"}, "id": "EXISTING METHODS", "label": "EXISTING METHODS", "shape": "dot", "size": 10, "title": "Prior techniques for 3D reconstruction against which the proposed method is compared"}, {"color": "rgb(178, 204, 15)", "font": {"color": "white"}, "id": "WALKING", "label": "WALKING", "shape": "dot", "size": 10, "title": "The act of moving forward by alternating legs, demonstrated in the reconstruction results\nA specific motion state where the person is moving forward by alternating steps."}, {"color": "rgb(178, 204, 15)", "font": {"color": "white"}, "id": "SITTING DOWN", "label": "SITTING DOWN", "shape": "dot", "size": 10, "title": "The act of lowering oneself onto a seat, demonstrated in the reconstruction results\nA specific motion state where the person transitions from standing to a seated posture."}, {"color": "rgb(178, 204, 15)", "font": {"color": "white"}, "id": "FRAMES", "label": "FRAMES", "shape": "dot", "size": 10, "title": "Individual time\u2011stamped images or slices used in the reconstruction pipeline."}, {"color": "rgb(70, 100, 22)", "font": {"color": "white"}, "id": "WIDE-BASELINE SETUP", "label": "WIDE-BASELINE SETUP", "shape": "dot", "size": 10, "title": "A configuration in multi-view imaging where camera viewpoints are widely separated, enabling better depth estimation through increased parallax."}, {"color": "rgb(213, 59, 55)", "font": {"color": "white"}, "id": "THOMAS LEIMK\u00dcHLER", "label": "THOMAS LEIMK\u00dcHLER", "shape": "dot", "size": 10, "title": "Co-author of the 3D Gaussian Splatting paper."}, {"color": "rgb(57, 107, 249)", "font": {"color": "white"}, "id": "ABHINAV SHRIVASTAVA", "label": "ABHINAV SHRIVASTAVA", "shape": "dot", "size": 10, "title": "Co\u2011author of the same paper."}, {"color": "rgb(57, 62, 17)", "font": {"color": "white"}, "id": "WOJCIECH MATUSIK", "label": "WOJCIECH MATUSIK", "shape": "dot", "size": 10, "title": "Wojciech Matusik is a co\u2011author of the dynamic shape capture paper."}, {"color": "rgb(105, 132, 238)", "font": {"color": "white"}, "id": "GEOMETRY CONSTRAINTS", "label": "GEOMETRY CONSTRAINTS", "shape": "dot", "size": 10, "title": "Regularization terms that enforce plausible 3D structure in the reconstructed scene."}, {"color": "rgb(226, 210, 2)", "font": {"color": "white"}, "id": "DISTANCES", "label": "DISTANCES", "shape": "dot", "size": 10, "title": "Measured discrepancies between reconstructed and ground\u2011truth geometry."}, {"color": "rgb(226, 210, 2)", "font": {"color": "white"}, "id": "CONSTRAINT OF LG", "label": "CONSTRAINT OF LG", "shape": "dot", "size": 10, "title": "The enforcement of the irradiance loss term during optimization; its absence leads to increased errors."}, {"color": "rgb(103, 156, 25)", "font": {"color": "white"}, "id": "SHENGHUA GAO", "label": "SHENGHUA GAO", "shape": "dot", "size": 10, "title": "Shenghua Gao is a researcher cited as an author in the paper \u00272d gaussian splatting for geometrically accurate radiance fields\u0027"}, {"color": "rgb(105, 132, 238)", "font": {"color": "white"}, "id": "JFT-300M", "label": "JFT-300M", "shape": "dot", "size": 10, "title": "A large-scale image-text dataset containing 300 million images, used to train RegNeRF."}, {"color": "rgb(105, 132, 238)", "font": {"color": "white"}, "id": "MICHAEL NIEMEYER", "label": "MICHAEL NIEMEYER", "shape": "dot", "size": 10, "title": "Author of the Regnerf paper, a researcher in computer vision and machine learning."}, {"color": "rgb(105, 132, 238)", "font": {"color": "white"}, "id": "MEHDI S. M. SAJJADI", "label": "MEHDI S. M. SAJJADI", "shape": "dot", "size": 10, "title": "Co\u2011author of the Regnerf paper, specialist in photometric stereo."}, {"color": "rgb(105, 132, 238)", "font": {"color": "white"}, "id": "ANDREAS GEIGER", "label": "ANDREAS GEIGER", "shape": "dot", "size": 10, "title": "Andreas Geiger is a researcher cited as an author in the paper \u00272d gaussian splatting for geometrically accurate radiance fields\u0027\nCo\u2011author of the Regnerf paper, prominent figure in autonomous driving perception.\nResearcher and author of the 2017 multi-view stereo benchmark paper"}, {"color": "rgb(105, 132, 238)", "font": {"color": "white"}, "id": "VIEW SYNTHESIS", "label": "VIEW SYNTHESIS", "shape": "dot", "size": 10, "title": "The process of generating new viewpoints of a scene from existing images, often used as the primary application of NeRF\nGenerating new viewpoints of a scene from existing images\nThe process of generating new viewpoints of a scene from limited input images."}, {"color": "rgb(57, 62, 17)", "font": {"color": "white"}, "id": "PIETER PEERS", "label": "PIETER PEERS", "shape": "dot", "size": 10, "title": "Pieter Peers is a co\u2011author of the dynamic shape capture paper."}, {"color": "rgb(228, 9, 85)", "font": {"color": "white"}, "id": "CAMERA IMAGE", "label": "CAMERA IMAGE", "shape": "dot", "size": 10, "title": "The 2\u2011D array of pixel intensities captured by a camera, comprising multiple channels under different lighting conditions.\nThe visual data captured by the cameras, used to analyze intensity and irradiance."}, {"color": "rgb(228, 9, 85)", "font": {"color": "white"}, "id": "INTENSITY CALCULATION", "label": "INTENSITY CALCULATION", "shape": "dot", "size": 10, "title": "The mathematical computation of light intensity at a point, as defined by Eq. (1)."}, {"color": "rgb(152, 180, 111)", "font": {"color": "white"}, "id": "KONSTANTINOS REMATAS", "label": "KONSTANTINOS REMATAS", "shape": "dot", "size": 10, "title": "A researcher who co\u2011authored the paper \u201cSharf: Shape-conditioned radiance fields from a single view.\u201d"}, {"color": "rgb(32, 135, 241)", "font": {"color": "white"}, "id": "AARRUSHI SHANDILYA", "label": "AARRUSHI SHANDILYA", "shape": "dot", "size": 10, "title": "Researcher and co-author of the 2023 paper on neural fields for structured lighting."}, {"color": "rgb(70, 100, 22)", "font": {"color": "white"}, "id": "3D DATA PROCESSING", "label": "3D DATA PROCESSING", "shape": "dot", "size": 10, "title": "The field of manipulating, analyzing, and visualizing three-dimensional data structures."}, {"color": "rgb(187, 58, 56)", "font": {"color": "white"}, "id": "ZHIWEN FAN", "label": "ZHIWEN FAN", "shape": "dot", "size": 10, "title": "Researcher and co\u2011author of the Instantsplat paper."}, {"color": "rgb(136, 133, 36)", "font": {"color": "white"}, "id": "W/O IRRADIANCE LOSS LG", "label": "W/O IRRADIANCE LOSS LG", "shape": "dot", "size": 10, "title": "A setting where the irradiance loss term is omitted"}, {"color": "rgb(222, 222, 124)", "font": {"color": "white"}, "id": "DISTANCE", "label": "DISTANCE", "shape": "dot", "size": 10, "title": "The geometric measure returned by the SDF, indicating how far a point is from a surface."}, {"color": "rgb(222, 222, 124)", "font": {"color": "white"}, "id": "SIGNED DISTANCE FUNCTION (SDF)", "label": "SIGNED DISTANCE FUNCTION (SDF)", "shape": "dot", "size": 10, "title": "A mathematical function f(p) that assigns to any 3D point p a scalar value equal to the signed distance from the nearest surface, used for volumetric representation."}, {"color": "rgb(213, 59, 55)", "font": {"color": "white"}, "id": "BERNHARD KERBL", "label": "BERNHARD KERBL", "shape": "dot", "size": 10, "title": "Author of the 3D Gaussian Splatting paper."}, {"color": "rgb(99, 106, 157)", "font": {"color": "white"}, "id": "BOHYUNG HAN", "label": "BOHYUNG HAN", "shape": "dot", "size": 10, "title": "A researcher who co\u2011authored the paper \u201cInfonerf\u201d on neural volume rendering"}, {"color": "rgb(14, 125, 85)", "font": {"color": "white"}, "id": "IMAGE INTENSITIES", "label": "IMAGE INTENSITIES", "shape": "dot", "size": 10, "title": "The measured brightness values captured by a camera from different viewpoints"}, {"color": "rgb(14, 125, 85)", "font": {"color": "white"}, "id": "PROJECTED LIGHTS", "label": "PROJECTED LIGHTS", "shape": "dot", "size": 10, "title": "Light sources whose emitted irradiance is explicitly modeled and constrained in the reconstruction process."}, {"color": "rgb(14, 125, 85)", "font": {"color": "white"}, "id": "DIRECTION VJ", "label": "DIRECTION VJ", "shape": "dot", "size": 10, "title": "The unit vector from light position oj to a 3D point p(t)."}, {"color": "rgb(14, 125, 85)", "font": {"color": "white"}, "id": "R(P(T), VJP)", "label": "R(P(T), VJP)", "shape": "dot", "size": 10, "title": "The reflectance property at point\u202fp(t) for viewing direction\u202fvjp, describing how much incident light is reflected.\nthe reflectance property at point p(t) in the direction of light source vj"}, {"color": "rgb(178, 204, 15)", "font": {"color": "white"}, "id": "FIG. 12", "label": "FIG. 12", "shape": "dot", "size": 10, "title": "A figure displaying results for walking and sitting down motions"}, {"color": "rgb(178, 204, 15)", "font": {"color": "white"}, "id": "DYNAMIC OBJECTS", "label": "DYNAMIC OBJECTS", "shape": "dot", "size": 10, "title": "Objects that move within the scene\nObjects whose geometry or appearance changes over time, studied in computer vision for temporal 3D reconstruction."}, {"color": "rgb(178, 204, 15)", "font": {"color": "white"}, "id": "MULTIVIEW DATASETS", "label": "MULTIVIEW DATASETS", "shape": "dot", "size": 10, "title": "Collections of images captured from multiple viewpoints of a scene, used to train neural rendering models as a prior."}, {"color": "rgb(178, 204, 15)", "font": {"color": "white"}, "id": "SCENE PRIOR", "label": "SCENE PRIOR", "shape": "dot", "size": 10, "title": "A pre-trained representation of typical scene geometry and appearance that guides reconstruction when limited observations are available."}, {"color": "rgb(14, 125, 85)", "font": {"color": "white"}, "id": "LAMBERTIAN SURFACE", "label": "LAMBERTIAN SURFACE", "shape": "dot", "size": 10, "title": "A surface model assuming diffuse reflectance, where reflected intensity is independent of viewing direction."}, {"color": "rgb(184, 117, 145)", "font": {"color": "white"}, "id": "OTTO SEISKARI", "label": "OTTO SEISKARI", "shape": "dot", "size": 10, "title": "Otto Seiskari is a co\u2011author of the Dn-splatter paper."}, {"color": "rgb(184, 117, 145)", "font": {"color": "white"}, "id": "DN-SPLATTER", "label": "DN-SPLATTER", "shape": "dot", "size": 10, "title": "Dn-splatter is a method that incorporates depth and normal priors into Gaussian splatting for improved 3D reconstruction and meshing."}, {"color": "rgb(29, 29, 90)", "font": {"color": "white"}, "id": "TONY TUNG", "label": "TONY TUNG", "shape": "dot", "size": 10, "title": "Author of the multi-view reconstruction paper"}, {"color": "rgb(29, 29, 90)", "font": {"color": "white"}, "id": "COMPLETE MULTI-VIEW RECONSTRUCTION OF DYNAMIC SCENES FROM PROBABILISTIC FUSION OF NARROW AND WIDE BASELINE STEREO", "label": "COMPLETE MULTI-VIEW RECONSTRUCTION OF DYNAMIC SCENES FROM PROBABILISTIC FUSION OF NARROW AND WIDE BASELINE STEREO", "shape": "dot", "size": 10, "title": "A method for reconstructing dynamic scenes using probabilistic fusion of stereo images"}, {"color": "rgb(187, 58, 56)", "font": {"color": "white"}, "id": "DANFEI XU", "label": "DANFEI XU", "shape": "dot", "size": 10, "title": "Researcher and co\u2011author of the Instantsplat paper."}, {"color": "rgb(99, 215, 10)", "font": {"color": "white"}, "id": "DENSE CORRESPONDENCE ESTIMATION", "label": "DENSE CORRESPONDENCE ESTIMATION", "shape": "dot", "size": 10, "title": "The process of establishing a dense mapping between pixels in multiple images to infer 3D structure"}, {"color": "rgb(200, 199, 161)", "font": {"color": "white"}, "id": "TIME-MULTIPLEXED ILLUMINATION", "label": "TIME-MULTIPLEXED ILLUMINATION", "shape": "dot", "size": 10, "title": "A technique where multiple light sources are activated sequentially to capture different lighting views."}, {"color": "rgb(187, 58, 56)", "font": {"color": "white"}, "id": "MARCO PAVONE", "label": "MARCO PAVONE", "shape": "dot", "size": 10, "title": "Researcher and co\u2011author of the Instantsplat paper.\nMarco Pavone is an author of an unspecified paper cited in the document.\nMarco Pavone is a researcher who co-authored the paper Freenerf: Improving few-shot neural rendering with free frequency regularization."}, {"color": "rgb(187, 58, 56)", "font": {"color": "white"}, "id": "YUE WANG", "label": "YUE WANG", "shape": "dot", "size": 10, "title": "Researcher and co\u2011author of the Instantsplat paper.\nYue Wang is an author of an unspecified paper cited in the document.\nYue Wang is a researcher who co-authored the paper Freenerf: Improving few-shot neural rendering with free frequency regularization."}, {"color": "rgb(70, 100, 22)", "font": {"color": "white"}, "id": "PERSON IN MOTION", "label": "PERSON IN MOTION", "shape": "dot", "size": 10, "title": "A human subject whose movement is recorded in the demultiplexed image frames"}, {"color": "rgb(57, 92, 92)", "font": {"color": "white"}, "id": "Y. MUKAIGAWA", "label": "Y. MUKAIGAWA", "shape": "dot", "size": 10, "title": "Japanese researcher known for work on multiplexed illumination techniques for BRDF measurement."}, {"color": "rgb(57, 92, 92)", "font": {"color": "white"}, "id": "K. SUMINO", "label": "K. SUMINO", "shape": "dot", "size": 10, "title": "Co-author of the 2007 paper on multiplexed illumination for BRDF measurement."}, {"color": "rgb(143, 201, 116)", "font": {"color": "white"}, "id": "TOTAL-RECON", "label": "TOTAL-RECON", "shape": "dot", "size": 10, "title": "A 2023 ICCV paper introducing a deformable scene reconstruction method for embodied view synthesis."}, {"color": "rgb(236, 30, 243)", "font": {"color": "white"}, "id": "HIROMU KATO", "label": "HIROMU KATO", "shape": "dot", "size": 10, "title": "Researcher and co\u2011author of the multi\u2011view neural surface reconstruction paper."}, {"color": "rgb(119, 10, 68)", "font": {"color": "white"}, "id": "EQ. (1)", "label": "EQ. (1)", "shape": "dot", "size": 10, "title": "formula used to calculate the intensity of an image point by integrating along view directions\nThe equation used to compute intensity based on irradiance and other parameters."}, {"color": "rgb(119, 10, 68)", "font": {"color": "white"}, "id": "LOSS TO MINIMIZE", "label": "LOSS TO MINIMIZE", "shape": "dot", "size": 10, "title": "the objective function defined to be minimized during optimization"}, {"color": "rgb(222, 222, 124)", "font": {"color": "white"}, "id": "X", "label": "X", "shape": "dot", "size": 10, "title": "3D point in the scene whose intensity is evaluated."}, {"color": "rgb(222, 222, 124)", "font": {"color": "white"}, "id": "PX(T)", "label": "PX(T)", "shape": "dot", "size": 10, "title": "Parametric representation of the ray passing through point x.\na 3D point located at distance t from the camera along a ray"}, {"color": "rgb(32, 135, 241)", "font": {"color": "white"}, "id": "FANBO XIANG", "label": "FANBO XIANG", "shape": "dot", "size": 10, "title": "A researcher who contributed to the Mvsnerf paper."}, {"color": "rgb(14, 125, 85)", "font": {"color": "white"}, "id": "LARGE IMAGE SET", "label": "LARGE IMAGE SET", "shape": "dot", "size": 10, "title": "a substantial collection of images needed to train a multi-layer perceptron"}, {"color": "rgb(70, 100, 22)", "font": {"color": "white"}, "id": "PATTERN RECOGNITION (CVPR\u201906)", "label": "PATTERN RECOGNITION (CVPR\u201906)", "shape": "dot", "size": 10, "title": "A 2006 computer vision paper on pattern recognition presented at CVPR, covering techniques for identifying patterns in visual data."}, {"color": "rgb(57, 62, 17)", "font": {"color": "white"}, "id": "ACM SIGGRAPH ASIA 2009", "label": "ACM SIGGRAPH ASIA 2009", "shape": "dot", "size": 10, "title": "A conference where research papers on computer graphics and interactive techniques are presented"}, {"color": "rgb(105, 132, 238)", "font": {"color": "white"}, "id": "ALEX YU", "label": "ALEX YU", "shape": "dot", "size": 10, "title": "A researcher who co\u2011authored the paper \u201cpixelnerf: Neural radiance \ufb01elds from one or few images.\u201d"}, {"color": "rgb(105, 132, 238)", "font": {"color": "white"}, "id": "ANGJOO KANAZAWA", "label": "ANGJOO KANAZAWA", "shape": "dot", "size": 10, "title": "Co\u2011author of the \u201cpixelnerf\u201d paper."}, {"color": "rgb(130, 83, 45)", "font": {"color": "white"}, "id": "ERRORS", "label": "ERRORS", "shape": "dot", "size": 10, "title": "Quantitative differences between reconstructed shapes and ground truth, measured by Chamfer distance\nThe magnitude of deviation between reconstructed surfaces and ground truth, quantified in the study"}, {"color": "rgb(236, 30, 243)", "font": {"color": "white"}, "id": "ARXIV PREPRINT ARXIV:2211.11971", "label": "ARXIV PREPRINT ARXIV:2211.11971", "shape": "dot", "size": 10, "title": "An online preprint of the multi\u2011view neural surface reconstruction paper, available on arXiv in 2022."}, {"color": "rgb(178, 204, 15)", "font": {"color": "white"}, "id": "DEMULTIPLEXING", "label": "DEMULTIPLEXING", "shape": "dot", "size": 10, "title": "The method of separating combined images into individual images per light source."}, {"color": "rgb(178, 204, 15)", "font": {"color": "white"}, "id": "DEMODULATES", "label": "DEMODULATES", "shape": "dot", "size": 10, "title": "The operation of extracting individual light source contributions from captured images."}, {"color": "rgb(222, 222, 124)", "font": {"color": "white"}, "id": "SURFACE SHAPE", "label": "SURFACE SHAPE", "shape": "dot", "size": 10, "title": "The set of points p where f(p)=0, representing the geometry of the surface."}, {"color": "rgb(222, 222, 124)", "font": {"color": "white"}, "id": "MLPS", "label": "MLPS", "shape": "dot", "size": 10, "title": "multilayer perceptrons that represent functions f(p) and c(p(t), vkx) in the model\nMulti\u2011Layer Perceptrons, neural network architectures used to realize the SDF and intensity functions."}, {"color": "rgb(222, 222, 124)", "font": {"color": "white"}, "id": "TK(P(T))", "label": "TK(P(T))", "shape": "dot", "size": 10, "title": "the cumulative transmittance function for point p(t) in the NeuS framework"}, {"color": "rgb(14, 125, 85)", "font": {"color": "white"}, "id": "J-TH LIGHT SOURCE", "label": "J-TH LIGHT SOURCE", "shape": "dot", "size": 10, "title": "A discrete light emitter indexed by\u202fj, providing illumination to the scene.\nA specific individual light source indexed by j, used to illuminate the scene during data capture."}, {"color": "rgb(187, 58, 56)", "font": {"color": "white"}, "id": "WENYAN CONG", "label": "WENYAN CONG", "shape": "dot", "size": 10, "title": "Researcher and co\u2011author of the Instantsplat paper."}, {"color": "rgb(187, 58, 56)", "font": {"color": "white"}, "id": "XINGHAO DING", "label": "XINGHAO DING", "shape": "dot", "size": 10, "title": "Researcher and co\u2011author of the Instantsplat paper."}, {"color": "rgb(187, 58, 56)", "font": {"color": "white"}, "id": "BORIS IVANOVIC", "label": "BORIS IVANOVIC", "shape": "dot", "size": 10, "title": "Researcher and co\u2011author of the Instantsplat paper."}, {"color": "rgb(187, 58, 56)", "font": {"color": "white"}, "id": "GEORGIOS PAVLAKOS", "label": "GEORGIOS PAVLAKOS", "shape": "dot", "size": 10, "title": "Researcher and co\u2011author of the Instantsplat paper."}, {"color": "rgb(187, 58, 56)", "font": {"color": "white"}, "id": "ZHANGYANG WANG", "label": "ZHANGYANG WANG", "shape": "dot", "size": 10, "title": "Researcher and co\u2011author of the Instantsplat paper."}, {"color": "rgb(32, 135, 241)", "font": {"color": "white"}, "id": "MULTIVIEW SYSTEM", "label": "MULTIVIEW SYSTEM", "shape": "dot", "size": 10, "title": "System that uses multiple camera viewpoints to capture comprehensive visual data"}, {"color": "rgb(62, 243, 139)", "font": {"color": "white"}, "id": "TRAINING DATASETS", "label": "TRAINING DATASETS", "shape": "dot", "size": 10, "title": "The collections of images or data used to train neural rendering models."}, {"color": "rgb(62, 243, 139)", "font": {"color": "white"}, "id": "SIMILARITY", "label": "SIMILARITY", "shape": "dot", "size": 10, "title": "A measure of how closely the training data matches the target scene, influencing reconstruction accuracy."}, {"color": "rgb(213, 59, 55)", "font": {"color": "white"}, "id": "REAL-TIME", "label": "REAL-TIME", "shape": "dot", "size": 10, "title": "Indicates that the 3D Gaussian Splatting method is capable of rendering at interactive frame rates."}, {"color": "rgb(70, 100, 22)", "font": {"color": "white"}, "id": "ZHENGQI LI", "label": "ZHENGQI LI", "shape": "dot", "size": 10, "title": "Co-author of the paper \"Iron: Inverse rendering by optimizing neural sdfs and materials from photometric images\""}, {"color": "rgb(178, 204, 15)", "font": {"color": "white"}, "id": "KOTA NISHIHARA", "label": "KOTA NISHIHARA", "shape": "dot", "size": 10, "title": "Co-author and researcher at Kyushu University"}, {"color": "rgb(105, 132, 238)", "font": {"color": "white"}, "id": "CAMERA PARAMETER ESTIMATION", "label": "CAMERA PARAMETER ESTIMATION", "shape": "dot", "size": 10, "title": "The process of determining camera intrinsic and extrinsic parameters from image data"}, {"color": "rgb(14, 125, 85)", "font": {"color": "white"}, "id": "TRANSMITTANCE TJ(P(T))", "label": "TRANSMITTANCE TJ(P(T))", "shape": "dot", "size": 10, "title": "The fraction of light that passes through a medium along a ray, dependent on position p(t)\nThe optical transmittance function along a ray from a light source, used to model light attenuation in volume rendering."}, {"color": "rgb(14, 125, 85)", "font": {"color": "white"}, "id": "INTENSITY CJ(P(T), VJP)", "label": "INTENSITY CJ(P(T), VJP)", "shape": "dot", "size": 10, "title": "The light intensity function depending on point p(t) and view direction vjp, modeled as a neural network.\nThe measured light intensity along a ray direction from a camera, parameterized by position p(t) and view direction vjp"}, {"color": "rgb(70, 100, 22)", "font": {"color": "white"}, "id": "MULTI-VIEW NORMAL INTEGRATION", "label": "MULTI-VIEW NORMAL INTEGRATION", "shape": "dot", "size": 10, "title": "Combining normal maps from multiple viewpoints to reconstruct a detailed surface."}, {"color": "rgb(105, 132, 238)", "font": {"color": "white"}, "id": "AJAY JAIN", "label": "AJAY JAIN", "shape": "dot", "size": 10, "title": "Researcher involved in developing few-shot view synthesis methods."}, {"color": "rgb(105, 132, 238)", "font": {"color": "white"}, "id": "PIETER ABBEEL", "label": "PIETER ABBEEL", "shape": "dot", "size": 10, "title": "Researcher involved in developing few-shot view synthesis methods."}, {"color": "rgb(184, 117, 145)", "font": {"color": "white"}, "id": "IAROSLAV MELEKHOV", "label": "IAROSLAV MELEKHOV", "shape": "dot", "size": 10, "title": "Iaroslav Melekhov is a co\u2011author of the Dn-splatter paper."}, {"color": "rgb(57, 62, 17)", "font": {"color": "white"}, "id": "BOXIN SHI", "label": "BOXIN SHI", "shape": "dot", "size": 10, "title": "Researcher and co\u2011author of the multi\u2011view photometric stereo paper."}, {"color": "rgb(70, 100, 22)", "font": {"color": "white"}, "id": "CHENG LIN", "label": "CHENG LIN", "shape": "dot", "size": 10, "title": "Cheng Lin is a researcher who co-authored the paper \u201cNero: Neural geometry and brdf reconstruction of reflective objects.\u201d"}, {"color": "rgb(14, 125, 85)", "font": {"color": "white"}, "id": "LIGHT POSITION OJ", "label": "LIGHT POSITION OJ", "shape": "dot", "size": 10, "title": "The spatial location of the j\u2011th light source."}, {"color": "rgb(32, 135, 241)", "font": {"color": "white"}, "id": "BENJAMIN ATTAL", "label": "BENJAMIN ATTAL", "shape": "dot", "size": 10, "title": "Researcher and co-author of the 2023 paper on neural fields for structured lighting."}, {"color": "rgb(32, 135, 241)", "font": {"color": "white"}, "id": "CHRISTIAN RICHARDT", "label": "CHRISTIAN RICHARDT", "shape": "dot", "size": 10, "title": "Researcher and co-author of the 2023 paper on neural fields for structured lighting."}, {"color": "rgb(143, 201, 116)", "font": {"color": "white"}, "id": "181_NEURAL_4D_SCENE_RECONSTRUC.PDF", "label": "181_NEURAL_4D_SCENE_RECONSTRUC.PDF", "shape": "dot", "size": 10, "title": "PDF containing research on 4D scene reconstruction presented at CVPR 2024.\nPDF document containing the cited research papers and related content."}, {"color": "rgb(103, 156, 25)", "font": {"color": "white"}, "id": "BINBIN HUANG", "label": "BINBIN HUANG", "shape": "dot", "size": 10, "title": "Binbin Huang is a researcher cited as an author in the paper \u00272d gaussian splatting for geometrically accurate radiance fields\u0027"}, {"color": "rgb(14, 125, 85)", "font": {"color": "white"}, "id": "SURFACE MESH RECONSTRUCTION VIA SDF", "label": "SURFACE MESH RECONSTRUCTION VIA SDF", "shape": "dot", "size": 10, "title": "A method of building a 3D surface mesh from a signed distance function representation"}, {"color": "rgb(154, 161, 89)", "font": {"color": "white"}, "id": "METHODS", "label": "METHODS", "shape": "dot", "size": 10, "title": "The procedural steps or algorithms employed in Multiview stereo to achieve 3D reconstruction.\nTechniques tested to reconstruct the 3D scene\nDifferent algorithmic approaches compared in the study"}, {"color": "rgb(154, 161, 89)", "font": {"color": "white"}, "id": "OBJECT", "label": "OBJECT", "shape": "dot", "size": 10, "title": "A specific item or entity within a scene whose complete shape is to be captured.\nThe item being reconstructed and illuminated in the simulation"}, {"color": "rgb(70, 100, 22)", "font": {"color": "white"}, "id": "BACKGROUND MASK", "label": "BACKGROUND MASK", "shape": "dot", "size": 10, "title": "A technique typically used to isolate foreground objects, not employed in the proposed method."}, {"color": "rgb(70, 100, 22)", "font": {"color": "white"}, "id": "PLANAR SURFACE", "label": "PLANAR SURFACE", "shape": "dot", "size": 10, "title": "Flat surface placed under the objects to aid in shadow analysis\nA flat background surface successfully reconstructed by the method."}, {"color": "rgb(70, 100, 22)", "font": {"color": "white"}, "id": "OCCLUDED AREAS", "label": "OCCLUDED AREAS", "shape": "dot", "size": 10, "title": "Regions of the scene that are not visible or are hidden from the cameras, leading to reconstruction challenges"}, {"color": "rgb(213, 59, 55)", "font": {"color": "white"}, "id": "ACM TRANSACTIONS ON GRAPHICS", "label": "ACM TRANSACTIONS ON GRAPHICS", "shape": "dot", "size": 10, "title": "A peer\u2011reviewed journal published by ACM that covers advances in computer graphics and interactive techniques\nPeer\u2011reviewed journal publishing research on computer graphics, including the 2022 neural graphics paper."}, {"color": "rgb(70, 100, 22)", "font": {"color": "white"}, "id": "GAUSSIAN SPLATTING", "label": "GAUSSIAN SPLATTING", "shape": "dot", "size": 10, "title": "A rendering technique that projects Gaussian kernels onto a surface to produce smooth depth maps.\nGaussian splatting is a rendering technique that represents geometry with Gaussian kernels for efficient surface reconstruction.\nA rendering technique that represents surfaces with Gaussian functions for efficient synthesis."}, {"color": "rgb(70, 100, 22)", "font": {"color": "white"}, "id": "TSDF", "label": "TSDF", "shape": "dot", "size": 10, "title": "Truncated Signed Distance Function, a volumetric representation used to fuse depth maps into a coherent 3D surface."}, {"color": "rgb(57, 62, 17)", "font": {"color": "white"}, "id": "ILYA BARAN", "label": "ILYA BARAN", "shape": "dot", "size": 10, "title": "Ilya Baran is a co\u2011author of the dynamic shape capture paper."}, {"color": "rgb(57, 62, 17)", "font": {"color": "white"}, "id": "PAUL DEBEVEC", "label": "PAUL DEBEVEC", "shape": "dot", "size": 10, "title": "Paul Debevec is a co\u2011author of the dynamic shape capture paper."}, {"color": "rgb(57, 62, 17)", "font": {"color": "white"}, "id": "JOVAN POPOVI\u0106", "label": "JOVAN POPOVI\u0106", "shape": "dot", "size": 10, "title": "Jovan Popovi\u0107 is a co\u2011author of the dynamic shape capture paper."}, {"color": "rgb(14, 125, 85)", "font": {"color": "white"}, "id": "EIKONAL TERM", "label": "EIKONAL TERM", "shape": "dot", "size": 10, "title": "A regularization term that penalizes deviations from unit gradient magnitude in the SDF."}, {"color": "rgb(14, 125, 85)", "font": {"color": "white"}, "id": "LREG", "label": "LREG", "shape": "dot", "size": 10, "title": "The Eikonal regularization term that enforces the signed distance function (SDF) property."}, {"color": "rgb(70, 100, 22)", "font": {"color": "white"}, "id": "PLASTER OBJECT", "label": "PLASTER OBJECT", "shape": "dot", "size": 10, "title": "A 0.6\u202fm tall plaster object placed at the center of the scene\nThe physical 3D model displayed in Fig.\u202f9 whose shape is measured and reconstructed\nA physical object whose captured and demultiplexed images are shown in the third row of Figure\u202f9\nA 0.6\u202fm high physical model made of plaster, positioned at the center of the scene for scanning and reconstruction"}, {"color": "rgb(70, 100, 22)", "font": {"color": "white"}, "id": "TABLE\u202f3", "label": "TABLE\u202f3", "shape": "dot", "size": 10, "title": "Tabulated Chamfer distances between reference and measured shapes"}, {"color": "rgb(32, 135, 241)", "font": {"color": "white"}, "id": "ANPEI CHEN", "label": "ANPEI CHEN", "shape": "dot", "size": 10, "title": "A researcher who contributed to the Mvsnerf paper on fast generalizable radiance field reconstruction.\nAnpei Chen is a researcher cited as an author in the paper \u00272d gaussian splatting for geometrically accurate radiance fields\u0027"}, {"color": "rgb(143, 201, 116)", "font": {"color": "white"}, "id": "IEEE/CVF CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION (CVPR)", "label": "IEEE/CVF CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION (CVPR)", "shape": "dot", "size": 10, "title": "Annual international conference focused on computer vision and pattern recognition research, held by IEEE and CVF, where papers such as the referenced 4D scene reconstruction work are presented."}, {"color": "rgb(70, 100, 22)", "font": {"color": "white"}, "id": "IRRADIANCE LOSS LG", "label": "IRRADIANCE LOSS LG", "shape": "dot", "size": 10, "title": "A loss term introduced in the proposed method to penalize discrepancies in predicted irradiance, thereby encouraging physically plausible lighting estimates.\nA loss function that penalizes deviations in estimated light intensity, used to improve reconstruction accuracy."}, {"color": "rgb(70, 100, 22)", "font": {"color": "white"}, "id": "MVPS-BASED METHODS", "label": "MVPS-BASED METHODS", "shape": "dot", "size": 10, "title": "A class of multi-view photometric stereo techniques used for 3D reconstruction, exemplified by IRON and SuperNormal"}, {"color": "rgb(70, 100, 22)", "font": {"color": "white"}, "id": "PARSA MIRDEHGHAN", "label": "PARSA MIRDEHGHAN", "shape": "dot", "size": 10, "title": "Researcher and author of the Turbosl paper"}, {"color": "rgb(70, 100, 22)", "font": {"color": "white"}, "id": "MAXX WU", "label": "MAXX WU", "shape": "dot", "size": 10, "title": "Researcher and author of the Turbosl paper"}, {"color": "rgb(70, 100, 22)", "font": {"color": "white"}, "id": "WENZHENG CHEN", "label": "WENZHENG CHEN", "shape": "dot", "size": 10, "title": "Researcher and author of the Turbosl paper"}, {"color": "rgb(70, 100, 22)", "font": {"color": "white"}, "id": "DAVID B. LINDELL", "label": "DAVID B. LINDELL", "shape": "dot", "size": 10, "title": "Researcher and author of the Turbosl paper"}, {"color": "rgb(32, 135, 241)", "font": {"color": "white"}, "id": "YI WEI", "label": "YI WEI", "shape": "dot", "size": 10, "title": "Yi Wei is a researcher who co\u2011authored the paper \u201cNerfingmvs: Guided optimization of neural radiance fields for indoor multi\u2011view stereo.\u201d"}, {"color": "rgb(32, 135, 241)", "font": {"color": "white"}, "id": "YONGMING RAO", "label": "YONGMING RAO", "shape": "dot", "size": 10, "title": "Yongming Rao is a researcher who co\u2011authored the paper \u201cNerfingmvs: Guided optimization of neural radiance fields for indoor multi\u2011view stereo.\u201d"}, {"color": "rgb(32, 135, 241)", "font": {"color": "white"}, "id": "JIWEN LU", "label": "JIWEN LU", "shape": "dot", "size": 10, "title": "Jiwen Lu is a researcher who co\u2011authored the paper \u201cNerfingmvs: Guided optimization of neural radiance fields for indoor multi\u2011view stereo.\u201d"}, {"color": "rgb(32, 135, 241)", "font": {"color": "white"}, "id": "JIE ZHOU", "label": "JIE ZHOU", "shape": "dot", "size": 10, "title": "Jie Zhou is a researcher who co\u2011authored the paper \u201cNerfingmvs: Guided optimization of neural radiance fields for indoor multi\u2011view stereo.\u201d"}, {"color": "rgb(32, 135, 241)", "font": {"color": "white"}, "id": "SOCIAL MOTION CAPTURE", "label": "SOCIAL MOTION CAPTURE", "shape": "dot", "size": 10, "title": "The process of recording human movement from multiple viewpoints for analysis or animation."}, {"color": "rgb(171, 2, 163)", "font": {"color": "white"}, "id": "6 LIGHTS FOR HOTDOG", "label": "6 LIGHTS FOR HOTDOG", "shape": "dot", "size": 10, "title": "Six light sources dedicated to illuminating the Hotdog scene"}, {"color": "rgb(171, 2, 163)", "font": {"color": "white"}, "id": "2 CAMERAS", "label": "2 CAMERAS", "shape": "dot", "size": 10, "title": "Two cameras employed for the Hotdog reconstruction scenario"}, {"color": "rgb(51, 253, 7)", "font": {"color": "white"}, "id": "MEASUREMENT-BASED APPROACHES", "label": "MEASUREMENT-BASED APPROACHES", "shape": "dot", "size": 10, "title": "Methods that rely on explicit sensor measurements (e.g., depth, point clouds) to build 3D reconstructions"}, {"color": "rgb(51, 253, 7)", "font": {"color": "white"}, "id": "FEED-FORWARD SPARSE-VIEW RECONSTRUCTION METHODS", "label": "FEED-FORWARD SPARSE-VIEW RECONSTRUCTION METHODS", "shape": "dot", "size": 10, "title": "Techniques that reconstruct 3D scenes from a limited number of views using a forward\u2011pass neural network, often trading accuracy for convenience"}, {"color": "rgb(57, 107, 249)", "font": {"color": "white"}, "id": "ANTONIO TORRALBA", "label": "ANTONIO TORRALBA", "shape": "dot", "size": 10, "title": "Antonio Torralba is a researcher who co-authored the paper \u201cBarf: Bundle-adjusting neural radiance \ufb01elds\u201d."}, {"color": "rgb(70, 100, 22)", "font": {"color": "white"}, "id": "CONTROLLED LIGHTS", "label": "CONTROLLED LIGHTS", "shape": "dot", "size": 10, "title": "Lighting conditions whose channel information is used or ignored by a reconstruction method"}, {"color": "rgb(178, 204, 15)", "font": {"color": "white"}, "id": "YUICHI OTA", "label": "YUICHI OTA", "shape": "dot", "size": 10, "title": "A researcher in computer vision, co\u2011author of a dense 3D reconstruction method using a single pattern for fast moving objects."}, {"color": "rgb(178, 204, 15)", "font": {"color": "white"}, "id": "NAOKI ASADA", "label": "NAOKI ASADA", "shape": "dot", "size": 10, "title": "A researcher in computer vision, co\u2011author of a dense 3D reconstruction method using a single pattern for fast moving objects."}, {"color": "rgb(222, 222, 124)", "font": {"color": "white"}, "id": "N", "label": "N", "shape": "dot", "size": 10, "title": "Number of light sources illuminating the scene."}, {"color": "rgb(222, 222, 124)", "font": {"color": "white"}, "id": "M", "label": "M", "shape": "dot", "size": 10, "title": "Number of camera images captured."}, {"color": "rgb(222, 222, 124)", "font": {"color": "white"}, "id": "OK", "label": "OK", "shape": "dot", "size": 10, "title": "Camera position for the k\u2011th image."}, {"color": "rgb(222, 222, 124)", "font": {"color": "white"}, "id": "\u02c6CKX", "label": "\u02c6CKX", "shape": "dot", "size": 10, "title": "The estimated intensity at pixel x for the k\u2011th captured image, produced by the neural network.\nThe estimated value of C at a particular surface position kx, derived from the high-density area and used to infer surface geometry."}, {"color": "rgb(184, 117, 145)", "font": {"color": "white"}, "id": "MATIAS TURKULAINEN", "label": "MATIAS TURKULAINEN", "shape": "dot", "size": 10, "title": "Matias Turkulainen is a researcher contributing to the Dn-splatter paper on depth and normal priors for Gaussian splatting and meshing."}, {"color": "rgb(184, 117, 145)", "font": {"color": "white"}, "id": "JUHO KANNALA", "label": "JUHO KANNALA", "shape": "dot", "size": 10, "title": "Juho Kannala is a co\u2011author of the Dn-splatter paper."}, {"color": "rgb(105, 132, 238)", "font": {"color": "white"}, "id": "MESH", "label": "MESH", "shape": "dot", "size": 10, "title": "A 3D surface representation composed of vertices, edges, and faces\na 3D representation composed of vertices, edges, and faces"}, {"color": "rgb(105, 132, 238)", "font": {"color": "white"}, "id": "DS", "label": "DS", "shape": "dot", "size": 10, "title": "A reconstruction approach that optimizes a mesh generated from object masks\nA NeRF-based method referenced as (d) in the study"}, {"color": "rgb(184, 117, 145)", "font": {"color": "white"}, "id": "MESHING", "label": "MESHING", "shape": "dot", "size": 10, "title": "Meshing refers to the process of generating a mesh representation from point or volumetric data."}, {"color": "rgb(70, 100, 22)", "font": {"color": "white"}, "id": "KAI ZHANG", "label": "KAI ZHANG", "shape": "dot", "size": 10, "title": "Author of the paper \"Iron: Inverse rendering by optimizing neural sdfs and materials from photometric images\""}, {"color": "rgb(70, 100, 22)", "font": {"color": "white"}, "id": "FUJUN LUAN", "label": "FUJUN LUAN", "shape": "dot", "size": 10, "title": "Co-author of the paper \"Iron: Inverse rendering by optimizing neural sdfs and materials from photometric images\""}, {"color": "rgb(178, 204, 15)", "font": {"color": "white"}, "id": "SDF F(PX(T))", "label": "SDF F(PX(T))", "shape": "dot", "size": 10, "title": "The signed distance function that maps a point px(t) in 4D space to its distance from the surface, a key function to be estimated."}, {"color": "rgb(103, 156, 25)", "font": {"color": "white"}, "id": "ZEHAO YU", "label": "ZEHAO YU", "shape": "dot", "size": 10, "title": "Zehao Yu is a researcher cited as an author in the paper \u00272d gaussian splatting for geometrically accurate radiance fields\u0027"}, {"color": "rgb(70, 100, 22)", "font": {"color": "white"}, "id": "MULTIPLE LIGHT SOURCES", "label": "MULTIPLE LIGHT SOURCES", "shape": "dot", "size": 10, "title": "Several distinct illumination sources used in a scene to capture varied lighting conditions, enhancing the richness of photometric information."}, {"color": "rgb(70, 100, 22)", "font": {"color": "white"}, "id": "NUMBER OF CAMERAS", "label": "NUMBER OF CAMERAS", "shape": "dot", "size": 10, "title": "The count of camera viewpoints employed in the reconstruction process; experiments vary this number to assess impact."}, {"color": "rgb(70, 100, 22)", "font": {"color": "white"}, "id": "TABLE 2", "label": "TABLE 2", "shape": "dot", "size": 10, "title": "Numerical comparison of reconstruction quality metrics across different experimental settings."}, {"color": "rgb(70, 100, 22)", "font": {"color": "white"}, "id": "FIGURE\u202f9", "label": "FIGURE\u202f9", "shape": "dot", "size": 10, "title": "Illustration of the plaster object used for Chamfer distance evaluation\nIllustration showing results of the 3D reconstruction for various methods"}, {"color": "rgb(143, 201, 116)", "font": {"color": "white"}, "id": "CHONGHYUK SONG", "label": "CHONGHYUK SONG", "shape": "dot", "size": 10, "title": "Researcher and co-author of the 2023 paper on Total-recon."}, {"color": "rgb(143, 201, 116)", "font": {"color": "white"}, "id": "JUN\u2011YAN ZHU", "label": "JUN\u2011YAN ZHU", "shape": "dot", "size": 10, "title": "Researcher and co-author of the 2023 paper on Total-recon."}, {"color": "rgb(57, 107, 249)", "font": {"color": "white"}, "id": "ICCV", "label": "ICCV", "shape": "dot", "size": 10, "title": "ICCV 2021 is the specific edition of the conference hosting the Barf paper.\nThe International Conference on Computer Vision, a premier biennial event that publishes cutting\u2011edge research in computer vision.\nInternational Conference on Computer Vision, 2023, where the Total-recon paper was presented.\nInternational Conference on Computer Vision, a major annual event in computer vision research, held in 2023 in this context."}, {"color": "rgb(104, 88, 124)", "font": {"color": "white"}, "id": "3D SHAPE", "label": "3D SHAPE", "shape": "dot", "size": 10, "title": "the three\u2011dimensional geometry of an object"}, {"color": "rgb(104, 88, 124)", "font": {"color": "white"}, "id": "SYSTEMS", "label": "SYSTEMS", "shape": "dot", "size": 10, "title": "software/hardware setups that recover the 3D shape of moving objects using cameras"}, {"color": "rgb(99, 106, 157)", "font": {"color": "white"}, "id": "FEW-SHOT NEURAL VOLUME RENDERING", "label": "FEW-SHOT NEURAL VOLUME RENDERING", "shape": "dot", "size": 10, "title": "Rendering 3D volumes from a limited number of input views using neural networks"}, {"color": "rgb(213, 59, 55)", "font": {"color": "white"}, "id": "GEORGIOS KOPANAS", "label": "GEORGIOS KOPANAS", "shape": "dot", "size": 10, "title": "Co-author of the 3D Gaussian Splatting paper."}, {"color": "rgb(222, 222, 124)", "font": {"color": "white"}, "id": "SCALAR VALUE", "label": "SCALAR VALUE", "shape": "dot", "size": 10, "title": "The numerical output of the SDF, representing the distance from p to the closest surface."}, {"color": "rgb(222, 222, 124)", "font": {"color": "white"}, "id": "SURFACE", "label": "SURFACE", "shape": "dot", "size": 10, "title": "The true geometric boundary of the object being reconstructed; the SDF zero level set.\nThe true 3D boundary that the reconstruction algorithm aims to approximate.\nThe outer boundary of the scene or object where high-density areas may be offset from during iteration.\nThe geometric surface being reconstructed in the 4D scene reconstruction process."}, {"color": "rgb(70, 100, 22)", "font": {"color": "white"}, "id": "METHODS USING A LARGER NUMBER OF CAMERAS", "label": "METHODS USING A LARGER NUMBER OF CAMERAS", "shape": "dot", "size": 10, "title": "Alternative reconstruction approaches that employ more cameras to reduce occlusion"}, {"color": "rgb(178, 204, 15)", "font": {"color": "white"}, "id": "SURFACE POINT", "label": "SURFACE POINT", "shape": "dot", "size": 10, "title": "A specific point on the surface of the scene whose properties (e.g., normal, albedo) are estimated by photometric stereo."}, {"color": "rgb(178, 204, 15)", "font": {"color": "white"}, "id": "FIGURE 2", "label": "FIGURE 2", "shape": "dot", "size": 10, "title": "Illustration showing a scene illuminated by seven different point light positions"}, {"color": "rgb(178, 204, 15)", "font": {"color": "white"}, "id": "MVS", "label": "MVS", "shape": "dot", "size": 10, "title": "Multi\u2011View Stereo, a traditional 3D reconstruction approach that matches points across multiple images to recover depth.\nMulti-View Stereo, a technique to reconstruct 3D geometry from multiple images."}, {"color": "rgb(178, 204, 15)", "font": {"color": "white"}, "id": "BACKGROUND CURTAIN", "label": "BACKGROUND CURTAIN", "shape": "dot", "size": 10, "title": "A curtain surrounding the scene, with a diameter of about five meters."}, {"color": "rgb(99, 215, 10)", "font": {"color": "white"}, "id": "ACTIVE-LIGHT-BASED APPROACHES", "label": "ACTIVE-LIGHT-BASED APPROACHES", "shape": "dot", "size": 10, "title": "Techniques that use projected light patterns to aid in 3D or 4D reconstruction, reducing the need for many cameras."}, {"color": "rgb(152, 180, 111)", "font": {"color": "white"}, "id": "ICML", "label": "ICML", "shape": "dot", "size": 10, "title": "The International Conference on Machine Learning, where the Sharf paper was presented in 2021."}, {"color": "rgb(213, 59, 55)", "font": {"color": "white"}, "id": "GEORGE DRETTAKIS", "label": "GEORGE DRETTAKIS", "shape": "dot", "size": 10, "title": "Co-author of the 3D Gaussian Splatting paper."}, {"color": "rgb(178, 204, 15)", "font": {"color": "white"}, "id": "YASUYUKI MATSUSHITA", "label": "YASUYUKI MATSUSHITA", "shape": "dot", "size": 10, "title": "Co-author of the active lighting and computer vision publication."}, {"color": "rgb(185, 21, 66)", "font": {"color": "white"}, "id": "VOXEL DENSITIES", "label": "VOXEL DENSITIES", "shape": "dot", "size": 10, "title": "A volumetric representation where each voxel stores a density value indicating occupancy or material."}, {"color": "rgb(46, 237, 214)", "font": {"color": "white"}, "id": "IRRADIANCE CONSTRAINT", "label": "IRRADIANCE CONSTRAINT", "shape": "dot", "size": 10, "title": "A condition that relates the light ray and camera ray to enforce consistency of illumination at a point."}, {"color": "rgb(46, 237, 214)", "font": {"color": "white"}, "id": "RAY FROM LIGHT", "label": "RAY FROM LIGHT", "shape": "dot", "size": 10, "title": "A ray emitted from a light source used to model illumination in the scene."}, {"color": "rgb(46, 237, 214)", "font": {"color": "white"}, "id": "INTENSITY", "label": "INTENSITY", "shape": "dot", "size": 10, "title": "The brightness value measured at a point on the true surface, relevant for irradiance constraints."}, {"color": "rgb(46, 237, 214)", "font": {"color": "white"}, "id": "MONOTONICITY", "label": "MONOTONICITY", "shape": "dot", "size": 10, "title": "The property that irradiance should decrease consistently along a light ray."}, {"color": "rgb(70, 100, 22)", "font": {"color": "white"}, "id": "JIEPENG WANG", "label": "JIEPENG WANG", "shape": "dot", "size": 10, "title": "Jiepeng Wang is a researcher who co-authored the paper \u201cNero: Neural geometry and brdf reconstruction of reflective objects.\u201d"}, {"color": "rgb(154, 161, 89)", "font": {"color": "white"}, "id": "3D GAUSSIAN SPLATTING (3DGS)", "label": "3D GAUSSIAN SPLATTING (3DGS)", "shape": "dot", "size": 10, "title": "A rendering technique that represents 3D scenes as a collection of Gaussian blobs for efficient synthesis"}, {"color": "rgb(154, 161, 89)", "font": {"color": "white"}, "id": "MONOCULAR", "label": "MONOCULAR", "shape": "dot", "size": 10, "title": "Single\u2011view depth estimation method"}, {"color": "rgb(154, 161, 89)", "font": {"color": "white"}, "id": "MULTIVIEW DEPTH", "label": "MULTIVIEW DEPTH", "shape": "dot", "size": 10, "title": "Depth information obtained from multiple camera views"}, {"color": "rgb(154, 161, 89)", "font": {"color": "white"}, "id": "RESULTS", "label": "RESULTS", "shape": "dot", "size": 10, "title": "Output visualizations or quantitative metrics demonstrating performance"}, {"color": "rgb(105, 132, 238)", "font": {"color": "white"}, "id": "VIEW-INDEPENDENT SURFACE", "label": "VIEW-INDEPENDENT SURFACE", "shape": "dot", "size": 10, "title": "A surface representation generated independently of specific viewpoints, used as a form of regularization"}, {"color": "rgb(105, 132, 238)", "font": {"color": "white"}, "id": "MIN CHEN", "label": "MIN CHEN", "shape": "dot", "size": 10, "title": "Min Chen is a researcher who co\u2011authored the paper \u201cNeRF\u2212\u2212: Neural radiance fields without known camera parameters.\u201d"}, {"color": "rgb(178, 204, 15)", "font": {"color": "white"}, "id": "MULTI-CHANNEL IMAGE", "label": "MULTI-CHANNEL IMAGE", "shape": "dot", "size": 10, "title": "An image composed of multiple single-channel images stacked together, each captured under different illumination conditions."}, {"color": "rgb(178, 204, 15)", "font": {"color": "white"}, "id": "IMAGE", "label": "IMAGE", "shape": "dot", "size": 10, "title": "A single-channel photograph captured under a specific illumination, used as a component of a multi-channel image."}, {"color": "rgb(178, 204, 15)", "font": {"color": "white"}, "id": "SIGNAL-TO-NOISE RATIO (SNR)", "label": "SIGNAL-TO-NOISE RATIO (SNR)", "shape": "dot", "size": 10, "title": "A metric indicating the level of desired signal relative to background noise, often a challenge in low\u2011light imaging"}, {"color": "rgb(152, 180, 111)", "font": {"color": "white"}, "id": "VITTORIO FERRARI", "label": "VITTORIO FERRARI", "shape": "dot", "size": 10, "title": "A researcher who co\u2011authored the paper \u201cSharf: Shape-conditioned radiance fields from a single view.\u201d"}, {"color": "rgb(143, 28, 146)", "font": {"color": "white"}, "id": "OPTICAL FLOW", "label": "OPTICAL FLOW", "shape": "dot", "size": 10, "title": "A computational method for estimating motion between consecutive images, used for motion compensation."}, {"color": "rgb(143, 28, 146)", "font": {"color": "white"}, "id": "WENGER ET AL.", "label": "WENGER ET AL.", "shape": "dot", "size": 10, "title": "Researchers who introduced motion compensation using optical flow for moving subjects in imaging."}, {"color": "rgb(143, 28, 146)", "font": {"color": "white"}, "id": "MOVING SUBJECTS", "label": "MOVING SUBJECTS", "shape": "dot", "size": 10, "title": "Objects or entities that are in motion within a scene."}, {"color": "rgb(70, 100, 22)", "font": {"color": "white"}, "id": "TAKU KOMURA", "label": "TAKU KOMURA", "shape": "dot", "size": 10, "title": "Taku Komura is a researcher who co-authored the paper \u201cNero: Neural geometry and brdf reconstruction of reflective objects.\u201d\nResearcher and author of the cited paper"}, {"color": "rgb(105, 132, 238)", "font": {"color": "white"}, "id": "OBJECT MASKS", "label": "OBJECT MASKS", "shape": "dot", "size": 10, "title": "Binary images indicating object silhouettes used to guide mesh generation"}, {"color": "rgb(57, 107, 249)", "font": {"color": "white"}, "id": "SIMON LUCEY", "label": "SIMON LUCEY", "shape": "dot", "size": 10, "title": "Simon Lucey is a researcher who co-authored the paper \u201cBarf: Bundle-adjusting neural radiance \ufb01elds\u201d."}, {"color": "rgb(105, 132, 238)", "font": {"color": "white"}, "id": "ZIRUI WANG", "label": "ZIRUI WANG", "shape": "dot", "size": 10, "title": "Zirui Wang is a researcher who co\u2011authored the paper \u201cNeRF\u2212\u2212: Neural radiance fields without known camera parameters.\u201d"}, {"color": "rgb(105, 132, 238)", "font": {"color": "white"}, "id": "SHANGZHE WU", "label": "SHANGZHE WU", "shape": "dot", "size": 10, "title": "Shangzhe Wu is a researcher who co\u2011authored the paper \u201cNeRF\u2212\u2212: Neural radiance fields without known camera parameters.\u201d"}, {"color": "rgb(32, 135, 241)", "font": {"color": "white"}, "id": "ARXIV PREPRINT", "label": "ARXIV PREPRINT", "shape": "dot", "size": 10, "title": "A preprint posted on the arXiv repository, typically used for early dissemination of research findings in computer science and related fields.\narXiv preprint is the platform where the NeRF\u2212\u2212 paper was first made publicly available."}, {"color": "rgb(204, 18, 150)", "font": {"color": "white"}, "id": "MUKAIGAWA ET AL.", "label": "MUKAIGAWA ET AL.", "shape": "dot", "size": 10, "title": "Researchers who extended illumination techniques to estimate an object\u0027s Bidirectional Reflectance Distribution Function (BRDF)."}, {"color": "rgb(204, 18, 150)", "font": {"color": "white"}, "id": "BRDF", "label": "BRDF", "shape": "dot", "size": 10, "title": "Bidirectional Reflectance Distribution Function, a model describing how light is reflected at an opaque surface."}, {"color": "rgb(178, 204, 15)", "font": {"color": "white"}, "id": "DIAMETER", "label": "DIAMETER", "shape": "dot", "size": 10, "title": "The size of the background curtain, approximately five meters."}, {"color": "rgb(29, 29, 90)", "font": {"color": "white"}, "id": "TAKASHI MATSUYAMA", "label": "TAKASHI MATSUYAMA", "shape": "dot", "size": 10, "title": "Co-author of the multi-view reconstruction paper"}, {"color": "rgb(14, 125, 85)", "font": {"color": "white"}, "id": "POSITIONAL ENCODING", "label": "POSITIONAL ENCODING", "shape": "dot", "size": 10, "title": "A technique that maps spatial coordinates into a higher\u2011dimensional space using sinusoidal functions, often with frequency regularization, to improve neural network learning of spatial patterns."}, {"color": "rgb(29, 29, 90)", "font": {"color": "white"}, "id": "NARROW BASELINE STEREO", "label": "NARROW BASELINE STEREO", "shape": "dot", "size": 10, "title": "Stereo imaging with small baseline"}, {"color": "rgb(29, 29, 90)", "font": {"color": "white"}, "id": "PROBABILISTIC FUSION", "label": "PROBABILISTIC FUSION", "shape": "dot", "size": 10, "title": "Method combining narrow and wide baseline stereo data"}, {"color": "rgb(213, 59, 55)", "font": {"color": "white"}, "id": "THOMAS M\u00dcLLER", "label": "THOMAS M\u00dcLLER", "shape": "dot", "size": 10, "title": "Computer graphics researcher who co-authored the 2022 paper on instant neural graphics primitives."}, {"color": "rgb(213, 59, 55)", "font": {"color": "white"}, "id": "CHRISTOPH SCHIED", "label": "CHRISTOPH SCHIED", "shape": "dot", "size": 10, "title": "Co-author of the 2022 paper on instant neural graphics primitives."}, {"color": "rgb(4, 95, 99)", "font": {"color": "white"}, "id": "WENLI XU", "label": "WENLI XU", "shape": "dot", "size": 10, "title": "Wenli Xu is a researcher who co-authored a point-cloud-based multiview stereo algorithm for free-viewpoint video."}, {"color": "rgb(4, 95, 99)", "font": {"color": "white"}, "id": "FREE-VIEWPOINT VIDEO", "label": "FREE-VIEWPOINT VIDEO", "shape": "dot", "size": 10, "title": "A video format that allows the viewer to change the viewpoint freely, requiring advanced 3D reconstruction techniques."}, {"color": "rgb(4, 95, 99)", "font": {"color": "white"}, "id": "IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS", "label": "IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS", "shape": "dot", "size": 10, "title": "A peer-reviewed journal where the multiview stereo algorithm was published."}, {"color": "rgb(4, 95, 99)", "font": {"color": "white"}, "id": "2009", "label": "2009", "shape": "dot", "size": 10, "title": "Year the multiview stereo algorithm was published.\nYear of the IEEE conference presentation"}, {"color": "rgb(57, 107, 249)", "font": {"color": "white"}, "id": "CHEN-HSUAN LIN", "label": "CHEN-HSUAN LIN", "shape": "dot", "size": 10, "title": "Chen-Hsuan Lin is a researcher who co-authored the paper \u201cBarf: Bundle-adjusting neural radiance \ufb01elds\u201d."}, {"color": "rgb(57, 107, 249)", "font": {"color": "white"}, "id": "WEI-CHIU MA", "label": "WEI-CHIU MA", "shape": "dot", "size": 10, "title": "Wei-Chiu Ma is a researcher who co-authored the paper \u201cBarf: Bundle-adjusting neural radiance \ufb01elds\u201d."}, {"color": "rgb(103, 113, 222)", "font": {"color": "white"}, "id": "NEURAL\u2011BASED METHODS", "label": "NEURAL\u2011BASED METHODS", "shape": "dot", "size": 10, "title": "Modern MVS techniques that incorporate neural rendering models such as NeRF, NeuS, and 3DGS to improve reconstruction quality."}, {"color": "rgb(103, 113, 222)", "font": {"color": "white"}, "id": "SFM/V\u2011SLAM", "label": "SFM/V\u2011SLAM", "shape": "dot", "size": 10, "title": "Structure\u2011from\u2011Motion and Visual SLAM frameworks that estimate camera motion and scene structure from moving\u2011camera image sequences."}, {"color": "rgb(103, 113, 222)", "font": {"color": "white"}, "id": "STATIC SCENES", "label": "STATIC SCENES", "shape": "dot", "size": 10, "title": "Environments that do not contain moving objects, assumed by many neural\u2011based MVS methods."}, {"color": "rgb(99, 106, 157)", "font": {"color": "white"}, "id": "MIJEONG KIM", "label": "MIJEONG KIM", "shape": "dot", "size": 10, "title": "A researcher who co\u2011authored the paper \u201cInfonerf\u201d on neural volume rendering"}, {"color": "rgb(14, 125, 85)", "font": {"color": "white"}, "id": "RAY DIRECTION", "label": "RAY DIRECTION", "shape": "dot", "size": 10, "title": "The line along which light travels from a camera towards a point in space"}, {"color": "rgb(178, 204, 15)", "font": {"color": "white"}, "id": "4D SCENES", "label": "4D SCENES", "shape": "dot", "size": 10, "title": "Dynamic scenes that include temporal changes, effectively a 3D space plus time dimension.\nThree-dimensional spatial data that also includes a temporal dimension, representing how a scene evolves over time.\nA spatial-temporal representation of environments that includes time as a fourth dimension, used for dynamic scene reconstruction"}, {"color": "rgb(103, 113, 222)", "font": {"color": "white"}, "id": "STATIONARY MULTI\u2011CAMERA SETUPS", "label": "STATIONARY MULTI\u2011CAMERA SETUPS", "shape": "dot", "size": 10, "title": "MVS methods that employ multiple fixed cameras simultaneously to capture a scene from different viewpoints."}, {"color": "rgb(155, 169, 205)", "font": {"color": "white"}, "id": "P. DEBEVEC", "label": "P. DEBEVEC", "shape": "dot", "size": 10, "title": "P. Debevec is an author cited in the same 2005 SIGGRAPH paper on performance relighting and reflectance transformation with time-multiplexed illumination."}, {"color": "rgb(155, 169, 205)", "font": {"color": "white"}, "id": "HAWKINS", "label": "HAWKINS", "shape": "dot", "size": 10, "title": "Hawkins is an author cited in a 2005 SIGGRAPH paper on performance relighting and reflectance transformation with time-multiplexed illumination."}, {"color": "rgb(14, 125, 85)", "font": {"color": "white"}, "id": "SHARED PARAMETERS", "label": "SHARED PARAMETERS", "shape": "dot", "size": 10, "title": "Weights and biases that are common across multiple MLP instances, enabling parameter sharing between camera and light ray computations."}, {"color": "rgb(143, 28, 146)", "font": {"color": "white"}, "id": "SIGNAL PROCESSING", "label": "SIGNAL PROCESSING", "shape": "dot", "size": 10, "title": "Techniques for analyzing, modifying, and synthesizing signals to improve information extraction."}, {"color": "rgb(158, 206, 123)", "font": {"color": "white"}, "id": "ROBUST MULTIVIEW PHOTOMETRIC STEREO", "label": "ROBUST MULTIVIEW PHOTOMETRIC STEREO", "shape": "dot", "size": 10, "title": "A method for estimating surface normals and reflectance from multiple images."}, {"color": "rgb(70, 100, 22)", "font": {"color": "white"}, "id": "WENPING WANG", "label": "WENPING WANG", "shape": "dot", "size": 10, "title": "Wenping Wang is a researcher who co-authored the paper \u201cNero: Neural geometry and brdf reconstruction of reflective objects.\u201d\nResearcher and author of the cited paper"}, {"color": "rgb(32, 135, 241)", "font": {"color": "white"}, "id": "FUQIANG ZHAO", "label": "FUQIANG ZHAO", "shape": "dot", "size": 10, "title": "A researcher who contributed to the Mvsnerf paper."}, {"color": "rgb(143, 28, 146)", "font": {"color": "white"}, "id": "SAGAWA ET AL.", "label": "SAGAWA ET AL.", "shape": "dot", "size": 10, "title": "Researchers who applied signal processing to capture illumination on fast\u2011moving objects under strong external light."}, {"color": "rgb(32, 135, 241)", "font": {"color": "white"}, "id": "IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV)", "label": "IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV)", "shape": "dot", "size": 10, "title": "A premier annual conference in the field of computer vision, where the Panoptic studio was showcased in 2015."}, {"color": "rgb(99, 215, 10)", "font": {"color": "white"}, "id": "FIG. 3(A)", "label": "FIG. 3(A)", "shape": "dot", "size": 10, "title": "Illustration showing a narrow baseline configuration where projected points P1 and P2 are close together."}, {"color": "rgb(228, 9, 85)", "font": {"color": "white"}, "id": "PIXEL X", "label": "PIXEL X", "shape": "dot", "size": 10, "title": "A single location in a camera image where intensity values are recorded."}, {"color": "rgb(105, 132, 238)", "font": {"color": "white"}, "id": "FRANCESCO SARNO", "label": "FRANCESCO SARNO", "shape": "dot", "size": 10, "title": "Co-author of the 2022 Neural Radiance Fields paper."}, {"color": "rgb(105, 132, 238)", "font": {"color": "white"}, "id": "IEEE/CVF WINTER CONFERENCE ON APPLICATIONS OF COMPUTER VISION", "label": "IEEE/CVF WINTER CONFERENCE ON APPLICATIONS OF COMPUTER VISION", "shape": "dot", "size": 10, "title": "Conference where the 2022 Neural Radiance Fields paper was presented."}, {"color": "rgb(178, 204, 15)", "font": {"color": "white"}, "id": "KATSUSHI IKEUCHI", "label": "KATSUSHI IKEUCHI", "shape": "dot", "size": 10, "title": "Researcher and author of works on active lighting and computer vision."}, {"color": "rgb(57, 107, 249)", "font": {"color": "white"}, "id": "SAURABH SINGH", "label": "SAURABH SINGH", "shape": "dot", "size": 10, "title": "Co\u2011author of the same paper."}, {"color": "rgb(222, 222, 124)", "font": {"color": "white"}, "id": "HIGH\u2011DENSITY AREA", "label": "HIGH\u2011DENSITY AREA", "shape": "dot", "size": 10, "title": "A region in the scene where many points or rays are concentrated, affecting reconstruction quality.\nA region in the projected point cloud where point counts per unit area are high, indicating potential surface proximity."}, {"color": "rgb(178, 204, 15)", "font": {"color": "white"}, "id": "SCHECHNER ET AL.", "label": "SCHECHNER ET AL.", "shape": "dot", "size": 10, "title": "Authors who proposed the multiplexed illumination method referenced in the text"}, {"color": "rgb(178, 204, 15)", "font": {"color": "white"}, "id": "SHAPE INFORMATION", "label": "SHAPE INFORMATION", "shape": "dot", "size": 10, "title": "Geometric details inferred from correspondences between camera images and projected patterns."}, {"color": "rgb(99, 215, 10)", "font": {"color": "white"}, "id": "HIGH-FREQUENCY PATTERNS", "label": "HIGH-FREQUENCY PATTERNS", "shape": "dot", "size": 10, "title": "Patterns of light with high spatial frequency used in imaging and reconstruction techniques"}, {"color": "rgb(105, 132, 238)", "font": {"color": "white"}, "id": "REN NG", "label": "REN NG", "shape": "dot", "size": 10, "title": "Researcher and author of the Nerf paper"}, {"color": "rgb(70, 100, 22)", "font": {"color": "white"}, "id": "HIGH-SPEED CAMERAS", "label": "HIGH-SPEED CAMERAS", "shape": "dot", "size": 10, "title": "Cameras capable of capturing images at a high frame rate, used here to obtain 256 images per frame"}, {"color": "rgb(132, 139, 193)", "font": {"color": "white"}, "id": "3 CAMERAS", "label": "3 CAMERAS", "shape": "dot", "size": 10, "title": "Three cameras specifically used for the Lego reconstruction scenario"}, {"color": "rgb(132, 139, 193)", "font": {"color": "white"}, "id": "6 LIGHTS FOR LEGO", "label": "6 LIGHTS FOR LEGO", "shape": "dot", "size": 10, "title": "Six light sources dedicated to illuminating the Lego scene"}, {"color": "rgb(136, 57, 198)", "font": {"color": "white"}, "id": "SUNGHO JO", "label": "SUNGHO JO", "shape": "dot", "size": 10, "title": "Researcher and co\u2011author of the same paper."}, {"color": "rgb(57, 107, 249)", "font": {"color": "white"}, "id": "2023", "label": "2023", "shape": "dot", "size": 10, "title": "The year in which the arXiv preprint was published.\nYear the arXiv preprint was released\nYear of publication for the Neural geometry and BRDF reconstruction study"}, {"color": "rgb(14, 125, 85)", "font": {"color": "white"}, "id": "LOSS", "label": "LOSS", "shape": "dot", "size": 10, "title": "The objective function minimized during training, measuring discrepancy between generated and captured images.\nThe objective function measuring the discrepancy \u2225C(ok, vkx) \u2212\u02c6Ckx\u2225 across all image channels, used to train the reconstruction model."}, {"color": "rgb(228, 9, 85)", "font": {"color": "white"}, "id": "STATIONARY CAMERAS", "label": "STATIONARY CAMERAS", "shape": "dot", "size": 10, "title": "fixed cameras that capture images from limited viewpoints"}, {"color": "rgb(105, 132, 238)", "font": {"color": "white"}, "id": "RAVI RAMAMOORTHI", "label": "RAVI RAMAMOORTHI", "shape": "dot", "size": 10, "title": "Researcher and author of the Nerf paper"}, {"color": "rgb(184, 117, 145)", "font": {"color": "white"}, "id": "ESA RAHTU", "label": "ESA RAHTU", "shape": "dot", "size": 10, "title": "Esa Rahtu is a co\u2011author of the Dn-splatter paper."}, {"color": "rgb(70, 100, 22)", "font": {"color": "white"}, "id": "PENG WANG", "label": "PENG WANG", "shape": "dot", "size": 10, "title": "Peng Wang is a researcher who co-authored the paper \u201cNero: Neural geometry and brdf reconstruction of reflective objects.\u201d\nResearcher and author of the cited paper"}, {"color": "rgb(70, 100, 22)", "font": {"color": "white"}, "id": "LINGJIE LIU", "label": "LINGJIE LIU", "shape": "dot", "size": 10, "title": "Lingjie Liu is a researcher who co-authored the paper \u201cNero: Neural geometry and brdf reconstruction of reflective objects.\u201d\nResearcher and author of the cited paper"}, {"color": "rgb(70, 100, 22)", "font": {"color": "white"}, "id": "YUAN LIU", "label": "YUAN LIU", "shape": "dot", "size": 10, "title": "Yuan Liu is a researcher who co-authored the paper \u201cNero: Neural geometry and brdf reconstruction of reflective objects.\u201d\nResearcher and author of the cited paper"}, {"color": "rgb(70, 100, 22)", "font": {"color": "white"}, "id": "CHRISTIAN THEOBALT", "label": "CHRISTIAN THEOBALT", "shape": "dot", "size": 10, "title": "Researcher and author of the cited paper"}, {"color": "rgb(70, 100, 22)", "font": {"color": "white"}, "id": "NEURAL IMPLICIT SURFACES", "label": "NEURAL IMPLICIT SURFACES", "shape": "dot", "size": 10, "title": "Implicit 3D surface representations learned by neural networks"}, {"color": "rgb(70, 100, 22)", "font": {"color": "white"}, "id": "MULTI-VIEW RECONSTRUCTION", "label": "MULTI-VIEW RECONSTRUCTION", "shape": "dot", "size": 10, "title": "Technique for reconstructing scenes from multiple viewpoints\nReconstructing 3D scenes from multiple camera views"}, {"color": "rgb(243, 72, 250)", "font": {"color": "white"}, "id": "MULTI-CAMERA VIDEOS", "label": "MULTI-CAMERA VIDEOS", "shape": "dot", "size": 10, "title": "Video data captured from multiple synchronized cameras used as input for multi-view stereo reconstruction"}, {"color": "rgb(57, 107, 249)", "font": {"color": "white"}, "id": "S.K. NAYAR", "label": "S.K. NAYAR", "shape": "dot", "size": 10, "title": "Researcher in computer vision and photometry"}, {"color": "rgb(32, 135, 241)", "font": {"color": "white"}, "id": "JINGYI YU", "label": "JINGYI YU", "shape": "dot", "size": 10, "title": "A researcher who contributed to the Mvsnerf paper."}, {"color": "rgb(14, 125, 85)", "font": {"color": "white"}, "id": "CJ(P3, VJP3)", "label": "CJ(P3, VJP3)", "shape": "dot", "size": 10, "title": "A function or coefficient representing light interaction at point P3 with direction vjP3."}, {"color": "rgb(70, 100, 22)", "font": {"color": "white"}, "id": "SIGNAL-TO-NOISE RATIO", "label": "SIGNAL-TO-NOISE RATIO", "shape": "dot", "size": 10, "title": "A metric indicating the level of desired signal relative to background noise in an image"}, {"color": "rgb(14, 125, 85)", "font": {"color": "white"}, "id": "KAYA ET AL.", "label": "KAYA ET AL.", "shape": "dot", "size": 10, "title": "Researchers who employed normal vectors from photometric stereo as input to an MLP for color prediction."}, {"color": "rgb(32, 135, 241)", "font": {"color": "white"}, "id": "HAO SU", "label": "HAO SU", "shape": "dot", "size": 10, "title": "A researcher who contributed to the Mvsnerf paper."}, {"color": "rgb(103, 113, 222)", "font": {"color": "white"}, "id": "MOVING\u2011CAMERA APPROACHES", "label": "MOVING\u2011CAMERA APPROACHES", "shape": "dot", "size": 10, "title": "A subset of MVS methods where the camera moves during image capture, often associated with Structure\u2011from\u2011Motion (SfM) or Visual SLAM (V\u2011SLAM) techniques."}, {"color": "rgb(103, 113, 222)", "font": {"color": "white"}, "id": "MVS METHODS", "label": "MVS METHODS", "shape": "dot", "size": 10, "title": "Multi\u2011view stereo techniques used to reconstruct 3D scenes from multiple images, typically divided into moving\u2011camera and stationary multi\u2011camera approaches.\nMulti\u2011View Stereo methods used for 3D reconstruction, recently improved by neural\u2011based techniques"}, {"color": "rgb(243, 72, 250)", "font": {"color": "white"}, "id": "HIGH-RESOLUTION IMAGES", "label": "HIGH-RESOLUTION IMAGES", "shape": "dot", "size": 10, "title": "Image data with high pixel density used as input for multi-view stereo reconstruction"}, {"color": "rgb(32, 135, 241)", "font": {"color": "white"}, "id": "ANYSPLAT", "label": "ANYSPLAT", "shape": "dot", "size": 10, "title": "Feed\u2011forward 3D Gaussian splatting method that reconstructs scenes from unconstrained views, described in a 2025 ACM TOG paper"}, {"color": "rgb(32, 135, 241)", "font": {"color": "white"}, "id": "ACM TRANSACTIONS ON GRAPHICS (TOG)", "label": "ACM TRANSACTIONS ON GRAPHICS (TOG)", "shape": "dot", "size": 10, "title": "Peer\u2011reviewed journal publishing research on computer graphics, venue for the Anysplat paper"}, {"color": "rgb(32, 135, 241)", "font": {"color": "white"}, "id": "LIHAN JIANG", "label": "LIHAN JIANG", "shape": "dot", "size": 10, "title": "Researcher and author of the paper \u201cAnysplat\u201d"}, {"color": "rgb(57, 62, 17)", "font": {"color": "white"}, "id": "CHANGYU DIAO", "label": "CHANGYU DIAO", "shape": "dot", "size": 10, "title": "Researcher and co\u2011author of the multi\u2011view photometric stereo paper."}, {"color": "rgb(70, 100, 22)", "font": {"color": "white"}, "id": "HIGH-FREQUENCY DETAILS", "label": "HIGH-FREQUENCY DETAILS", "shape": "dot", "size": 10, "title": "details in a 3D reconstruction that vary rapidly and are often lost during processing"}, {"color": "rgb(184, 117, 145)", "font": {"color": "white"}, "id": "XUQIAN REN", "label": "XUQIAN REN", "shape": "dot", "size": 10, "title": "Xuqian Ren is a co\u2011author of the Dn-splatter study on depth and normal priors for Gaussian splatting and meshing."}, {"color": "rgb(172, 138, 158)", "font": {"color": "white"}, "id": "NEAR-FIELD DEVIATIONS", "label": "NEAR-FIELD DEVIATIONS", "shape": "dot", "size": 10, "title": "Errors in rendering caused by insufficient source separation, which become negligible when sources are placed far apart."}, {"color": "rgb(172, 138, 158)", "font": {"color": "white"}, "id": "VOLUMETRIC RENDERING", "label": "VOLUMETRIC RENDERING", "shape": "dot", "size": 10, "title": "The process of generating a 3D image by integrating light contributions over a volume, used here to compute surface properties from multiple viewpoints."}, {"color": "rgb(70, 100, 22)", "font": {"color": "white"}, "id": "DYNAMIC SCENES", "label": "DYNAMIC SCENES", "shape": "dot", "size": 10, "title": "Scenes that change over time, addressed by the probabilistic fusion method"}, {"color": "rgb(99, 106, 157)", "font": {"color": "white"}, "id": "RAY ENTROPY", "label": "RAY ENTROPY", "shape": "dot", "size": 10, "title": "A measure of uncertainty in ray\u2011surface intersections, minimized to improve reconstruction quality"}, {"color": "rgb(57, 62, 17)", "font": {"color": "white"}, "id": "ZHE WU", "label": "ZHE WU", "shape": "dot", "size": 10, "title": "Researcher and co\u2011author of the multi\u2011view photometric stereo paper."}, {"color": "rgb(200, 199, 161)", "font": {"color": "white"}, "id": "REFLECTANCE TRANSFORMATION", "label": "REFLECTANCE TRANSFORMATION", "shape": "dot", "size": 10, "title": "The modification of surface reflectance properties to simulate different materials or lighting conditions."}, {"color": "rgb(57, 62, 17)", "font": {"color": "white"}, "id": "ROBUST SOLUTION AND BENCHMARK DATASET FOR SPATIALLY VARYING ISOTROPIC MATERIALS", "label": "ROBUST SOLUTION AND BENCHMARK DATASET FOR SPATIALLY VARYING ISOTROPIC MATERIALS", "shape": "dot", "size": 10, "title": "A methodological contribution providing a dataset and algorithmic approach for handling materials with spatially varying isotropic reflectance."}, {"color": "rgb(14, 125, 85)", "font": {"color": "white"}, "id": "ILLUMINATION CONDITIONS", "label": "ILLUMINATION CONDITIONS", "shape": "dot", "size": 10, "title": "The varying lighting setups under which images of the scene are captured to provide diverse irradiance data."}, {"color": "rgb(57, 62, 17)", "font": {"color": "white"}, "id": "ZHENGLONG ZHOU", "label": "ZHENGLONG ZHOU", "shape": "dot", "size": 10, "title": "Researcher and co\u2011author of the multi\u2011view photometric stereo paper."}, {"color": "rgb(14, 125, 85)", "font": {"color": "white"}, "id": "\u2207F(P(T))", "label": "\u2207F(P(T))", "shape": "dot", "size": 10, "title": "The gradient of the SDF evaluated along a ray, used in the Eikonal term."}, {"color": "rgb(70, 100, 22)", "font": {"color": "white"}, "id": "3D SCANNER", "label": "3D SCANNER", "shape": "dot", "size": 10, "title": "A handheld device that captures three\u2011dimensional data of an object, providing high\u2011resolution point clouds or meshes"}, {"color": "rgb(99, 215, 10)", "font": {"color": "white"}, "id": "SYSTEM SETUP COSTS", "label": "SYSTEM SETUP COSTS", "shape": "dot", "size": 10, "title": "The financial expense associated with configuring and deploying a large number of cameras for 4D reconstruction."}, {"color": "rgb(70, 100, 22)", "font": {"color": "white"}, "id": "CAPTURED CAMERA IMAGES", "label": "CAPTURED CAMERA IMAGES", "shape": "dot", "size": 10, "title": "Images taken by a camera under the known lighting condition."}, {"color": "rgb(222, 222, 124)", "font": {"color": "white"}, "id": "PROJECTED POINTS", "label": "PROJECTED POINTS", "shape": "dot", "size": 10, "title": "Points obtained by projecting 3D geometry onto a 2D plane for analysis."}, {"color": "rgb(158, 206, 123)", "font": {"color": "white"}, "id": "IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE", "label": "IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE", "shape": "dot", "size": 10, "title": "IEEE Transactions on Pattern Analysis and Machine Intelligence is a peer\u2011reviewed scientific journal where the paper \u0027Multiview photometric stereo\u0027 was published in 2008\nJournal where the multiview photometric stereo paper was published.\nA peer\u2011reviewed journal publishing research in computer vision and pattern analysis."}, {"color": "rgb(94, 131, 77)", "font": {"color": "white"}, "id": "MESHES", "label": "MESHES", "shape": "dot", "size": 10, "title": "Three\u2011dimensional surface representations used in computer graphics and vision."}, {"color": "rgb(94, 131, 77)", "font": {"color": "white"}, "id": "MULTIPLE VIEWS", "label": "MULTIPLE VIEWS", "shape": "dot", "size": 10, "title": "Several distinct camera perspectives from which a scene is observed."}, {"color": "rgb(185, 21, 66)", "font": {"color": "white"}, "id": "SIGNED DISTANCE FIELDS (SDFS)", "label": "SIGNED DISTANCE FIELDS (SDFS)", "shape": "dot", "size": 10, "title": "A continuous function that gives the signed distance from any point in space to the nearest surface of a 3D object."}, {"color": "rgb(126, 255, 173)", "font": {"color": "white"}, "id": "DIFFERENTIABLE STEREOPSIS", "label": "DIFFERENTIABLE STEREOPSIS", "shape": "dot", "size": 10, "title": "A method for reconstructing 3D meshes from multiple camera views using differentiable rendering techniques."}, {"color": "rgb(126, 255, 173)", "font": {"color": "white"}, "id": "DIFFERENTIABLE RENDERING", "label": "DIFFERENTIABLE RENDERING", "shape": "dot", "size": 10, "title": "A rendering pipeline that allows gradients to flow from image space back to 3D geometry."}, {"color": "rgb(57, 62, 17)", "font": {"color": "white"}, "id": "PING TAN", "label": "PING TAN", "shape": "dot", "size": 10, "title": "Researcher and co\u2011author of the multi\u2011view photometric stereo paper."}, {"color": "rgb(130, 83, 45)", "font": {"color": "white"}, "id": "MVPS METHODS", "label": "MVPS METHODS", "shape": "dot", "size": 10, "title": "Multi-View Point Set methods that produce noisy point clouds in this context"}, {"color": "rgb(158, 206, 123)", "font": {"color": "white"}, "id": "PLANAR MESH PARAMETERIZATION", "label": "PLANAR MESH PARAMETERIZATION", "shape": "dot", "size": 10, "title": "A technique for representing surfaces as planar meshes in photometric stereo."}, {"color": "rgb(178, 204, 15)", "font": {"color": "white"}, "id": "ANGLES", "label": "ANGLES", "shape": "dot", "size": 10, "title": "The different viewing directions from which images are taken in MVS."}, {"color": "rgb(226, 210, 2)", "font": {"color": "white"}, "id": "ARTIFACTS", "label": "ARTIFACTS", "shape": "dot", "size": 10, "title": "Unwanted geometric distortions that persist when constraints are relaxed."}, {"color": "rgb(99, 215, 10)", "font": {"color": "white"}, "id": "EQ. (3)", "label": "EQ. (3)", "shape": "dot", "size": 10, "title": "A mathematical equation used for minimization in the reconstruction process, referenced as Eq. (3) in the document.\nA mathematical expression referenced in the text for computing a quantity related to transmittance."}, {"color": "rgb(119, 10, 68)", "font": {"color": "white"}, "id": "EQ. (6)", "label": "EQ. (6)", "shape": "dot", "size": 10, "title": "alternative constraint relating cj(p(t), vjp) to irradiance"}, {"color": "rgb(99, 106, 157)", "font": {"color": "white"}, "id": "RAY ENTROPY MINIMIZATION", "label": "RAY ENTROPY MINIMIZATION", "shape": "dot", "size": 10, "title": "An optimization technique that reduces uncertainty in ray sampling to improve rendering quality"}, {"color": "rgb(29, 29, 90)", "font": {"color": "white"}, "id": "SHOHEI NOBUHARA", "label": "SHOHEI NOBUHARA", "shape": "dot", "size": 10, "title": "Researcher and author of the Panoptic studio paper\nCo-author of the multi-view reconstruction paper"}, {"color": "rgb(105, 132, 238)", "font": {"color": "white"}, "id": "IEEE/CVF CON.", "label": "IEEE/CVF CON.", "shape": "dot", "size": 10, "title": "Conference proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"}, {"color": "rgb(70, 100, 22)", "font": {"color": "white"}, "id": "SHUZHE WANG", "label": "SHUZHE WANG", "shape": "dot", "size": 10, "title": "Co\u2011author of the Dust3r paper, contributing to geometric 3D vision."}, {"color": "rgb(57, 62, 17)", "font": {"color": "white"}, "id": "MIN LI", "label": "MIN LI", "shape": "dot", "size": 10, "title": "Researcher and author of the multi\u2011view photometric stereo paper."}, {"color": "rgb(29, 29, 90)", "font": {"color": "white"}, "id": "IEEE 12TH INTERNATIONAL CONFERENCE ON COMPUTER VISION", "label": "IEEE 12TH INTERNATIONAL CONFERENCE ON COMPUTER VISION", "shape": "dot", "size": 10, "title": "Conference where the multi-view reconstruction paper was presented in 2009"}, {"color": "rgb(62, 243, 139)", "font": {"color": "white"}, "id": "TARGET SCENE", "label": "TARGET SCENE", "shape": "dot", "size": 10, "title": "The specific 3D environment that the model aims to reconstruct."}, {"color": "rgb(136, 57, 198)", "font": {"color": "white"}, "id": "DAEKYUM KIM", "label": "DAEKYUM KIM", "shape": "dot", "size": 10, "title": "Researcher and co\u2011author of the same paper."}, {"color": "rgb(29, 29, 90)", "font": {"color": "white"}, "id": "WIDE BASELINE STEREO", "label": "WIDE BASELINE STEREO", "shape": "dot", "size": 10, "title": "Stereo imaging with large baseline"}, {"color": "rgb(14, 125, 85)", "font": {"color": "white"}, "id": "OPTIMIZATION", "label": "OPTIMIZATION", "shape": "dot", "size": 10, "title": "The iterative adjustment of model parameters to minimize the loss function and improve shape estimation"}, {"color": "rgb(193, 230, 136)", "font": {"color": "white"}, "id": "BART NABBE", "label": "BART NABBE", "shape": "dot", "size": 10, "title": "Researcher and author of the Panoptic studio paper"}, {"color": "rgb(193, 230, 136)", "font": {"color": "white"}, "id": "AMBIGUOUS CORRESPONDENCES", "label": "AMBIGUOUS CORRESPONDENCES", "shape": "dot", "size": 10, "title": "uncertain matching between points or features in different views that can degrade reconstruction quality"}, {"color": "rgb(193, 230, 136)", "font": {"color": "white"}, "id": "TK", "label": "TK", "shape": "dot", "size": 10, "title": "transmittance term associated with the camera direction"}, {"color": "rgb(193, 230, 136)", "font": {"color": "white"}, "id": "REFERENCE SHAPES", "label": "REFERENCE SHAPES", "shape": "dot", "size": 10, "title": "ground truth 3D models obtained from a handheld 3D scanner"}, {"color": "rgb(193, 230, 136)", "font": {"color": "white"}, "id": "RAY SAMPLING", "label": "RAY SAMPLING", "shape": "dot", "size": 10, "title": "the process of sampling rays in 3D space for rendering or reconstruction"}, {"color": "rgb(193, 230, 136)", "font": {"color": "white"}, "id": "4D SCENE RECONSTRUCTION RESULTS", "label": "4D SCENE RECONSTRUCTION RESULTS", "shape": "dot", "size": 10, "title": "The outcomes of reconstructing a scene in four dimensions (spatial + time) using multiple cameras and active lighting."}, {"color": "rgb(193, 230, 136)", "font": {"color": "white"}, "id": "L", "label": "L", "shape": "dot", "size": 10, "title": "The overall loss value, defined as L = LC + \u03b1Lg + \u03b2Lreg."}, {"color": "rgb(193, 230, 136)", "font": {"color": "white"}, "id": "COLOR DIFFERENCES", "label": "COLOR DIFFERENCES", "shape": "dot", "size": 10, "title": "The photometric discrepancy between corresponding pixels in different images, used as a cost function for matching."}, {"color": "rgb(193, 230, 136)", "font": {"color": "white"}, "id": "MACHINE INTELLIGENCE", "label": "MACHINE INTELLIGENCE", "shape": "dot", "size": 10, "title": "Broad field encompassing machine learning and related technologies"}, {"color": "rgb(193, 230, 136)", "font": {"color": "white"}, "id": "LEI TAN", "label": "LEI TAN", "shape": "dot", "size": 10, "title": "Researcher and author of the Panoptic studio paper"}, {"color": "rgb(193, 230, 136)", "font": {"color": "white"}, "id": "Z", "label": "Z", "shape": "dot", "size": 10, "title": "Upper limit of the integral (infinity)."}, {"color": "rgb(193, 230, 136)", "font": {"color": "white"}, "id": "COLMAP", "label": "COLMAP", "shape": "dot", "size": 10, "title": "an open-source SfM and MVS pipeline that provides depth maps and camera poses"}, {"color": "rgb(193, 230, 136)", "font": {"color": "white"}, "id": "DENSE VOLUMETRIC INFORMATION", "label": "DENSE VOLUMETRIC INFORMATION", "shape": "dot", "size": 10, "title": "information about the full 3D volume of an object captured during training"}, {"color": "rgb(193, 230, 136)", "font": {"color": "white"}, "id": "3D POSITION P", "label": "3D POSITION P", "shape": "dot", "size": 10, "title": "A point in three\u2011dimensional space whose coordinates are input to the SDF to obtain a distance value."}, {"color": "rgb(193, 230, 136)", "font": {"color": "white"}, "id": "COLOR", "label": "COLOR", "shape": "dot", "size": 10, "title": "RGB value predicted for a 3D point"}, {"color": "rgb(193, 230, 136)", "font": {"color": "white"}, "id": "BARBARA ROESSLE", "label": "BARBARA ROESSLE", "shape": "dot", "size": 10, "title": "A researcher mentioned in the document, likely a co\u2011author of a related work.\nResearcher and co\u2011author of the paper \u201cDense depth priors for neural radiance fields from sparse input views.\u201d"}, {"color": "rgb(193, 230, 136)", "font": {"color": "white"}, "id": "YU-KUN LAI", "label": "YU-KUN LAI", "shape": "dot", "size": 10, "title": "Co\u2011author of the sparse RGB\u2011D NeRF paper."}, {"color": "rgb(193, 230, 136)", "font": {"color": "white"}, "id": "S", "label": "S", "shape": "dot", "size": 10, "title": "Scale parameter used in the definition of \u03c6(x)."}, {"color": "rgb(193, 230, 136)", "font": {"color": "white"}, "id": "NERF-BASED 3D RECONSTRUCTION", "label": "NERF-BASED 3D RECONSTRUCTION", "shape": "dot", "size": 10, "title": "reconstruction of 3D scenes using neural radiance fields from limited images"}, {"color": "rgb(193, 230, 136)", "font": {"color": "white"}, "id": "MULTICHANNEL IMAGES", "label": "MULTICHANNEL IMAGES", "shape": "dot", "size": 10, "title": "Images captured in multiple channels (e.g., color, depth) used to derive constraints."}, {"color": "rgb(193, 230, 136)", "font": {"color": "white"}, "id": "LIN GUI", "label": "LIN GUI", "shape": "dot", "size": 10, "title": "Researcher and author of the Panoptic studio paper"}, {"color": "rgb(193, 230, 136)", "font": {"color": "white"}, "id": "ADVANTAGE", "label": "ADVANTAGE", "shape": "dot", "size": 10, "title": "The benefit or improvement offered by a method or technology."}, {"color": "rgb(193, 230, 136)", "font": {"color": "white"}, "id": "INTERNATIONAL JOURNAL OF COMPUTER VISION", "label": "INTERNATIONAL JOURNAL OF COMPUTER VISION", "shape": "dot", "size": 10, "title": "A peer\u2011reviewed scientific journal publishing research in computer vision and related fields."}, {"color": "rgb(193, 230, 136)", "font": {"color": "white"}, "id": "44(6)", "label": "44(6)", "shape": "dot", "size": 10, "title": "Volume 44, issue 6 of ACM TOG where Anysplat was published"}, {"color": "rgb(193, 230, 136)", "font": {"color": "white"}, "id": "IMAGES CAPTURED UNDER CONTROLLED ILLUMINATION", "label": "IMAGES CAPTURED UNDER CONTROLLED ILLUMINATION", "shape": "dot", "size": 10, "title": "Photographic inputs taken with a fixed lighting setup to aid accurate 3D reconstruction."}, {"color": "rgb(193, 230, 136)", "font": {"color": "white"}, "id": "T", "label": "T", "shape": "dot", "size": 10, "title": "the scalar distance along a ray from the camera to a point"}, {"color": "rgb(193, 230, 136)", "font": {"color": "white"}, "id": "3DGS", "label": "3DGS", "shape": "dot", "size": 10, "title": "3D Gaussian Splatting, a neural approach that uses Gaussian distributions to represent 3D scenes efficiently.\nA 3D Gaussian Splatting framework used for generating mesh surfaces\nA 3D Gaussian splatting approach that struggles with shading effects from multiple light sources."}, {"color": "rgb(193, 230, 136)", "font": {"color": "white"}, "id": "FIGURE 5", "label": "FIGURE 5", "shape": "dot", "size": 10, "title": "Illustration depicting the workflow of the proposed method."}, {"color": "rgb(193, 230, 136)", "font": {"color": "white"}, "id": "GT", "label": "GT", "shape": "dot", "size": 10, "title": "Ground Truth, the reference standard for evaluating reconstructed shapes"}, {"color": "rgb(193, 230, 136)", "font": {"color": "white"}, "id": "1\u201316", "label": "1\u201316", "shape": "dot", "size": 10, "title": "Page range of the Anysplat article in ACM TOG"}, {"color": "rgb(193, 230, 136)", "font": {"color": "white"}, "id": "SPARSE VIEW RECONSTRUCTION", "label": "SPARSE VIEW RECONSTRUCTION", "shape": "dot", "size": 10, "title": "Reconstructing 3D geometry from a limited number of viewpoints"}, {"color": "rgb(193, 230, 136)", "font": {"color": "white"}, "id": "MOVING HUMANS", "label": "MOVING HUMANS", "shape": "dot", "size": 10, "title": "A type of dynamic object representing human motion"}, {"color": "rgb(193, 230, 136)", "font": {"color": "white"}, "id": "P(T)", "label": "P(T)", "shape": "dot", "size": 10, "title": "The parametric representation of a 3D point in the scene.\nA point on a surface or in space parameterized by time\u202ft, where light interactions are evaluated."}, {"color": "rgb(193, 230, 136)", "font": {"color": "white"}, "id": "XUDONG XU", "label": "XUDONG XU", "shape": "dot", "size": 10, "title": "Researcher and author of the paper \u201cAnysplat\u201d"}, {"color": "rgb(193, 230, 136)", "font": {"color": "white"}, "id": "LIGHTING CONDITION", "label": "LIGHTING CONDITION", "shape": "dot", "size": 10, "title": "The state of illumination assumed to be known for the reconstruction process."}, {"color": "rgb(193, 230, 136)", "font": {"color": "white"}, "id": "2016", "label": "2016", "shape": "dot", "size": 10, "title": "Year the structure-from-motion revisited paper was published"}, {"color": "rgb(193, 230, 136)", "font": {"color": "white"}, "id": "WORKS [8, 42, 64]", "label": "WORKS [8, 42, 64]", "shape": "dot", "size": 10, "title": "research papers that have applied depth from SfM as a constraint in NeRF training"}, {"color": "rgb(193, 230, 136)", "font": {"color": "white"}, "id": "\u0392", "label": "\u0392", "shape": "dot", "size": 10, "title": "Weight coefficient for the Lreg term in the loss function."}, {"color": "rgb(193, 230, 136)", "font": {"color": "white"}, "id": "MULTIPLE KNOWN PATTERNS", "label": "MULTIPLE KNOWN PATTERNS", "shape": "dot", "size": 10, "title": "a set of predefined patterns required for some reconstruction methods"}, {"color": "rgb(193, 230, 136)", "font": {"color": "white"}, "id": "SPRINGER", "label": "SPRINGER", "shape": "dot", "size": 10, "title": "Academic publisher of the active lighting and computer vision book."}, {"color": "rgb(193, 230, 136)", "font": {"color": "white"}, "id": "VIEWING DIRECTION", "label": "VIEWING DIRECTION", "shape": "dot", "size": 10, "title": "The direction from which a 3D point is observed"}, {"color": "rgb(193, 230, 136)", "font": {"color": "white"}, "id": "MATTHIAS NIESSNER", "label": "MATTHIAS NIESSNER", "shape": "dot", "size": 10, "title": "Researcher and co\u2011author of the paper \u201cDense depth priors for neural radiance fields from sparse input views.\u201d"}, {"color": "rgb(193, 230, 136)", "font": {"color": "white"}, "id": "CJ(P1, VJP1)", "label": "CJ(P1, VJP1)", "shape": "dot", "size": 10, "title": "A mathematical notation representing a point or vector in a 3D space used in the context of light ray calculations."}, {"color": "rgb(193, 230, 136)", "font": {"color": "white"}, "id": "DYNAMIC SCENE CAPTURE", "label": "DYNAMIC SCENE CAPTURE", "shape": "dot", "size": 10, "title": "The process of capturing scenes that change over time, often using multiple cameras to record motion and depth."}, {"color": "rgb(193, 230, 136)", "font": {"color": "white"}, "id": "CAMERA POSITION", "label": "CAMERA POSITION", "shape": "dot", "size": 10, "title": "the origin point from which rays are cast in the scene"}, {"color": "rgb(193, 230, 136)", "font": {"color": "white"}, "id": "YUTAKA SATOH", "label": "YUTAKA SATOH", "shape": "dot", "size": 10, "title": "Researcher and co\u2011author of the paper \u201cIlluminant\u2011camera communication to observe moving objects under strong external light by spread spectrum modulation.\u201d"}, {"color": "rgb(193, 230, 136)", "font": {"color": "white"}, "id": "LEIF KOBBELT", "label": "LEIF KOBBELT", "shape": "dot", "size": 10, "title": "Co\u2011author of the sparse RGB\u2011D NeRF paper."}, {"color": "rgb(193, 230, 136)", "font": {"color": "white"}, "id": "YU-JIE YUAN", "label": "YU-JIE YUAN", "shape": "dot", "size": 10, "title": "Researcher who co\u2011authored a paper on sparse RGB\u2011D NeRFs."}, {"color": "rgb(193, 230, 136)", "font": {"color": "white"}, "id": "2 CAMS", "label": "2 CAMS", "shape": "dot", "size": 10, "title": "Number of cameras in a further experimental condition"}, {"color": "rgb(193, 230, 136)", "font": {"color": "white"}, "id": "SEVEN CHANNELS", "label": "SEVEN CHANNELS", "shape": "dot", "size": 10, "title": "The combined image data channels resulting from merging six images per viewpoint."}, {"color": "rgb(193, 230, 136)", "font": {"color": "white"}, "id": "EXTERNAL LIGHTS", "label": "EXTERNAL LIGHTS", "shape": "dot", "size": 10, "title": "Ambient lighting sources such as ceiling lights that illuminate the scene but are not part of the controlled setup"}, {"color": "rgb(193, 230, 136)", "font": {"color": "white"}, "id": "STATIC OBJECTS", "label": "STATIC OBJECTS", "shape": "dot", "size": 10, "title": "Objects that do not move within the scene"}, {"color": "rgb(193, 230, 136)", "font": {"color": "white"}, "id": "NETWORK PARAMETERS", "label": "NETWORK PARAMETERS", "shape": "dot", "size": 10, "title": "The learnable weights of the neural network being optimized."}, {"color": "rgb(193, 230, 136)", "font": {"color": "white"}, "id": "\u0391", "label": "\u0391", "shape": "dot", "size": 10, "title": "Weight coefficient for the Lg term in the loss function."}, {"color": "rgb(193, 230, 136)", "font": {"color": "white"}, "id": "CALIBRATED LIGHT SOURCE", "label": "CALIBRATED LIGHT SOURCE", "shape": "dot", "size": 10, "title": "A light source whose intensity and position are precisely measured and aligned with the camera for accurate imaging."}, {"color": "rgb(193, 230, 136)", "font": {"color": "white"}, "id": "TAKEO KANADE", "label": "TAKEO KANADE", "shape": "dot", "size": 10, "title": "Researcher and author of the Panoptic studio paper"}, {"color": "rgb(193, 230, 136)", "font": {"color": "white"}, "id": "JIANGMIAO PANG", "label": "JIANGMIAO PANG", "shape": "dot", "size": 10, "title": "Researcher and author of the paper \u201cAnysplat\u201d"}, {"color": "rgb(193, 230, 136)", "font": {"color": "white"}, "id": "SIX IMAGES", "label": "SIX IMAGES", "shape": "dot", "size": 10, "title": "The set of images captured from six distinct viewpoints in the simulation."}, {"color": "rgb(193, 230, 136)", "font": {"color": "white"}, "id": "HAO LIU", "label": "HAO LIU", "shape": "dot", "size": 10, "title": "Researcher and author of the Panoptic studio paper"}, {"color": "rgb(193, 230, 136)", "font": {"color": "white"}, "id": "CAMERA RAYS", "label": "CAMERA RAYS", "shape": "dot", "size": 10, "title": "lines extending from the camera center through pixels into the scene"}, {"color": "rgb(193, 230, 136)", "font": {"color": "white"}, "id": "CHAMFER DISTANCE ERRORS", "label": "CHAMFER DISTANCE ERRORS", "shape": "dot", "size": 10, "title": "a quantitative measure of the discrepancy between two point clouds, used here to evaluate reconstruction accuracy"}, {"color": "rgb(193, 230, 136)", "font": {"color": "white"}, "id": "RGB-D DATA", "label": "RGB-D DATA", "shape": "dot", "size": 10, "title": "Data that includes both color (RGB) and depth information used in 3D reconstruction"}, {"color": "rgb(193, 230, 136)", "font": {"color": "white"}, "id": "SYNTHETIC DATASETS", "label": "SYNTHETIC DATASETS", "shape": "dot", "size": 10, "title": "Artificially generated collections of data used for training and evaluating computer vision and reconstruction algorithms\nArtificially generated data used for training and evaluation"}, {"color": "rgb(193, 230, 136)", "font": {"color": "white"}, "id": "TAO LU", "label": "TAO LU", "shape": "dot", "size": 10, "title": "Researcher and author of the paper \u201cAnysplat\u201d"}, {"color": "rgb(193, 230, 136)", "font": {"color": "white"}, "id": "LIN GAO", "label": "LIN GAO", "shape": "dot", "size": 10, "title": "Co\u2011author of the sparse RGB\u2011D NeRF paper."}, {"color": "rgb(193, 230, 136)", "font": {"color": "white"}, "id": "FIG. 1", "label": "FIG. 1", "shape": "dot", "size": 10, "title": "A diagram illustrating the layout of the capturing system, showing the arrangement of cameras and light sources around the target.\nA figure showing standing-up and jump\u2011ing motion results"}, {"color": "rgb(193, 230, 136)", "font": {"color": "white"}, "id": "PATTERN ANALYSIS", "label": "PATTERN ANALYSIS", "shape": "dot", "size": 10, "title": "Field of study involving the extraction of patterns from data"}, {"color": "rgb(193, 230, 136)", "font": {"color": "white"}, "id": "STRUCTURED LIGHT-BASED METHODS", "label": "STRUCTURED LIGHT-BASED METHODS", "shape": "dot", "size": 10, "title": "A category of reconstruction techniques that use structured light, including ActiveNeuS and TurboSL"}, {"color": "rgb(193, 230, 136)", "font": {"color": "white"}, "id": "LARGE SCENE", "label": "LARGE SCENE", "shape": "dot", "size": 10, "title": "A broad or extensive environment or area that is being scanned or reconstructed."}, {"color": "rgb(193, 230, 136)", "font": {"color": "white"}, "id": "KERUI REN", "label": "KERUI REN", "shape": "dot", "size": 10, "title": "Researcher and author of the paper \u201cAnysplat\u201d"}, {"color": "rgb(193, 230, 136)", "font": {"color": "white"}, "id": "VOLUME REPRESENTATIONS", "label": "VOLUME REPRESENTATIONS", "shape": "dot", "size": 10, "title": "a 3D representation that encodes density or occupancy in a volumetric grid"}, {"color": "rgb(193, 230, 136)", "font": {"color": "white"}, "id": "2003", "label": "2003", "shape": "dot", "size": 10, "title": "Year the multiplexed illumination paper was published"}, {"color": "rgb(193, 230, 136)", "font": {"color": "white"}, "id": "FIG. 11", "label": "FIG. 11", "shape": "dot", "size": 10, "title": "A figure in the document illustrating the results of the 4D scene reconstruction"}, {"color": "rgb(193, 230, 136)", "font": {"color": "white"}, "id": "SPARSE INPUT VIEWS", "label": "SPARSE INPUT VIEWS", "shape": "dot", "size": 10, "title": "A small number of camera images used as input for depth and radiance field estimation."}, {"color": "rgb(193, 230, 136)", "font": {"color": "white"}, "id": "LINNING XU", "label": "LINNING XU", "shape": "dot", "size": 10, "title": "Researcher and author of the paper \u201cAnysplat\u201d"}, {"color": "rgb(193, 230, 136)", "font": {"color": "white"}, "id": "NOHA RADWAN.REGNERF", "label": "NOHA RADWAN.REGNERF", "shape": "dot", "size": 10, "title": "A regularized neural radiance field method for view synthesis from sparse inputs, presented at CVPR 2022."}, {"color": "rgb(193, 230, 136)", "font": {"color": "white"}, "id": "MULTILAYER PERCEPTRONS (MLPS)", "label": "MULTILAYER PERCEPTRONS (MLPS)", "shape": "dot", "size": 10, "title": "Feed\u2011forward neural networks with multiple hidden layers used to model density and color for 3D points"}, {"color": "rgb(193, 230, 136)", "font": {"color": "white"}, "id": "NEURAL-BASED APPROACHES", "label": "NEURAL-BASED APPROACHES", "shape": "dot", "size": 10, "title": "Machine learning methods that employ neural networks to predict scene properties such as density and color"}, {"color": "rgb(193, 230, 136)", "font": {"color": "white"}, "id": "FIXED PATTERN", "label": "FIXED PATTERN", "shape": "dot", "size": 10, "title": "The spatial pattern emitted by the laser light sources, not used in the proposed method."}, {"color": "rgb(193, 230, 136)", "font": {"color": "white"}, "id": "SIGNAL", "label": "SIGNAL", "shape": "dot", "size": 10, "title": "The electrical or optical signal that is demodulated after low\u2011frequency removal"}, {"color": "rgb(193, 230, 136)", "font": {"color": "white"}, "id": "OCCLUSION REGULARIZATION", "label": "OCCLUSION REGULARIZATION", "shape": "dot", "size": 10, "title": "a constraint that penalizes unrealistic occlusions in the rendered scene"}, {"color": "rgb(193, 230, 136)", "font": {"color": "white"}, "id": "FIG. 6", "label": "FIG. 6", "shape": "dot", "size": 10, "title": "Illustration showing some of the input images in the study"}, {"color": "rgb(193, 230, 136)", "font": {"color": "white"}, "id": "3 CAMS", "label": "3 CAMS", "shape": "dot", "size": 10, "title": "Number of cameras in another experimental condition"}, {"color": "rgb(193, 230, 136)", "font": {"color": "white"}, "id": "PREVIOUS APPROACHES", "label": "PREVIOUS APPROACHES", "shape": "dot", "size": 10, "title": "earlier 3D reconstruction methods referenced in the study"}, {"color": "rgb(193, 230, 136)", "font": {"color": "white"}, "id": "29:4159\u20134173", "label": "29:4159\u20134173", "shape": "dot", "size": 10, "title": "Volume 29, pages 4159\u20134173 of IEEE Transactions on Image Processing, published in 2020."}, {"color": "rgb(193, 230, 136)", "font": {"color": "white"}, "id": "OBJECT SHAPE DISTRIBUTIONS", "label": "OBJECT SHAPE DISTRIBUTIONS", "shape": "dot", "size": 10, "title": "Statistical models describing the possible shapes of objects, used as priors in reconstruction"}, {"color": "rgb(193, 230, 136)", "font": {"color": "white"}, "id": "3D SHAPE OF A SCENE", "label": "3D SHAPE OF A SCENE", "shape": "dot", "size": 10, "title": "The three\u2011dimensional geometry of the scene being reconstructed."}, {"color": "rgb(193, 230, 136)", "font": {"color": "white"}, "id": "JUMPING HUMAN", "label": "JUMPING HUMAN", "shape": "dot", "size": 10, "title": "Specific dynamic subject used in the experiment"}, {"color": "rgb(193, 230, 136)", "font": {"color": "white"}, "id": "INTERNATIONAL JOURNAL OF COMPUTER", "label": "INTERNATIONAL JOURNAL OF COMPUTER", "shape": "dot", "size": 10, "title": "A scholarly journal publishing research on computer science and related fields"}, {"color": "rgb(193, 230, 136)", "font": {"color": "white"}, "id": "INTENSITIES", "label": "INTENSITIES", "shape": "dot", "size": 10, "title": "Scalar values associated with each projected point, reflecting signal strength or color information."}, {"color": "rgb(193, 230, 136)", "font": {"color": "white"}, "id": "YUCHENG MAO", "label": "YUCHENG MAO", "shape": "dot", "size": 10, "title": "Researcher and author of the paper \u201cAnysplat\u201d"}, {"color": "rgb(193, 230, 136)", "font": {"color": "white"}, "id": "129:1\u201316", "label": "129:1\u201316", "shape": "dot", "size": 10, "title": "Volume 129, pages 1\u201316 of the International Journal of Computer Vision, published in 2021."}, {"color": "rgb(193, 230, 136)", "font": {"color": "white"}, "id": "2022", "label": "2022", "shape": "dot", "size": 10, "title": "Year the arXiv preprint was posted.\nYear of publication for the Regnerf paper.\nThe year in which the referenced ECCV proceedings were published.\nThe year the sparse RGB\u2011D NeRF paper was published in the IEEE TPAMI."}, {"color": "rgb(193, 230, 136)", "font": {"color": "white"}, "id": "YASER SHEIKH", "label": "YASER SHEIKH", "shape": "dot", "size": 10, "title": "Researcher and author of the Panoptic studio paper"}, {"color": "rgb(193, 230, 136)", "font": {"color": "white"}, "id": "0", "label": "0", "shape": "dot", "size": 10, "title": "Lower limit of the integral."}, {"color": "rgb(193, 230, 136)", "font": {"color": "white"}, "id": "EUROPEAN CONFERENCE ON COMPUTER VISION (ECCV)", "label": "EUROPEAN CONFERENCE ON COMPUTER VISION (ECCV)", "shape": "dot", "size": 10, "title": "ECCV is a leading European conference on computer vision where Ps-nerf was presented."}, {"color": "rgb(193, 230, 136)", "font": {"color": "white"}, "id": "VISUAL GEOMETRY GROUNDED TRANSFORMER", "label": "VISUAL GEOMETRY GROUNDED TRANSFORMER", "shape": "dot", "size": 10, "title": "A transformer-based model that incorporates visual geometry cues"}, {"color": "rgb(193, 230, 136)", "font": {"color": "white"}, "id": "ARXIV:2309.03008", "label": "ARXIV:2309.03008", "shape": "dot", "size": 10, "title": "The identifier for a specific arXiv preprint that discusses object\u2011centric ray sampling for neural 4D scene reconstruction."}, {"color": "rgb(193, 230, 136)", "font": {"color": "white"}, "id": "MULIN YU", "label": "MULIN YU", "shape": "dot", "size": 10, "title": "Researcher and author of the paper \u201cAnysplat\u201d"}, {"color": "rgb(193, 230, 136)", "font": {"color": "white"}, "id": "DEPTH", "label": "DEPTH", "shape": "dot", "size": 10, "title": "distance information from camera to scene points, often used as a density cue\nThe distance information of scene points from the camera"}, {"color": "rgb(193, 230, 136)", "font": {"color": "white"}, "id": "3D INFORMATION", "label": "3D INFORMATION", "shape": "dot", "size": 10, "title": "The spatial data describing the shape and structure of a scene or object in three dimensions."}, {"color": "rgb(193, 230, 136)", "font": {"color": "white"}, "id": "3 LIGHTS", "label": "3 LIGHTS", "shape": "dot", "size": 10, "title": "Number of light sources in another experimental condition"}, {"color": "rgb(193, 230, 136)", "font": {"color": "white"}, "id": "FENG ZHAO", "label": "FENG ZHAO", "shape": "dot", "size": 10, "title": "Researcher and author of the paper \u201cAnysplat\u201d"}, {"color": "rgb(193, 230, 136)", "font": {"color": "white"}, "id": "REAL EXPERIMENTS", "label": "REAL EXPERIMENTS", "shape": "dot", "size": 10, "title": "Experimental tests conducted on actual objects to validate the proposed reconstruction method."}, {"color": "rgb(193, 230, 136)", "font": {"color": "white"}, "id": "LIMITED NUMBER OF VIEWS", "label": "LIMITED NUMBER OF VIEWS", "shape": "dot", "size": 10, "title": "A constraint indicating that the reconstruction algorithm operates with few camera perspectives"}, {"color": "rgb(193, 230, 136)", "font": {"color": "white"}, "id": "DENSE DEPTH PRIORS", "label": "DENSE DEPTH PRIORS", "shape": "dot", "size": 10, "title": "A set of depth estimates used to improve neural radiance field reconstruction from limited view inputs."}, {"color": "rgb(193, 230, 136)", "font": {"color": "white"}, "id": "IAIN MATTHEWS", "label": "IAIN MATTHEWS", "shape": "dot", "size": 10, "title": "Researcher and author of the Panoptic studio paper"}, {"color": "rgb(193, 230, 136)", "font": {"color": "white"}, "id": "CALIBRATION ERRORS", "label": "CALIBRATION ERRORS", "shape": "dot", "size": 10, "title": "inaccuracies in sensor alignment or parameter estimation that affect the fidelity of 3D data"}, {"color": "rgb(193, 230, 136)", "font": {"color": "white"}, "id": "OVER-SMOOTHED", "label": "OVER-SMOOTHED", "shape": "dot", "size": 10, "title": "A quality issue where reconstructed shapes lack sharp details"}, {"color": "rgb(193, 230, 136)", "font": {"color": "white"}, "id": "REAL\u2011WORLD LARGE\u2011SCALE ENVIRONMENTS", "label": "REAL\u2011WORLD LARGE\u2011SCALE ENVIRONMENTS", "shape": "dot", "size": 10, "title": "Physical environments with many objects, both static and dynamic"}, {"color": "rgb(193, 230, 136)", "font": {"color": "white"}, "id": "2D IMAGE POINTS", "label": "2D IMAGE POINTS", "shape": "dot", "size": 10, "title": "Pixel coordinates in a 2D image that correspond to projections of 3D scene points."}, {"color": "rgb(193, 230, 136)", "font": {"color": "white"}, "id": "IEEE/CVF INTERNATIONAL CONFERENCE", "label": "IEEE/CVF INTERNATIONAL CONFERENCE", "shape": "dot", "size": 10, "title": "A joint conference organized by IEEE and CVF focusing on computer vision research."}, {"color": "rgb(193, 230, 136)", "font": {"color": "white"}, "id": "IMAGE INTENSITY", "label": "IMAGE INTENSITY", "shape": "dot", "size": 10, "title": "The brightness value recorded in an image pixel."}, {"color": "rgb(193, 230, 136)", "font": {"color": "white"}, "id": "CEILING LIGHTS", "label": "CEILING LIGHTS", "shape": "dot", "size": 10, "title": "Standard lighting fixtures located on the ceiling, acting as an external light source"}, {"color": "rgb(193, 230, 136)", "font": {"color": "white"}, "id": "HANDHELD 3D SCANNER", "label": "HANDHELD 3D SCANNER", "shape": "dot", "size": 10, "title": "a portable device used to capture 3D geometry for reference purposes"}, {"color": "rgb(193, 230, 136)", "font": {"color": "white"}, "id": "YI-HUA HUANG", "label": "YI-HUA HUANG", "shape": "dot", "size": 10, "title": "Co\u2011author of the sparse RGB\u2011D NeRF paper."}, {"color": "rgb(193, 230, 136)", "font": {"color": "white"}, "id": "PATTERN INFORMATION", "label": "PATTERN INFORMATION", "shape": "dot", "size": 10, "title": "information about recurring visual structures used to improve reconstruction accuracy"}, {"color": "rgb(193, 230, 136)", "font": {"color": "white"}, "id": "DEPTH FROM SFM", "label": "DEPTH FROM SFM", "shape": "dot", "size": 10, "title": "depth estimates obtained from Structure-from-Motion pipelines"}, {"color": "rgb(193, 230, 136)", "font": {"color": "white"}, "id": "3D SCENE POINTS", "label": "3D SCENE POINTS", "shape": "dot", "size": 10, "title": "Spatial coordinates in a 3D scene that correspond to physical points in the environment."}, {"color": "rgb(193, 230, 136)", "font": {"color": "white"}, "id": "RAY FROM CAMERA", "label": "RAY FROM CAMERA", "shape": "dot", "size": 10, "title": "A ray emitted from a camera used to capture visual information."}, {"color": "rgb(193, 230, 136)", "font": {"color": "white"}, "id": "YICHEN JIN", "label": "YICHEN JIN", "shape": "dot", "size": 10, "title": "Researcher and author of the paper \u201cAnysplat\u201d"}, {"color": "rgb(193, 230, 136)", "font": {"color": "white"}, "id": "ARXIV PREPRINT ARXIV:2110.05472", "label": "ARXIV PREPRINT ARXIV:2110.05472", "shape": "dot", "size": 10, "title": "The cited paper presenting the differentiable stereopsis approach, published in 2021."}, {"color": "rgb(193, 230, 136)", "font": {"color": "white"}, "id": "EXPERIMENTS", "label": "EXPERIMENTS", "shape": "dot", "size": 10, "title": "Empirical tests conducted to evaluate the effectiveness of active lighting."}, {"color": "rgb(193, 230, 136)", "font": {"color": "white"}, "id": "GRADIENT DESCENT", "label": "GRADIENT DESCENT", "shape": "dot", "size": 10, "title": "An optimization technique that iteratively updates parameters to minimize a cost function."}, {"color": "rgb(193, 230, 136)", "font": {"color": "white"}, "id": "TABLE 1.4.3", "label": "TABLE 1.4.3", "shape": "dot", "size": 10, "title": "A table in the paper that presents quantitative results comparing reconstruction accuracy with and without active lighting."}]);
                  edges = new vis.DataSet([{"from": "DIEGO THOMAS", "title": "8", "to": "ACTIVENEUS: NEURAL SIGNED", "width": 1}, {"from": "STRUCTURED LIGHT", "title": "7", "to": "MVPS", "width": 1}, {"from": "NEURAL INVERSE STRUCTURED LIGHT", "title": "27", "to": "TURBOSL", "width": 1}, {"from": "JOHANNES L SCHONBERGER", "title": "12", "to": "STRUCTURE-FROM-MOTION REVISITED", "width": 1}, {"from": "CHAOFENG CHEN", "title": "17", "to": "PS-NERF", "width": 1}, {"from": "WIDE-BASELINE STEREO", "title": "7", "to": "REGULARIZATION", "width": 1}, {"from": "TJ(P(T))", "title": "6", "to": "IJ(P(T))", "width": 1}, {"from": "DEPTH-SUPERVISED NERF", "title": "11", "to": "KANGLE DENG", "width": 1}, {"from": "DEPTH-SUPERVISED NERF", "title": "9", "to": "ANDREW LIU", "width": 1}, {"from": "DEPTH-SUPERVISED NERF", "title": "9", "to": "JUN-YAN ZHU", "width": 1}, {"from": "DEPTH-SUPERVISED NERF", "title": "9", "to": "DEVA RAMAN", "width": 1}, {"from": "DEPTH-SUPERVISED NERF", "title": "9", "to": "FREE TRAINING", "width": 1}, {"from": "DEPTH-SUPERVISED NERF", "title": "28", "to": "NEURAL 4D SCENE RECONSTRUCTION", "width": 1}, {"from": "OBJECTS", "title": "4", "to": "GROUND TRUTHS (GT)", "width": 1}, {"from": "IEEE/CVF CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION", "title": "14", "to": "ANTI-ALIASED NEURAL RADIANCE FIELDS", "width": 1}, {"from": "IEEE/CVF CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION", "title": "21", "to": "RNB-NEUS", "width": 1}, {"from": "IEEE/CVF CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION", "title": "16", "to": "SPARSE 3D RECONSTRUCTION VIA OBJECT-CENTRIC RAY SAMPLING", "width": 1}, {"from": "IEEE/CVF CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION", "title": "21", "to": "INFONERF", "width": 1}, {"from": "IEEE/CVF CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION", "title": "37", "to": "TURBOSL", "width": 1}, {"from": "IEEE/CVF CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION", "title": "21", "to": "VGGT", "width": 1}, {"from": "NEURAL FIELDS", "title": "17", "to": "IMAGES", "width": 1}, {"from": "DEEP MULTI-VIEW PHOTOMETRIC STEREO", "title": "27", "to": "NEURAL RADIANCE FIELDS", "width": 1}, {"from": "HIROSHI KAWASAKI", "title": "17", "to": "KYUSHU UNIVERSITY", "width": 1}, {"from": "HIROSHI KAWASAKI", "title": "21", "to": "NERF-BASED MULTI-FRAME 3D INTEGRATION", "width": 1}, {"from": "HIROSHI KAWASAKI", "title": "19", "to": "ACTIVENEUS: NEURAL SIGNED", "width": 1}, {"from": "HIROSHI KAWASAKI", "title": "36", "to": "ACTIVE LIGHTING", "width": 1}, {"from": "HIROSHI KAWASAKI", "title": "20", "to": "DEPTH RECONSTRUCTION WITH NEURAL SIGNED DISTANCE FIELDS IN STRUCTURED LIGHT SYSTEMS", "width": 1}, {"from": "HIROSHI KAWASAKI", "title": "21", "to": "DENSE 3D RECONSTRUCTION METHOD USING A SINGLE PATTERN FOR FAST MOVING OBJECT", "width": 1}, {"from": "HIROSHI KAWASAKI", "title": "20", "to": "GRID-BASED ACTIVE STEREO WITH SINGLE-COLORED WAVE PATTERN FOR DENSE ONE-SHOT 3D SCAN", "width": 1}, {"from": "CAMERA IMAGES", "title": "9", "to": "PROJECTED PATTERNS", "width": 1}, {"from": "CAMERA IMAGES", "title": "7", "to": "CONTROLLED LIGHT SOURCES", "width": 1}, {"from": "NARROW BASELINE", "title": "8", "to": "RAY FROM CAM1", "width": 1}, {"from": "CLIP-BASED SIMILARITY CONSTRAINTS", "title": "4", "to": "DIET-NERF", "width": 1}, {"from": "OURS", "title": "25", "to": "CHAMFER DISTANCE", "width": 1}, {"from": "OURS", "title": "16", "to": "LEGO", "width": 1}, {"from": "OURS", "title": "16", "to": "HOTDOG", "width": 1}, {"from": "OURS", "title": "31", "to": "NEUS", "width": 1}, {"from": "OURS", "title": "23", "to": "DUST3R", "width": 1}, {"from": "OURS", "title": "24", "to": "CHAMFER DISTANCES", "width": 1}, {"from": "NEURAL SIGNED DISTANCE FIELDS", "title": "27", "to": "ACTIVENEUS", "width": 1}, {"from": "NEURAL SIGNED DISTANCE FIELDS", "title": "11", "to": "DEPTH RECONSTRUCTION WITH NEURAL SIGNED DISTANCE FIELDS IN STRUCTURED LIGHT SYSTEMS", "width": 1}, {"from": "YEBIN LIU", "title": "9", "to": "POINT-CLOUD-BASED MULTIVIEW STEREO ALGORITHM", "width": 1}, {"from": "JAMES TOMPKIN", "title": "10", "to": "NEURAL FIELDS FOR STRUCTURED LIGHTING", "width": 1}, {"from": "NOVEL APPROACH", "title": "5", "to": "ACTIVE LIGHTING TECHNIQUES", "width": 1}, {"from": "2008", "title": "8", "to": "MULTIVIEW PHOTOMETRIC STEREO", "width": 1}, {"from": "SURYANSH KUMAR", "title": "27", "to": "NEURAL RADIANCE FIELDS", "width": 1}, {"from": "EIGHT POINT LIGHT", "title": "7", "to": "DYNAMIC SCENE", "width": 1}, {"from": "SUPPLEMENTAL VIDEO", "title": "18", "to": "4D SCENE RECONSTRUCTION", "width": 1}, {"from": "CORRESPONDENCES", "title": "18", "to": "MULTIVIEW STEREO (MVS)", "width": 1}, {"from": "CORRESPONDENCES", "title": "23", "to": "IMAGES", "width": 1}, {"from": "CORRESPONDENCES", "title": "16", "to": "NEURAL REPRESENTATION", "width": 1}, {"from": "CORRESPONDENCES", "title": "11", "to": "3DGS-BASED METHODS", "width": 1}, {"from": "CORRESPONDENCES", "title": "31", "to": "ACTIVE LIGHTING", "width": 1}, {"from": "NEURAL GEOMETRY", "title": "8", "to": "BRDF RECONSTRUCTION", "width": 1}, {"from": "NEURAL GEOMETRY", "title": "7", "to": "REFLECTIVE OBJECTS", "width": 1}, {"from": "NEURAL GEOMETRY", "title": "7", "to": "MULTIVIEW IMAGES", "width": 1}, {"from": "DEPTH RECONSTRUCTION WITH NEURAL SIGNED DISTANCE FIELDS IN STRUCTURED LIGHT SYSTEMS", "title": "9", "to": "RUKUN QIAO", "width": 1}, {"from": "DEPTH RECONSTRUCTION WITH NEURAL SIGNED DISTANCE FIELDS IN STRUCTURED LIGHT SYSTEMS", "title": "9", "to": "HONGBIN ZHA", "width": 1}, {"from": "DEPTH RECONSTRUCTION WITH NEURAL SIGNED DISTANCE FIELDS IN STRUCTURED LIGHT SYSTEMS", "title": "11", "to": "2024 INTERNATIONAL CONFERENCE ON 3D VISION (3DV)", "width": 1}, {"from": "DEPTH RECONSTRUCTION WITH NEURAL SIGNED DISTANCE FIELDS IN STRUCTURED LIGHT SYSTEMS", "title": "9", "to": "STRUCTURED LIGHT SYSTEMS", "width": 1}, {"from": "RAY", "title": "26", "to": "IRRADIANCE", "width": 1}, {"from": "RAY", "title": "29", "to": "LIGHT SOURCE", "width": 1}, {"from": "MACHINE LEARNING AND SYSTEMS", "title": "10", "to": "IMPLICIT GEOMETRIC REGULARIZATION", "width": 1}, {"from": "RAY FROM CAM2", "title": "10", "to": "RAY FROM CAM1", "width": 1}, {"from": "RAY FROM CAM2", "title": "13", "to": "P1", "width": 1}, {"from": "RAY FROM CAM2", "title": "13", "to": "P2", "width": 1}, {"from": "PRACTICAL SETUP", "title": "5", "to": "INTER\u2011REFLECTIONS", "width": 1}, {"from": "FIXED CAMERAS", "title": "5", "to": "DENSE VIEWPOINTS", "width": 1}, {"from": "YVAIN QU\u00b4EAU", "title": "11", "to": "RNB-NEUS", "width": 1}, {"from": "HIGH-PASS FILTERING", "title": "5", "to": "ARTIFACT", "width": 1}, {"from": "MOTION", "title": "5", "to": "ACQUISITION", "width": 1}, {"from": "MOTION", "title": "6", "to": "ARTIFACT", "width": 1}, {"from": "SHIRO OKA", "title": "10", "to": "NERF-BASED MULTI-FRAME 3D INTEGRATION", "width": 1}, {"from": "TAKAKI IKEDA", "title": "8", "to": "ACTIVENEUS: NEURAL SIGNED", "width": 1}, {"from": "LILIAN CALVET", "title": "11", "to": "RNB-NEUS", "width": 1}, {"from": "MULTIPLEXED ILLUMINATION TECHNIQUES", "title": "28", "to": "3D RECONSTRUCTION", "width": 1}, {"from": "CHEN SUN", "title": "8", "to": "REVISITING UNREASONABLE EFFECTIVENESS OF DATA IN DEEP LEARNING ERA", "width": 1}, {"from": "STEREO RADIANCE FIELDS (SRF)", "title": "9", "to": "JULIAN CHIBANE", "width": 1}, {"from": "STEREO RADIANCE FIELDS (SRF)", "title": "9", "to": "AAYUSH BANSAL", "width": 1}, {"from": "STEREO RADIANCE FIELDS (SRF)", "title": "9", "to": "VERICA LAZOVA", "width": 1}, {"from": "STEREO RADIANCE FIELDS (SRF)", "title": "9", "to": "GERARD PONS\u2011MOLL", "width": 1}, {"from": "STEREO RADIANCE FIELDS (SRF)", "title": "22", "to": "IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION", "width": 1}, {"from": "STEREO RADIANCE FIELDS (SRF)", "title": "18", "to": "MVSNERF", "width": 1}, {"from": "OBJECT SHAPE ESTIMATION", "title": "7", "to": "NEURAL SDF", "width": 1}, {"from": "LIHE YANG", "title": "10", "to": "DEPTH ANYTHING", "width": 1}, {"from": "INTENSITY OF AN IMAGE POINT", "title": "6", "to": "VIEW DIRECTIONS", "width": 1}, {"from": "INTENSITY OF AN IMAGE POINT", "title": "6", "to": "CAMERA VK", "width": 1}, {"from": "INTENSITY OF AN IMAGE POINT", "title": "6", "to": "LIGHT SOURCE VJ", "width": 1}, {"from": "LASER LIGHT SOURCES", "title": "8", "to": "DIFFRACTIVE OPTICAL ELEMENTS (DOE)", "width": 1}, {"from": "LASER LIGHT SOURCES", "title": "12", "to": "EXPERIMENT", "width": 1}, {"from": "LASER LIGHT SOURCES", "title": "9", "to": "ROOM LIGHTS", "width": 1}, {"from": "LASER LIGHT SOURCES", "title": "8", "to": "CHANNELS OF THE LASER LIGHTS", "width": 1}, {"from": "PHOTOMETRY CONSTRAINTS", "title": "16", "to": "REGNERF", "width": 1}, {"from": "MVPS", "title": "9", "to": "PHOTOMETRIC STEREO", "width": 1}, {"from": "MVPS", "title": "29", "to": "IRON", "width": 1}, {"from": "MVPS", "title": "26", "to": "SUPERNORMAL", "width": 1}, {"from": "JONATHAN T BARRON", "title": "14", "to": "MIP-NERF", "width": 1}, {"from": "JONATHAN T BARRON", "title": "11", "to": "MIP-NERF 360", "width": 1}, {"from": "SHAPE-CONDITIONED RADIANCE FIELDS", "title": "8", "to": "SHARF: SHAPE-CONDITIONED RADIANCE FIELDS FROM A SINGLE VIEW", "width": 1}, {"from": "LOW-FREQUENCY SHAPE", "title": "9", "to": "SPARSE VIEWS", "width": 1}, {"from": "QIONGHAI DAI", "title": "9", "to": "POINT-CLOUD-BASED MULTIVIEW STEREO ALGORITHM", "width": 1}, {"from": "Y.Y. SCHECHNER", "title": "10", "to": "IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION", "width": 1}, {"from": "BRDF RECONSTRUCTION", "title": "7", "to": "REFLECTIVE OBJECTS", "width": 1}, {"from": "BRDF RECONSTRUCTION", "title": "7", "to": "MULTIVIEW IMAGES", "width": 1}, {"from": "SIX CALIBRATED LIGHT SOURCES", "title": "5", "to": "EXTERNAL LIGHT", "width": 1}, {"from": "SIGGRAPH 2024 CONFERENCE PAPERS", "title": "11", "to": "2D GAUSSIAN SPLATTING FOR GEOMETRICALLY ACCURATE RADIANCE FIELDS", "width": 1}, {"from": "SIGGRAPH 2024 CONFERENCE PAPERS", "title": "6", "to": "ASSOCIATION FOR COMPUTING MACHINERY", "width": 1}, {"from": "SIGGRAPH 2024 CONFERENCE PAPERS", "title": "6", "to": "2024", "width": 1}, {"from": "SZYMON RUSINKIEWICZ", "title": "11", "to": "DYNAMIC SHAPE CAPTURE USING MULTI-VIEW PHOTOMETRIC STEREO", "width": 1}, {"from": "CHAMFER DISTANCE METRIC", "title": "21", "to": "PROPOSED METHOD", "width": 1}, {"from": "PRADYUMNA CHARI", "title": "7", "to": "HAOLIN XIONG", "width": 1}, {"from": "STRUCTURE-FROM-MOTION REVISITED", "title": "14", "to": "JAN-MICHAEL FRAHM", "width": 1}, {"from": "STRUCTURE-FROM-MOTION REVISITED", "title": "17", "to": "IEEE CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION", "width": 1}, {"from": "STRUCTURE-FROM-MOTION REVISITED", "title": "14", "to": "3DIMPVT", "width": 1}, {"from": "STRUCTURE-FROM-MOTION REVISITED", "title": "12", "to": "THOMAS SCHOPS", "width": 1}, {"from": "STRUCTURE-FROM-MOTION REVISITED", "title": "12", "to": "SILVANO GALLIANI", "width": 1}, {"from": "STRUCTURE-FROM-MOTION REVISITED", "title": "14", "to": "TORSTEN SATTLER", "width": 1}, {"from": "STRUCTURE-FROM-MOTION REVISITED", "title": "14", "to": "KONRAD SCHINDLER", "width": 1}, {"from": "STRUCTURE-FROM-MOTION REVISITED", "title": "14", "to": "MARC POLLEFEYS", "width": 1}, {"from": "OTHER APPROACHES", "title": "21", "to": "PROPOSED METHOD", "width": 1}, {"from": "SURFACE TEXTURE", "title": "8", "to": "STRUCTURED LIGHT (SL)", "width": 1}, {"from": "6 LIGHTS", "title": "8", "to": "ABLATION STUDY", "width": 1}, {"from": "6 LIGHTS", "title": "6", "to": "4 CAMERAS", "width": 1}, {"from": "KAIRUN WEN", "title": "21", "to": "INSTANTSPLAT", "width": 1}, {"from": "IEEE CONF. COMPUT. VIS. PATTERN RECOG.", "title": "26", "to": "IRON", "width": 1}, {"from": "ZILONG HUANG", "title": "10", "to": "DEPTH ANYTHING", "width": 1}, {"from": "LIGHT SOURCE POSITIONS", "title": "22", "to": "CAMERAS", "width": 1}, {"from": "LIGHT RAY", "title": "33", "to": "LIGHT SOURCE", "width": 1}, {"from": "LIGHT RAY", "title": "11", "to": "POSITION", "width": 1}, {"from": "LIGHT RAY", "title": "25", "to": "MLP", "width": 1}, {"from": "LIGHT RAY", "title": "31", "to": "NEUS", "width": 1}, {"from": "IEEE CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION (CVPR)", "title": "7", "to": "IEEE", "width": 1}, {"from": "IEEE CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION (CVPR)", "title": "12", "to": "2021", "width": 1}, {"from": "SIX CAMERAS", "title": "5", "to": "INPUT IMAGE", "width": 1}, {"from": "INPUT IMAGES", "title": "5", "to": "NINE CHANNELS", "width": 1}, {"from": "INPUT IMAGES", "title": "9", "to": "EXPERIMENT", "width": 1}, {"from": "JONATHAN T. BARRON", "title": "31", "to": "NERF", "width": 1}, {"from": "JONATHAN T. BARRON", "title": "20", "to": "REGNERF", "width": 1}, {"from": "JONATHAN T. BARRON", "title": "18", "to": "HYPERNERF", "width": 1}, {"from": "SIMULATION MODELS", "title": "11", "to": "GROUND TRUTH", "width": 1}, {"from": "GEORGE VOGIATZIS", "title": "9", "to": "MULTIVIEW PHOTOMETRIC STEREO", "width": 1}, {"from": "MULTIVIEW STEREO (MVS)", "title": "14", "to": "NEURAL IMPLICIT REPRESENTATION METHODS", "width": 1}, {"from": "MULTIVIEW STEREO (MVS)", "title": "15", "to": "4D SCENE", "width": 1}, {"from": "MULTIVIEW STEREO (MVS)", "title": "25", "to": "IMAGES", "width": 1}, {"from": "MULTIVIEW STEREO (MVS)", "title": "12", "to": "SURFACE POINTS", "width": 1}, {"from": "MULTIVIEW STEREO (MVS)", "title": "13", "to": "TRIANGULATION", "width": 1}, {"from": "MULTIVIEW STEREO (MVS)", "title": "12", "to": "RECONSTRUCTION ERRORS", "width": 1}, {"from": "CORRESPONDENCE", "title": "5", "to": "FEATURES", "width": 1}, {"from": "\u03a1(P)", "title": "6", "to": "F(P)", "width": 1}, {"from": "INDOOR MULTI\u2011VIEW STEREO", "title": "12", "to": "NERFINGMVS", "width": 1}, {"from": "OPAQUE DENSITY FUNCTION", "title": "12", "to": "SDF", "width": 1}, {"from": "GJ", "title": "13", "to": "LG", "width": 1}, {"from": "GJ", "title": "6", "to": "SG", "width": 1}, {"from": "GJ", "title": "8", "to": "TJ", "width": 1}, {"from": "VITTORIO FERARI", "title": "27", "to": "NEURAL RADIANCE FIELDS", "width": 1}, {"from": "SIGMOID FUNCTION", "title": "6", "to": "\u03a6(X)", "width": 1}, {"from": "SIGMOID FUNCTION", "title": "5", "to": "SCALE PARAMETER", "width": 1}, {"from": "CHUNYU LI", "title": "8", "to": "MULTI-VIEW NEURAL SURFACE RECONSTRUCTION WITH STRUCTURED LIGHT", "width": 1}, {"from": "REFLECTION PARAMETER ESTIMATION", "title": "27", "to": "NERF", "width": 1}, {"from": "NERS", "title": "15", "to": "SPHERICAL TOPOLOGY", "width": 1}, {"from": "NERS", "title": "20", "to": "RECONSTRUCTED SHAPES", "width": 1}, {"from": "NERS", "title": "30", "to": "CHAMFER DISTANCE", "width": 1}, {"from": "NERS", "title": "15", "to": "JASON ZHANG", "width": 1}, {"from": "NERS", "title": "17", "to": "GENGSHAN YANG", "width": 1}, {"from": "NERS", "title": "15", "to": "SHUBHAM TULSIANI", "width": 1}, {"from": "NERS", "title": "17", "to": "DEVA RAMANAN", "width": 1}, {"from": "NERS", "title": "16", "to": "ANALYSIS AND MACHINE INTELLIGENCE", "width": 1}, {"from": "NERS", "title": "34", "to": "NEURAL 4D SCENE RECONSTRUCTION", "width": 1}, {"from": "TAKETOMI TAKAFUMI", "title": "4", "to": "CAO XU", "width": 1}, {"from": "BERK KAYA", "title": "27", "to": "NEURAL RADIANCE FIELDS", "width": 1}, {"from": "MESH AND VOLUME REPRESENTATIONS", "title": "6", "to": "CERKEZI AND FAVARO", "width": 1}, {"from": "MESH AND VOLUME REPRESENTATIONS", "title": "6", "to": "OBJECT\u2011CENTRIC RAY SAMPLING", "width": 1}, {"from": "22H00545", "title": "11", "to": "JAPAN", "width": 1}, {"from": "INTENSITY CK", "title": "19", "to": "MLP", "width": 1}, {"from": "ACTIVENEUS: NEURAL SIGNED", "title": "8", "to": "KAZUTO ICHIMARU", "width": 1}, {"from": "ACTIVENEUS: NEURAL SIGNED", "title": "10", "to": "TAKAFUMI IWAGUCHI", "width": 1}, {"from": "LUC VAN GOOL", "title": "27", "to": "NEURAL RADIANCE FIELDS", "width": 1}, {"from": "ARXIV", "title": "11", "to": "IBRNET", "width": 1}, {"from": "ARXIV", "title": "22", "to": "DUST3R", "width": 1}, {"from": "ARXIV", "title": "17", "to": "SPARSEGS", "width": 1}, {"from": "ARXIV", "title": "12", "to": "OPEN3D", "width": 1}, {"from": "NEURAL RADIANCE FIELD", "title": "18", "to": "PS-NERF", "width": 1}, {"from": "SNR", "title": "4", "to": "HADAMARD-BASED MULTIPLEXING", "width": 1}, {"from": "LIGHT POWER PRIOR", "title": "8", "to": "NEURAL SDF", "width": 1}, {"from": "LIGHT POWER PRIOR", "title": "5", "to": "SURFACE REFLECTION", "width": 1}, {"from": "STRONG EXTERNAL LIGHT", "title": "6", "to": "ILLUMINANT\u2011CAMERA COMMUNICATION", "width": 1}, {"from": "JST PARKS2025 04", "title": "9", "to": "JAPAN", "width": 1}, {"from": "HAOLIN XIONG", "title": "7", "to": "SAIRISHEEK MUTTUKURU", "width": 1}, {"from": "HAOLIN XIONG", "title": "7", "to": "RISHI UPADHYAY", "width": 1}, {"from": "HAOLIN XIONG", "title": "7", "to": "ACHUTA KADAMBI", "width": 1}, {"from": "LEARNING SHAPES", "title": "10", "to": "IMPLICIT GEOMETRIC REGULARIZATION", "width": 1}, {"from": "DENSITY", "title": "18", "to": "VOLUME RENDERING", "width": 1}, {"from": "DENSITY", "title": "12", "to": "3D POINT", "width": 1}, {"from": "DENSITY", "title": "10", "to": "VOLUMETRIC REPRESENTATION", "width": 1}, {"from": "DENSITY", "title": "12", "to": "P3", "width": 1}, {"from": "WANG ZHAO", "title": "12", "to": "NERFINGMVS", "width": 1}, {"from": "LLUKMAN CERKEZI", "title": "6", "to": "SPARSE 3D RECONSTRUCTION VIA OBJECT-CENTRIC RAY SAMPLING", "width": 1}, {"from": "Y. YAGI", "title": "7", "to": "MULTIPLEXED ILLUMINATION FOR MEASURING BRDF USING AN ELLIPSOIDAL MIRROR AND A PROJECTOR", "width": 1}, {"from": "VICKIE YE", "title": "9", "to": "PIXELNERF", "width": 1}, {"from": "3DIMPVT", "title": "11", "to": "GRID-BASED ACTIVE STEREO WITH SINGLE-COLORED WAVE PATTERN FOR DENSE ONE-SHOT 3D SCAN", "width": 1}, {"from": "ASIAN CONFERENCE ON COMPUTER VISION", "title": "7", "to": "MULTIPLEXED ILLUMINATION FOR MEASURING BRDF USING AN ELLIPSOIDAL MIRROR AND A PROJECTOR", "width": 1}, {"from": "NORMAL VECTOR", "title": "12", "to": "SDF", "width": 1}, {"from": "NEURAL 4D SCENE RECONSTRUCTION", "title": "23", "to": "DEMULTIPLEXING ILLUMINATION", "width": 1}, {"from": "NEURAL 4D SCENE RECONSTRUCTION", "title": "23", "to": "OBSERVATION OF WHOLE SCENE WITH SPARSE CAMERAS AND LIGHT SOURCES", "width": 1}, {"from": "NEURAL 4D SCENE RECONSTRUCTION", "title": "23", "to": "ILLUMINATING A TARGET SIMULTANEOUSLY WITH LIGHTS", "width": 1}, {"from": "NEURAL 4D SCENE RECONSTRUCTION", "title": "24", "to": "MULTI-CHANNEL IMAGES", "width": 1}, {"from": "NEURAL 4D SCENE RECONSTRUCTION", "title": "29", "to": "DEPTH-REGULARIZED OPTIMIZATION", "width": 1}, {"from": "NEURAL 4D SCENE RECONSTRUCTION", "title": "24", "to": "DYNAMIC SHAPE CAPTURE", "width": 1}, {"from": "NEURAL 4D SCENE RECONSTRUCTION", "title": "26", "to": "COMPUTER VISION", "width": 1}, {"from": "NEURAL 4D SCENE RECONSTRUCTION", "title": "45", "to": "IRON", "width": 1}, {"from": "QIANQIAN WANG", "title": "6", "to": "IBRNET", "width": 1}, {"from": "WENQI YANG", "title": "17", "to": "PS-NERF", "width": 1}, {"from": "CAMERA", "title": "8", "to": "LIGHT", "width": 1}, {"from": "MULTI-VIEW STEREO RECONSTRUCTION ALGORITHMS", "title": "10", "to": "STEVEN M SEITZ", "width": 1}, {"from": "MULTI-VIEW STEREO RECONSTRUCTION ALGORITHMS", "title": "10", "to": "BRIAN CURLESS", "width": 1}, {"from": "MULTI-VIEW STEREO RECONSTRUCTION ALGORITHMS", "title": "10", "to": "JAMES DIEBEL", "width": 1}, {"from": "MULTI-VIEW STEREO RECONSTRUCTION ALGORITHMS", "title": "10", "to": "DANIEL SCHARSTEIN", "width": 1}, {"from": "MULTI-VIEW STEREO RECONSTRUCTION ALGORITHMS", "title": "10", "to": "RICHARD SZELISKI", "width": 1}, {"from": "MULTI-VIEW STEREO RECONSTRUCTION ALGORITHMS", "title": "10", "to": "CVPR\u002706", "width": 1}, {"from": "MULTI-VIEW STEREO RECONSTRUCTION ALGORITHMS", "title": "17", "to": "MULTI-VIEW STEREO BENCHMARK", "width": 1}, {"from": "FIG. 2", "title": "20", "to": "LIGHT SOURCES", "width": 1}, {"from": "MATTHEW TANCIK", "title": "18", "to": "MIP-NERF", "width": 1}, {"from": "MATTHEW TANCIK", "title": "34", "to": "NERF", "width": 1}, {"from": "MATTHEW TANCIK", "title": "14", "to": "FEW-SHOT VIEW SYNTHESIS", "width": 1}, {"from": "MATTHEW TANCIK", "title": "16", "to": "PIXELNERF", "width": 1}, {"from": "RNB-NEUS", "title": "11", "to": "BAPTISTE BRUMENT", "width": 1}, {"from": "RNB-NEUS", "title": "11", "to": "ROBIN BRUNEAU", "width": 1}, {"from": "RNB-NEUS", "title": "11", "to": "JEAN M\u00b4ELOU", "width": 1}, {"from": "RNB-NEUS", "title": "11", "to": "FRAN\u00c7OIS LAUZE", "width": 1}, {"from": "RNB-NEUS", "title": "11", "to": "JEAN-DENIS DUROU", "width": 1}, {"from": "ABHINAV GUPTA", "title": "8", "to": "REVISITING UNREASONABLE EFFECTIVENESS OF DATA IN DEEP LEARNING ERA", "width": 1}, {"from": "SEONGUK SEO", "title": "11", "to": "INFONERF", "width": 1}, {"from": "RYUSUKE SAGAWA", "title": "12", "to": "AIST", "width": 1}, {"from": "RYUSUKE SAGAWA", "title": "18", "to": "NERF-BASED MULTI-FRAME 3D INTEGRATION", "width": 1}, {"from": "RYUSUKE SAGAWA", "title": "33", "to": "ACTIVE LIGHTING", "width": 1}, {"from": "RYUSUKE SAGAWA", "title": "18", "to": "DENSE 3D RECONSTRUCTION METHOD USING A SINGLE PATTERN FOR FAST MOVING OBJECT", "width": 1}, {"from": "RYUSUKE SAGAWA", "title": "17", "to": "GRID-BASED ACTIVE STEREO WITH SINGLE-COLORED WAVE PATTERN FOR DENSE ONE-SHOT 3D SCAN", "width": 1}, {"from": "RECONSTRUCTION", "title": "18", "to": "PERSON", "width": 1}, {"from": "RECONSTRUCTION", "title": "20", "to": "EIGHT CAMERAS", "width": 1}, {"from": "RECONSTRUCTION", "title": "14", "to": "NVIDIA V100 GPU", "width": 1}, {"from": "RECONSTRUCTION", "title": "14", "to": "90 FRAMES", "width": 1}, {"from": "RECONSTRUCTION", "title": "14", "to": "512 RAYS", "width": 1}, {"from": "RECONSTRUCTION", "title": "14", "to": "210K ITERATIONS", "width": 1}, {"from": "RECONSTRUCTION", "title": "14", "to": "12 HOURS", "width": 1}, {"from": "RECONSTRUCTION", "title": "21", "to": "MANNEQUIN", "width": 1}, {"from": "RECONSTRUCTION", "title": "24", "to": "ILLUMINATION", "width": 1}, {"from": "RECONSTRUCTION", "title": "17", "to": "MULTIVIEW STEREO", "width": 1}, {"from": "RECONSTRUCTION", "title": "23", "to": "MOVING OBJECTS", "width": 1}, {"from": "REAL-WORLD EXPERIMENTS", "title": "11", "to": "NEURAL REPRESENTATIONS", "width": 1}, {"from": "EIGHT POINT LIGHT SOURCES", "title": "10", "to": "OUR METHOD", "width": 1}, {"from": "NEURAL SDF", "title": "12", "to": "LOSS FUNCTION", "width": 1}, {"from": "ITERATION", "title": "14", "to": "HIGH-DENSITY AREA", "width": 1}, {"from": "ITERATION", "title": "8", "to": "WEIGHT \u0391", "width": 1}, {"from": "ITERATION", "title": "12", "to": "LC", "width": 1}, {"from": "ITERATION", "title": "26", "to": "CAMERAS", "width": 1}, {"from": "SCENES ILLUMINATED VIRTUALLY", "title": "27", "to": "LIGHT SOURCE", "width": 1}, {"from": "YUCHEN FAN", "title": "15", "to": "MV-DUST3R+", "width": 1}, {"from": "RAY FROM CAM1", "title": "15", "to": "P1", "width": 1}, {"from": "RAY FROM CAM1", "title": "15", "to": "P2", "width": 1}, {"from": "RAY FROM CAM1", "title": "10", "to": "WIDE BASELINE", "width": 1}, {"from": "SHAOHUI LIU", "title": "12", "to": "NERFINGMVS", "width": 1}, {"from": "JAPAN", "title": "11", "to": "JSPS/KAKENHI JP25K22821", "width": 1}, {"from": "TRAINING", "title": "10", "to": "EQ. (7)", "width": 1}, {"from": "VICTOR ADRIAN PRISACARIU", "title": "10", "to": "NERF\u2212\u2212", "width": 1}, {"from": "REF-NERF", "title": "28", "to": "NEURAL RADIANCE FIELDS", "width": 1}, {"from": "REF-NERF", "title": "19", "to": "CVPR", "width": 1}, {"from": "ANDREA VEDALDI", "title": "11", "to": "VGGT", "width": 1}, {"from": "EIICHI MATSUMOTO", "title": "8", "to": "MULTI-VIEW NEURAL SURFACE RECONSTRUCTION WITH STRUCTURED LIGHT", "width": 1}, {"from": "NEURAL-BASED METHODS", "title": "20", "to": "CHAMFER DISTANCE", "width": 1}, {"from": "NEURAL-BASED METHODS", "title": "12", "to": "GROUND TRUTH", "width": 1}, {"from": "SPARSE INPUTS", "title": "16", "to": "REGNERF", "width": 1}, {"from": "ZEXIANG XU", "title": "13", "to": "MVSNERF", "width": 1}, {"from": "SIMULATION", "title": "18", "to": "IMAGES", "width": 1}, {"from": "VIEWPOINT", "title": "8", "to": "CAMERA RAY", "width": 1}, {"from": "NUMBER OF LIGHTS", "title": "21", "to": "PROPOSED METHOD", "width": 1}, {"from": "PS-NERF", "title": "18", "to": "NORMAL VECTORS", "width": 1}, {"from": "PS-NERF", "title": "22", "to": "RECONSTRUCTED SHAPES", "width": 1}, {"from": "PS-NERF", "title": "32", "to": "CHAMFER DISTANCE", "width": 1}, {"from": "PS-NERF", "title": "17", "to": "GUANYING CHEN", "width": 1}, {"from": "PS-NERF", "title": "17", "to": "ZHENFANG CHEN", "width": 1}, {"from": "PS-NERF", "title": "17", "to": "KWAN-YEE K. WONG", "width": 1}, {"from": "PS-NERF", "title": "30", "to": "MULTI-VIEW PHOTOMETRIC STEREO", "width": 1}, {"from": "PS-NERF", "title": "19", "to": "ECCV", "width": 1}, {"from": "VGGT", "title": "11", "to": "JIANYUAN WANG", "width": 1}, {"from": "VGGT", "title": "11", "to": "MINGHAO CHEN", "width": 1}, {"from": "VGGT", "title": "11", "to": "NIKITA KARAEV", "width": 1}, {"from": "VGGT", "title": "11", "to": "CHRISTIAN RUPPRECHT", "width": 1}, {"from": "VGGT", "title": "11", "to": "DAVID NOVOTNY", "width": 1}, {"from": "VGGT", "title": "12", "to": "2025", "width": 1}, {"from": "SIMPLE POINT LIGHT SOURCES", "title": "25", "to": "ACTIVE LIGHTING", "width": 1}, {"from": "PETER HEDMAN", "title": "14", "to": "MIP-NERF", "width": 1}, {"from": "PETER HEDMAN", "title": "11", "to": "MIP-NERF 360", "width": 1}, {"from": "PETER HEDMAN", "title": "17", "to": "HYPERNERF", "width": 1}, {"from": "BACKGROUND MASKS", "title": "13", "to": "NERF-BASED METHODS", "width": 1}, {"from": "NERF-BASED MULTI-FRAME 3D INTEGRATION", "title": "15", "to": "RYO FURUKAWA", "width": 1}, {"from": "NERF-BASED MULTI-FRAME 3D INTEGRATION", "title": "10", "to": "3D ENDOSCOPY", "width": 1}, {"from": "NERF-BASED MULTI-FRAME 3D INTEGRATION", "title": "12", "to": "ACTIVE STEREO", "width": 1}, {"from": "NERF-BASED MULTI-FRAME 3D INTEGRATION", "title": "11", "to": "COMPUTER VISION AND PATTERN RECOGNITION", "width": 1}, {"from": "ANALYSIS AND MACHINE INTELLIGENCE", "title": "27", "to": "IRON", "width": 1}, {"from": "ANDREW ZISSERMAN", "title": "6", "to": "MULTIPLE VIEW GEOMETRY", "width": 1}, {"from": "MV-DUST3R+", "title": "15", "to": "ZHENGGANG TANG", "width": 1}, {"from": "MV-DUST3R+", "title": "15", "to": "DILIN WANG", "width": 1}, {"from": "MV-DUST3R+", "title": "15", "to": "HONGYU XU", "width": 1}, {"from": "MV-DUST3R+", "title": "14", "to": "RAKESH RANJAN", "width": 1}, {"from": "MV-DUST3R+", "title": "14", "to": "ALEXANDER SCHWING", "width": 1}, {"from": "MV-DUST3R+", "title": "14", "to": "ZHICHENG YAN", "width": 1}, {"from": "MV-DUST3R+", "title": "14", "to": "ARXIV PREPRINT ARXIV:2412.06974", "width": 1}, {"from": "MV-DUST3R+", "title": "14", "to": "SINGLE-STAGE SCENE RECONSTRUCTION", "width": 1}, {"from": "MV-DUST3R+", "title": "19", "to": "SPARSE VIEWS", "width": 1}, {"from": "MV-DUST3R+", "title": "14", "to": "2 SECONDS", "width": 1}, {"from": "\u03a1(P(T))", "title": "8", "to": "W(T)", "width": 1}, {"from": "MULTI\u2011VIEW SHAPE RECONSTRUCTION", "title": "8", "to": "VOLUME SWEEPING", "width": 1}, {"from": "IEEE TRANSACTIONS ON IMAGE PROCESSING", "title": "17", "to": "MULTI-VIEW PHOTOMETRIC STEREO", "width": 1}, {"from": "COMPUTATIONAL OVERHEAD", "title": "22", "to": "CAMERAS", "width": 1}, {"from": "RAY PROJECTION", "title": "5", "to": "TRUE SURFACE", "width": 1}, {"from": "RAY PROJECTION", "title": "8", "to": "BASELINE", "width": 1}, {"from": "DEPTH-REGULARIZED OPTIMIZATION", "title": "10", "to": "JAEYOUNG CHUNG", "width": 1}, {"from": "DEPTH-REGULARIZED OPTIMIZATION", "title": "10", "to": "JEONGTAEK OH", "width": 1}, {"from": "DEPTH-REGULARIZED OPTIMIZATION", "title": "10", "to": "KYOUNG MU LEE", "width": 1}, {"from": "DEPTH-REGULARIZED OPTIMIZATION", "title": "19", "to": "3D GAUSSIAN SPLATTING", "width": 1}, {"from": "DEPTH-REGULARIZED OPTIMIZATION", "title": "10", "to": "FEW-SHOT IMAGES", "width": 1}, {"from": "DEPTH-REGULARIZED OPTIMIZATION", "title": "10", "to": "ARXIV PREPRINT ARXIV:2311.13398", "width": 1}, {"from": "P.N. BELHUMEUR", "title": "10", "to": "IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION", "width": 1}, {"from": "XIAOSHUAI ZHANG", "title": "13", "to": "MVSNERF", "width": 1}, {"from": "JAN-MICHAEL FRAHM", "title": "12", "to": "MVPSNET", "width": 1}, {"from": "SOOHWAN SONG", "title": "7", "to": "ACTIVE 3D MODELING VIA ONLINE MULTI-VIEW STEREO", "width": 1}, {"from": "HOTDOG", "title": "31", "to": "NEUS", "width": 1}, {"from": "HOTDOG", "title": "28", "to": "CAMERAS", "width": 1}, {"from": "HOTDOG", "title": "11", "to": "LIGHTS", "width": 1}, {"from": "HOTDOG", "title": "11", "to": "FIGURE\u202f8", "width": 1}, {"from": "DAN B GOLDMAN", "title": "14", "to": "HYPERNERF", "width": 1}, {"from": "LIGHT SOURCES", "title": "25", "to": "OUR METHOD", "width": 1}, {"from": "LIGHT SOURCES", "title": "23", "to": "MULTIPLEXED ILLUMINATION TECHNIQUE", "width": 1}, {"from": "LIGHT SOURCES", "title": "47", "to": "SCENE", "width": 1}, {"from": "LIGHT SOURCES", "title": "23", "to": "MULTIPLEXED ILLUMINATION", "width": 1}, {"from": "LIGHT SOURCES", "title": "43", "to": "3D RECONSTRUCTION", "width": 1}, {"from": "LIGHT SOURCES", "title": "32", "to": "IMAGES", "width": 1}, {"from": "LIGHT SOURCES", "title": "29", "to": "ILLUMINATION", "width": 1}, {"from": "LIGHT SOURCES", "title": "32", "to": "4D SCENE RECONSTRUCTION", "width": 1}, {"from": "JIAWEI YANG", "title": "12", "to": "FREENERF", "width": 1}, {"from": "GROUND TRUTH", "title": "24", "to": "4D SCENE RECONSTRUCTION", "width": 1}, {"from": "GROUND TRUTH", "title": "26", "to": "CHAMFER DISTANCE", "width": 1}, {"from": "VKX", "title": "13", "to": "C(OK, VKX)", "width": 1}, {"from": "DEPTH ANYTHING", "title": "10", "to": "BINGYI KANG", "width": 1}, {"from": "DEPTH ANYTHING", "title": "10", "to": "XIAOGANG XU", "width": 1}, {"from": "DEPTH ANYTHING", "title": "10", "to": "JIASHI FENG", "width": 1}, {"from": "DEPTH ANYTHING", "title": "10", "to": "HENGSHUANG ZHAO", "width": 1}, {"from": "DEPTH ANYTHING", "title": "24", "to": "CVPR", "width": 1}, {"from": "YASUHIRO MUKAIGAWA", "title": "25", "to": "ACTIVE LIGHTING", "width": 1}, {"from": "KIRIAKOS N. KUTULAKOS", "title": "27", "to": "TURBOSL", "width": 1}, {"from": "XIAOXIAO LONG", "title": "12", "to": "NERO", "width": 1}, {"from": "FIG. 8", "title": "21", "to": "PROPOSED METHOD", "width": 1}, {"from": "HANBYUL JOO", "title": "9", "to": "PANOPTIC STUDIO", "width": 1}, {"from": "OBJECT\u2011CENTRIC RAY SAMPLING", "title": "6", "to": "CERKEZI AND FAVARO", "width": 1}, {"from": "DYNAMIC SCENE (JUMPING HUMAN)", "title": "10", "to": "OUR METHOD", "width": 1}, {"from": "SPREAD SPECTRUM MODULATION", "title": "6", "to": "ILLUMINANT\u2011CAMERA COMMUNICATION", "width": 1}, {"from": "MESH SURFACES", "title": "13", "to": "2DGS", "width": 1}, {"from": "MESH SURFACES", "title": "8", "to": "POINT CLOUD", "width": 1}, {"from": "MESH SURFACES", "title": "12", "to": "POINT CLOUDS", "width": 1}, {"from": "MESH SURFACES", "title": "22", "to": "CHAMFER DISTANCE", "width": 1}, {"from": "DENSE VIEWPOINTS", "title": "6", "to": "OUR WORK", "width": 1}, {"from": "2021", "title": "24", "to": "IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION", "width": 1}, {"from": "2021", "title": "16", "to": "BARF", "width": 1}, {"from": "CARLOS HERNANDEZ", "title": "9", "to": "MULTIVIEW PHOTOMETRIC STEREO", "width": 1}, {"from": "2020", "title": "6", "to": "IEEE INTERNATIONAL CONFERENCE ON ROBOTICS AND AUTOMATION", "width": 1}, {"from": "FEATURE VECTOR", "title": "19", "to": "MLP", "width": 1}, {"from": "VOLUME SWEEPING", "title": "8", "to": "PHOTOCONSISTENCY", "width": 1}, {"from": "VOLUME SWEEPING", "title": "8", "to": "VINCENT LEROY", "width": 1}, {"from": "VOLUME SWEEPING", "title": "8", "to": "JEAN\u2011SEBASTIEN FRANCO", "width": 1}, {"from": "VOLUME SWEEPING", "title": "8", "to": "EDMOND BOYER", "width": 1}, {"from": "SIGGRAPH", "title": "6", "to": "PERFORMANCE RELIGHTING", "width": 1}, {"from": "ALEXANDER KELLER", "title": "8", "to": "INSTANT NEURAL GRAPHICS PRIMITIVES WITH A MULTIRESOLUTION HASH ENCODING", "width": 1}, {"from": "HYPERNERF", "title": "14", "to": "KEUNHONG PARK", "width": 1}, {"from": "HYPERNERF", "title": "14", "to": "UTKARSH SINHA", "width": 1}, {"from": "HYPERNERF", "title": "14", "to": "SOFIEN BOUAZIZ", "width": 1}, {"from": "HYPERNERF", "title": "18", "to": "RICARDO MARTIN-BRUALLA", "width": 1}, {"from": "HYPERNERF", "title": "14", "to": "STEVEN M. SEITZ", "width": 1}, {"from": "HYPERNERF", "title": "16", "to": "D-NERF", "width": 1}, {"from": "HYPERNERF", "title": "14", "to": "ACM TRANS. GRAPH.", "width": 1}, {"from": "HYPERNERF", "title": "37", "to": "NEURAL RADIANCE FIELDS", "width": 1}, {"from": "ALEX EVANS", "title": "8", "to": "INSTANT NEURAL GRAPHICS PRIMITIVES WITH A MULTIRESOLUTION HASH ENCODING", "width": 1}, {"from": "IMPLICIT GEOMETRIC REGULARIZATION", "title": "10", "to": "AMOS GROPP", "width": 1}, {"from": "IMPLICIT GEOMETRIC REGULARIZATION", "title": "10", "to": "LIOR YARIV", "width": 1}, {"from": "IMPLICIT GEOMETRIC REGULARIZATION", "title": "10", "to": "NIV HAIM", "width": 1}, {"from": "IMPLICIT GEOMETRIC REGULARIZATION", "title": "10", "to": "MATAN ATZMON", "width": 1}, {"from": "IMPLICIT GEOMETRIC REGULARIZATION", "title": "10", "to": "YARON LIPMAN", "width": 1}, {"from": "BASELINES", "title": "22", "to": "CAMERAS", "width": 1}, {"from": "PAOLO FAVARO", "title": "6", "to": "SPARSE 3D RECONSTRUCTION VIA OBJECT-CENTRIC RAY SAMPLING", "width": 1}, {"from": "SL BASED METHOD", "title": "9", "to": "POINT CLOUDS", "width": 1}, {"from": "PRATUL P. SRINIVASAN", "title": "27", "to": "NERF", "width": 1}, {"from": "SHAPE RECONSTRUCTION", "title": "7", "to": "MULTI-CHANNEL IMAGES", "width": 1}, {"from": "SHAPE RECONSTRUCTION", "title": "9", "to": "DYNAMIC SCENE", "width": 1}, {"from": "PHOTOMETRIC STEREO (PS)", "title": "31", "to": "ACTIVE LIGHTING", "width": 1}, {"from": "PHOTOMETRIC STEREO (PS)", "title": "12", "to": "MULTI-VIEW PHOTOMETRIC STEREO (MVPS)", "width": 1}, {"from": "PHOTOMETRIC STEREO (PS)", "title": "33", "to": "LIGHT SOURCE", "width": 1}, {"from": "PHOTOMETRIC STEREO (PS)", "title": "11", "to": "NORMAL VECTORS", "width": 1}, {"from": "PHOTOMETRIC STEREO (PS)", "title": "10", "to": "RECONSTRUCTION ACCURACY", "width": 1}, {"from": "PHOTOMETRIC STEREO (PS) TECHNIQUE", "title": "25", "to": "ACTIVE LIGHTING", "width": 1}, {"from": "EXTERNAL LIGHT", "title": "7", "to": "SHADOW", "width": 1}, {"from": "ROBERTO CIPOLLA", "title": "9", "to": "MULTIVIEW PHOTOMETRIC STEREO", "width": 1}, {"from": "TARGET OBJECT", "title": "6", "to": "OBSERVING MOVING TARGETS", "width": 1}, {"from": "TARGET OBJECT", "title": "34", "to": "SCENE", "width": 1}, {"from": "TAISUKE HASHIMOTO", "title": "8", "to": "MULTI-VIEW NEURAL SURFACE RECONSTRUCTION WITH STRUCTURED LIGHT", "width": 1}, {"from": "WEIDI XIE", "title": "10", "to": "NERF\u2212\u2212", "width": 1}, {"from": "CJ(P(T), VJP)", "title": "11", "to": "IJ(P(T))", "width": 1}, {"from": "CJ(P(T), VJP)", "title": "9", "to": "OBJECT SURFACE", "width": 1}, {"from": "CJ(P(T), VJP)", "title": "29", "to": "IRRADIANCE", "width": 1}, {"from": "CJ(P(T), VJP)", "title": "14", "to": "TRANSMITTANCE", "width": 1}, {"from": "CJ(P(T), VJP)", "title": "9", "to": "ATTENUATION", "width": 1}, {"from": "TRAINING/RENDERING SPEED", "title": "27", "to": "NERF", "width": 1}, {"from": "REAL-TIME RADIANCE FIELD RENDERING", "title": "13", "to": "3D GAUSSIAN SPLATTING", "width": 1}, {"from": "SEQUENTIAL ACQUISITION", "title": "33", "to": "SCENE", "width": 1}, {"from": "SEQUENTIAL ACQUISITION", "title": "28", "to": "LIGHT SOURCE", "width": 1}, {"from": "KEVIN WANG", "title": "21", "to": "INSTANTSPLAT", "width": 1}, {"from": "DANIEL VLASIC", "title": "11", "to": "DYNAMIC SHAPE CAPTURE USING MULTI-VIEW PHOTOMETRIC STEREO", "width": 1}, {"from": "JIAN ZHANG", "title": "21", "to": "INSTANTSPLAT", "width": 1}, {"from": "MIP-NERF 360", "title": "15", "to": "BEN MILDENHALL", "width": 1}, {"from": "MIP-NERF 360", "title": "10", "to": "PRATUL P SRINIVASAN", "width": 1}, {"from": "MIP-NERF 360", "title": "15", "to": "MIP-NERF", "width": 1}, {"from": "4 CAMS", "title": "6", "to": "ABLATION STUDY", "width": 1}, {"from": "MATTHEW O\u2019TOOLE", "title": "10", "to": "NEURAL FIELDS FOR STRUCTURED LIGHTING", "width": 1}, {"from": "LIGHT SOURCE", "title": "33", "to": "CHANNEL", "width": 1}, {"from": "LIGHT SOURCE", "title": "36", "to": "DEMULTIPLEXED IMAGES", "width": 1}, {"from": "LIGHT SOURCE", "title": "47", "to": "IRRADIANCE", "width": 1}, {"from": "LIGHT SOURCE", "title": "32", "to": "EQ. (7)", "width": 1}, {"from": "LIGHT SOURCE", "title": "31", "to": "MULTIPLEXED ILLUMINATION", "width": 1}, {"from": "LIGHT SOURCE", "title": "55", "to": "SCENE", "width": 1}, {"from": "LIGHT SOURCE", "title": "29", "to": "POINT LIGHT SOURCES", "width": 1}, {"from": "LIGHT SOURCE", "title": "27", "to": "PERSPECTIVE CAMERAS", "width": 1}, {"from": "BLENDER DATASET", "title": "4", "to": "3D MODELS", "width": 1}, {"from": "MARC POLLEFEYS", "title": "13", "to": "MULTI-VIEW STEREO BENCHMARK", "width": 1}, {"from": "VIEWPOINTS", "title": "31", "to": "3D RECONSTRUCTION", "width": 1}, {"from": "VIEWPOINTS", "title": "35", "to": "SCENE", "width": 1}, {"from": "VIEWPOINTS", "title": "20", "to": "4D SCENE RECONSTRUCTION", "width": 1}, {"from": "DUST3R+2DGS", "title": "20", "to": "CHAMFER DISTANCE", "width": 1}, {"from": "DUST3R+2DGS", "title": "22", "to": "INSTANTSPLAT", "width": 1}, {"from": "SINUSOIDAL FUNCTIONS", "title": "9", "to": "FREQUENCY REGULARIZATION", "width": 1}, {"from": "NOAH SNAVELY", "title": "26", "to": "IRON", "width": 1}, {"from": "MULTIPLE VIEW GEOMETRY", "title": "6", "to": "RICHARD HARTLEY", "width": 1}, {"from": "MULTIPLE VIEW GEOMETRY", "title": "6", "to": "CAMBRIDGE UNIVERSITY PRESS", "width": 1}, {"from": "EIGHT CAMERAS", "title": "13", "to": "DYNAMIC SCENE", "width": 1}, {"from": "EIGHT CAMERAS", "title": "16", "to": "OUR METHOD", "width": 1}, {"from": "EIGHT CAMERAS", "title": "23", "to": "4D SCENE RECONSTRUCTION", "width": 1}, {"from": "YASUSHI YAGI", "title": "11", "to": "DENSE 3D RECONSTRUCTION METHOD USING A SINGLE PATTERN FOR FAST MOVING OBJECT", "width": 1}, {"from": "YASUSHI YAGI", "title": "10", "to": "GRID-BASED ACTIVE STEREO WITH SINGLE-COLORED WAVE PATTERN FOR DENSE ONE-SHOT 3D SCAN", "width": 1}, {"from": "LIGHTS", "title": "11", "to": "LEGO", "width": 1}, {"from": "MVPSNET", "title": "10", "to": "DONGXU ZHAO", "width": 1}, {"from": "MVPSNET", "title": "10", "to": "DANIEL LICHY", "width": 1}, {"from": "MVPSNET", "title": "10", "to": "PIERRE-NICOLAS PERRIN", "width": 1}, {"from": "MVPSNET", "title": "10", "to": "SOUMYADIP SENGUPTA", "width": 1}, {"from": "MVPSNET", "title": "23", "to": "IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION", "width": 1}, {"from": "MVPSNET", "title": "10", "to": "FAST GENERALIZABLE MULTI-VIEW PHOTOMETRIC STEREO", "width": 1}, {"from": "NEURAL GEOMETRY AND BRDF RECONSTRUCTION OF REFLECTIVE OBJECTS", "title": "12", "to": "NERO", "width": 1}, {"from": "DAISUKE MIYAZAKI", "title": "25", "to": "ACTIVE LIGHTING", "width": 1}, {"from": "3D RECONSTRUCTION", "title": "49", "to": "ACTIVE LIGHTING", "width": 1}, {"from": "3D RECONSTRUCTION", "title": "29", "to": "PS", "width": 1}, {"from": "3D RECONSTRUCTION", "title": "41", "to": "IMAGES", "width": 1}, {"from": "3D RECONSTRUCTION", "title": "39", "to": "CAPTURED IMAGES", "width": 1}, {"from": "3D RECONSTRUCTION", "title": "28", "to": "EXISTING METHODS", "width": 1}, {"from": "3D RECONSTRUCTION", "title": "35", "to": "MANNEQUIN", "width": 1}, {"from": "3D RECONSTRUCTION", "title": "31", "to": "MULTIVIEW STEREO", "width": 1}, {"from": "3D RECONSTRUCTION", "title": "34", "to": "NEURAL REPRESENTATION", "width": 1}, {"from": "3D RECONSTRUCTION", "title": "31", "to": "WALKING", "width": 1}, {"from": "3D RECONSTRUCTION", "title": "31", "to": "SITTING DOWN", "width": 1}, {"from": "3D RECONSTRUCTION", "title": "31", "to": "4D SCENE", "width": 1}, {"from": "3D RECONSTRUCTION", "title": "37", "to": "MOVING OBJECTS", "width": 1}, {"from": "3D RECONSTRUCTION", "title": "28", "to": "FRAMES", "width": 1}, {"from": "LG", "title": "13", "to": "TJ", "width": 1}, {"from": "LG", "title": "16", "to": "LOSS FUNCTION", "width": 1}, {"from": "LG", "title": "17", "to": "CHANNEL", "width": 1}, {"from": "LG", "title": "20", "to": "VOLUME RENDERING", "width": 1}, {"from": "WIDE-BASELINE SETUP", "title": "21", "to": "PROPOSED METHOD", "width": 1}, {"from": "THOMAS LEIMK\u00dcHLER", "title": "13", "to": "3D GAUSSIAN SPLATTING", "width": 1}, {"from": "ABHINAV SHRIVASTAVA", "title": "8", "to": "REVISITING UNREASONABLE EFFECTIVENESS OF DATA IN DEEP LEARNING ERA", "width": 1}, {"from": "WOJCIECH MATUSIK", "title": "11", "to": "DYNAMIC SHAPE CAPTURE USING MULTI-VIEW PHOTOMETRIC STEREO", "width": 1}, {"from": "GEOMETRY CONSTRAINTS", "title": "16", "to": "REGNERF", "width": 1}, {"from": "DISTANCES", "title": "5", "to": "CONSTRAINT OF LG", "width": 1}, {"from": "SHENGHUA GAO", "title": "9", "to": "2D GAUSSIAN SPLATTING FOR GEOMETRICALLY ACCURATE RADIANCE FIELDS", "width": 1}, {"from": "REGNERF", "title": "16", "to": "JFT-300M", "width": 1}, {"from": "REGNERF", "title": "16", "to": "MICHAEL NIEMEYER", "width": 1}, {"from": "REGNERF", "title": "23", "to": "BEN MILDENHALL", "width": 1}, {"from": "REGNERF", "title": "17", "to": "MEHDI S. M. SAJJADI", "width": 1}, {"from": "REGNERF", "title": "20", "to": "ANDREAS GEIGER", "width": 1}, {"from": "REGNERF", "title": "39", "to": "NEURAL RADIANCE FIELDS", "width": 1}, {"from": "REGNERF", "title": "21", "to": "VIEW SYNTHESIS", "width": 1}, {"from": "REGNERF", "title": "30", "to": "CVPR", "width": 1}, {"from": "PIETER PEERS", "title": "11", "to": "DYNAMIC SHAPE CAPTURE USING MULTI-VIEW PHOTOMETRIC STEREO", "width": 1}, {"from": "LIGHT", "title": "13", "to": "P1", "width": 1}, {"from": "P2", "title": "18", "to": "P1", "width": 1}, {"from": "P2", "title": "14", "to": "CAMERA IMAGE", "width": 1}, {"from": "P2", "title": "11", "to": "INTENSITY CALCULATION", "width": 1}, {"from": "KONSTANTINOS REMATAS", "title": "8", "to": "SHARF: SHAPE-CONDITIONED RADIANCE FIELDS FROM A SINGLE VIEW", "width": 1}, {"from": "AARRUSHI SHANDILYA", "title": "10", "to": "NEURAL FIELDS FOR STRUCTURED LIGHTING", "width": 1}, {"from": "3D DATA PROCESSING", "title": "7", "to": "OPEN3D", "width": 1}, {"from": "ZHIWEN FAN", "title": "21", "to": "INSTANTSPLAT", "width": 1}, {"from": "ABLATION STUDY", "title": "6", "to": "W/O IRRADIANCE LOSS LG", "width": 1}, {"from": "DISTANCE", "title": "6", "to": "SIGNED DISTANCE FUNCTION (SDF)", "width": 1}, {"from": "BERNHARD KERBL", "title": "13", "to": "3D GAUSSIAN SPLATTING", "width": 1}, {"from": "BOHYUNG HAN", "title": "11", "to": "INFONERF", "width": 1}, {"from": "BEN MILDENHALL", "title": "18", "to": "MIP-NERF", "width": 1}, {"from": "BEN MILDENHALL", "title": "34", "to": "NERF", "width": 1}, {"from": "IRRADIANCE", "title": "24", "to": "IMAGE INTENSITIES", "width": 1}, {"from": "IRRADIANCE", "title": "27", "to": "P3", "width": 1}, {"from": "IRRADIANCE", "title": "30", "to": "CHANNEL", "width": 1}, {"from": "IRRADIANCE", "title": "24", "to": "PROJECTED LIGHTS", "width": 1}, {"from": "IRRADIANCE", "title": "25", "to": "DIRECTION VJ", "width": 1}, {"from": "IRRADIANCE", "title": "26", "to": "R(P(T), VJP)", "width": 1}, {"from": "IRRADIANCE", "title": "39", "to": "MLP", "width": 1}, {"from": "IRRADIANCE", "title": "26", "to": "SHADOW", "width": 1}, {"from": "FIG. 12", "title": "8", "to": "WALKING", "width": 1}, {"from": "FIG. 12", "title": "8", "to": "SITTING DOWN", "width": 1}, {"from": "NERF-BASED METHODS", "title": "14", "to": "DYNAMIC OBJECTS", "width": 1}, {"from": "NERF-BASED METHODS", "title": "14", "to": "OUR WORK", "width": 1}, {"from": "NERF-BASED METHODS", "title": "16", "to": "REGULARIZATION", "width": 1}, {"from": "NERF-BASED METHODS", "title": "13", "to": "MULTIVIEW DATASETS", "width": 1}, {"from": "NERF-BASED METHODS", "title": "13", "to": "SCENE PRIOR", "width": 1}, {"from": "NERF-BASED METHODS", "title": "41", "to": "SCENE", "width": 1}, {"from": "NERF-BASED METHODS", "title": "28", "to": "MLP", "width": 1}, {"from": "FEATURES", "title": "15", "to": "ILLUMINATION", "width": 1}, {"from": "R(P(T), VJP)", "title": "6", "to": "LAMBERTIAN SURFACE", "width": 1}, {"from": "OTTO SEISKARI", "title": "11", "to": "DN-SPLATTER", "width": 1}, {"from": "TONY TUNG", "title": "8", "to": "COMPLETE MULTI-VIEW RECONSTRUCTION OF DYNAMIC SCENES FROM PROBABILISTIC FUSION OF NARROW AND WIDE BASELINE STEREO", "width": 1}, {"from": "DANFEI XU", "title": "21", "to": "INSTANTSPLAT", "width": 1}, {"from": "PROJECTED PATTERNS", "title": "11", "to": "STRUCTURED LIGHT (SL)", "width": 1}, {"from": "PROJECTED PATTERNS", "title": "7", "to": "DENSE CORRESPONDENCE ESTIMATION", "width": 1}, {"from": "POINT LIGHT SOURCES", "title": "34", "to": "SCENE", "width": 1}, {"from": "TIME-MULTIPLEXED ILLUMINATION", "title": "6", "to": "PERFORMANCE RELIGHTING", "width": 1}, {"from": "FREENERF", "title": "16", "to": "FREQUENCY REGULARIZATION", "width": 1}, {"from": "FREENERF", "title": "14", "to": "MARCO PAVONE", "width": 1}, {"from": "FREENERF", "title": "14", "to": "YUE WANG", "width": 1}, {"from": "PERSON IN MOTION", "title": "13", "to": "DEMULTIPLEXED IMAGES", "width": 1}, {"from": "MULTIPLEXED ILLUMINATION FOR MEASURING BRDF USING AN ELLIPSOIDAL MIRROR AND A PROJECTOR", "title": "7", "to": "Y. MUKAIGAWA", "width": 1}, {"from": "MULTIPLEXED ILLUMINATION FOR MEASURING BRDF USING AN ELLIPSOIDAL MIRROR AND A PROJECTOR", "title": "7", "to": "K. SUMINO", "width": 1}, {"from": "GENGSHAN YANG", "title": "12", "to": "TOTAL-RECON", "width": 1}, {"from": "HIROMU KATO", "title": "8", "to": "MULTI-VIEW NEURAL SURFACE RECONSTRUCTION WITH STRUCTURED LIGHT", "width": 1}, {"from": "EQ. (1)", "title": "6", "to": "LOSS TO MINIMIZE", "width": 1}, {"from": "X", "title": "14", "to": "C(OK, VKX)", "width": 1}, {"from": "X", "title": "7", "to": "PX(T)", "width": 1}, {"from": "FANBO XIANG", "title": "13", "to": "MVSNERF", "width": 1}, {"from": "LARGE IMAGE SET", "title": "19", "to": "MLP", "width": 1}, {"from": "ECCV", "title": "29", "to": "NERF", "width": 1}, {"from": "PATTERN RECOGNITION (CVPR\u201906)", "title": "18", "to": "CVPR", "width": 1}, {"from": "ACM SIGGRAPH ASIA 2009", "title": "17", "to": "MULTI-VIEW PHOTOMETRIC STEREO", "width": 1}, {"from": "PIXELNERF", "title": "9", "to": "ALEX YU", "width": 1}, {"from": "PIXELNERF", "title": "9", "to": "ANGJOO KANAZAWA", "width": 1}, {"from": "PIXELNERF", "title": "23", "to": "CVPR", "width": 1}, {"from": "PIXELNERF", "title": "32", "to": "NEURAL RADIANCE FIELDS", "width": 1}, {"from": "ERRORS", "title": "10", "to": "POINT CLOUDS", "width": 1}, {"from": "MULTI-VIEW NEURAL SURFACE RECONSTRUCTION WITH STRUCTURED LIGHT", "title": "8", "to": "ARXIV PREPRINT ARXIV:2211.11971", "width": 1}, {"from": "DEMULTIPLEXING", "title": "18", "to": "IMAGES", "width": 1}, {"from": "DEMULTIPLEXING", "title": "5", "to": "DEMODULATES", "width": 1}, {"from": "F(P)", "title": "7", "to": "SURFACE SHAPE", "width": 1}, {"from": "F(P)", "title": "9", "to": "MLPS", "width": 1}, {"from": "W(T)", "title": "17", "to": "C(OK, VKX)", "width": 1}, {"from": "W(T)", "title": "10", "to": "PX(T)", "width": 1}, {"from": "W(T)", "title": "8", "to": "TK(P(T))", "width": 1}, {"from": "IJ(P(T))", "title": "8", "to": "J-TH LIGHT SOURCE", "width": 1}, {"from": "INSTANTSPLAT", "title": "26", "to": "RECONSTRUCTED SHAPES", "width": 1}, {"from": "INSTANTSPLAT", "title": "36", "to": "CHAMFER DISTANCE", "width": 1}, {"from": "INSTANTSPLAT", "title": "21", "to": "WENYAN CONG", "width": 1}, {"from": "INSTANTSPLAT", "title": "21", "to": "XINGHAO DING", "width": 1}, {"from": "INSTANTSPLAT", "title": "21", "to": "BORIS IVANOVIC", "width": 1}, {"from": "INSTANTSPLAT", "title": "24", "to": "MARCO PAVONE", "width": 1}, {"from": "INSTANTSPLAT", "title": "21", "to": "GEORGIOS PAVLAKOS", "width": 1}, {"from": "INSTANTSPLAT", "title": "21", "to": "ZHANGYANG WANG", "width": 1}, {"from": "INSTANTSPLAT", "title": "24", "to": "YUE WANG", "width": 1}, {"from": "INSTANTSPLAT", "title": "22", "to": "COMPUTER VISION AND PATTERN RECOGNITION", "width": 1}, {"from": "MULTIVIEW SYSTEM", "title": "9", "to": "PANOPTIC STUDIO", "width": 1}, {"from": "TRAINING DATASETS", "title": "5", "to": "SIMILARITY", "width": 1}, {"from": "REAL-TIME", "title": "13", "to": "3D GAUSSIAN SPLATTING", "width": 1}, {"from": "ZHENGQI LI", "title": "26", "to": "IRON", "width": 1}, {"from": "KOTA NISHIHARA", "title": "6", "to": "KYUSHU UNIVERSITY", "width": 1}, {"from": "CAMERA PARAMETER ESTIMATION", "title": "27", "to": "NERF", "width": 1}, {"from": "TRANSMITTANCE TJ(P(T))", "title": "10", "to": "INTENSITY CJ(P(T), VJP)", "width": 1}, {"from": "TRANSMITTANCE TJ(P(T))", "title": "15", "to": "VOLUME RENDERING", "width": 1}, {"from": "MULTI-VIEW NORMAL INTEGRATION", "title": "23", "to": "SUPERNORMAL", "width": 1}, {"from": "FEW-SHOT VIEW SYNTHESIS", "title": "30", "to": "NERF", "width": 1}, {"from": "FEW-SHOT VIEW SYNTHESIS", "title": "8", "to": "AJAY JAIN", "width": 1}, {"from": "FEW-SHOT VIEW SYNTHESIS", "title": "8", "to": "PIETER ABBEEL", "width": 1}, {"from": "IAROSLAV MELEKHOV", "title": "11", "to": "DN-SPLATTER", "width": 1}, {"from": "POINT CLOUD", "title": "13", "to": "SPARSEGS", "width": 1}, {"from": "BOXIN SHI", "title": "17", "to": "MULTI-VIEW PHOTOMETRIC STEREO", "width": 1}, {"from": "CHENG LIN", "title": "12", "to": "NERO", "width": 1}, {"from": "DIRECTION VJ", "title": "5", "to": "LIGHT POSITION OJ", "width": 1}, {"from": "IBRNET", "title": "20", "to": "CVPR", "width": 1}, {"from": "ACTIVE LIGHTING TECHNIQUES", "title": "10", "to": "SPARSE VIEWS", "width": 1}, {"from": "NEURAL FIELDS FOR STRUCTURED LIGHTING", "title": "10", "to": "BENJAMIN ATTAL", "width": 1}, {"from": "NEURAL FIELDS FOR STRUCTURED LIGHTING", "title": "10", "to": "CHRISTIAN RICHARDT", "width": 1}, {"from": "NEURAL FIELDS FOR STRUCTURED LIGHTING", "title": "23", "to": "IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION", "width": 1}, {"from": "NEURAL FIELDS FOR STRUCTURED LIGHTING", "title": "13", "to": "181_NEURAL_4D_SCENE_RECONSTRUC.PDF", "width": 1}, {"from": "BINBIN HUANG", "title": "9", "to": "2D GAUSSIAN SPLATTING FOR GEOMETRICALLY ACCURATE RADIANCE FIELDS", "width": 1}, {"from": "SURFACE MESH RECONSTRUCTION VIA SDF", "title": "12", "to": "SDF", "width": 1}, {"from": "EXPERIMENT", "title": "14", "to": "METHODS", "width": 1}, {"from": "EXPERIMENT", "title": "9", "to": "OBJECT", "width": 1}, {"from": "4D SCENE RECONSTRUCTION", "title": "38", "to": "NEUS", "width": 1}, {"from": "4D SCENE RECONSTRUCTION", "title": "36", "to": "SUPERNORMAL", "width": 1}, {"from": "4D SCENE RECONSTRUCTION", "title": "38", "to": "ACTIVENEUS", "width": 1}, {"from": "4D SCENE RECONSTRUCTION", "title": "40", "to": "TURBOSL", "width": 1}, {"from": "4D SCENE RECONSTRUCTION", "title": "17", "to": "BACKGROUND MASK", "width": 1}, {"from": "4D SCENE RECONSTRUCTION", "title": "19", "to": "PLANAR SURFACE", "width": 1}, {"from": "4D SCENE RECONSTRUCTION", "title": "18", "to": "OCCLUDED AREAS", "width": 1}, {"from": "ACM TRANSACTIONS ON GRAPHICS", "title": "15", "to": "3D GAUSSIAN SPLATTING", "width": 1}, {"from": "ACM TRANSACTIONS ON GRAPHICS", "title": "10", "to": "INSTANT NEURAL GRAPHICS PRIMITIVES WITH A MULTIRESOLUTION HASH ENCODING", "width": 1}, {"from": "2DGS", "title": "14", "to": "GAUSSIAN SPLATTING", "width": 1}, {"from": "2DGS", "title": "11", "to": "TSDF", "width": 1}, {"from": "2DGS", "title": "23", "to": "DUST3R", "width": 1}, {"from": "2DGS", "title": "24", "to": "CHAMFER DISTANCES", "width": 1}, {"from": "DYNAMIC SHAPE CAPTURE USING MULTI-VIEW PHOTOMETRIC STEREO", "title": "24", "to": "MULTI-VIEW PHOTOMETRIC STEREO", "width": 1}, {"from": "DYNAMIC SHAPE CAPTURE USING MULTI-VIEW PHOTOMETRIC STEREO", "title": "11", "to": "ILYA BARAN", "width": 1}, {"from": "DYNAMIC SHAPE CAPTURE USING MULTI-VIEW PHOTOMETRIC STEREO", "title": "11", "to": "PAUL DEBEVEC", "width": 1}, {"from": "DYNAMIC SHAPE CAPTURE USING MULTI-VIEW PHOTOMETRIC STEREO", "title": "11", "to": "JOVAN POPOVI\u0106", "width": 1}, {"from": "EIKONAL TERM", "title": "6", "to": "LREG", "width": 1}, {"from": "EIKONAL TERM", "title": "13", "to": "SDF", "width": 1}, {"from": "CHAMFER DISTANCES", "title": "25", "to": "MANNEQUIN", "width": 1}, {"from": "CHAMFER DISTANCES", "title": "27", "to": "PLASTER OBJECT", "width": 1}, {"from": "CHAMFER DISTANCES", "title": "46", "to": "SCENE", "width": 1}, {"from": "CHAMFER DISTANCES", "title": "31", "to": "DUST3R", "width": 1}, {"from": "CHAMFER DISTANCES", "title": "26", "to": "SPARSEGS", "width": 1}, {"from": "CHAMFER DISTANCES", "title": "40", "to": "IRON", "width": 1}, {"from": "CHAMFER DISTANCES", "title": "37", "to": "SUPERNORMAL", "width": 1}, {"from": "CHAMFER DISTANCES", "title": "39", "to": "ACTIVENEUS", "width": 1}, {"from": "CHAMFER DISTANCES", "title": "41", "to": "TURBOSL", "width": 1}, {"from": "CHAMFER DISTANCES", "title": "18", "to": "TABLE\u202f3", "width": 1}, {"from": "ANPEI CHEN", "title": "15", "to": "MVSNERF", "width": 1}, {"from": "ANPEI CHEN", "title": "11", "to": "2D GAUSSIAN SPLATTING FOR GEOMETRICALLY ACCURATE RADIANCE FIELDS", "width": 1}, {"from": "IEEE/CVF CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION (CVPR)", "title": "7", "to": "181_NEURAL_4D_SCENE_RECONSTRUC.PDF", "width": 1}, {"from": "IRRADIANCE LOSS LG", "title": "23", "to": "PROPOSED METHOD", "width": 1}, {"from": "2024 INTERNATIONAL CONFERENCE ON 3D VISION (3DV)", "title": "27", "to": "ACTIVENEUS", "width": 1}, {"from": "TURBOSL", "title": "38", "to": "CAPTURED IMAGES", "width": 1}, {"from": "TURBOSL", "title": "42", "to": "CHAMFER DISTANCE", "width": 1}, {"from": "TURBOSL", "title": "40", "to": "DUST3R", "width": 1}, {"from": "TURBOSL", "title": "49", "to": "IRON", "width": 1}, {"from": "TURBOSL", "title": "46", "to": "SUPERNORMAL", "width": 1}, {"from": "TURBOSL", "title": "48", "to": "ACTIVENEUS", "width": 1}, {"from": "TURBOSL", "title": "28", "to": "MVPS-BASED METHODS", "width": 1}, {"from": "TURBOSL", "title": "44", "to": "PROPOSED METHOD", "width": 1}, {"from": "TURBOSL", "title": "27", "to": "PARSA MIRDEHGHAN", "width": 1}, {"from": "TURBOSL", "title": "27", "to": "MAXX WU", "width": 1}, {"from": "TURBOSL", "title": "27", "to": "WENZHENG CHEN", "width": 1}, {"from": "TURBOSL", "title": "27", "to": "DAVID B. LINDELL", "width": 1}, {"from": "NERFINGMVS", "title": "12", "to": "YI WEI", "width": 1}, {"from": "NERFINGMVS", "title": "12", "to": "YONGMING RAO", "width": 1}, {"from": "NERFINGMVS", "title": "12", "to": "JIWEN LU", "width": 1}, {"from": "NERFINGMVS", "title": "12", "to": "JIE ZHOU", "width": 1}, {"from": "NERFINGMVS", "title": "35", "to": "NEURAL RADIANCE FIELDS", "width": 1}, {"from": "NERFINGMVS", "title": "25", "to": "IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION", "width": 1}, {"from": "SOCIAL MOTION CAPTURE", "title": "9", "to": "PANOPTIC STUDIO", "width": 1}, {"from": "6 LIGHTS FOR HOTDOG", "title": "4", "to": "2 CAMERAS", "width": 1}, {"from": "MEASUREMENT-BASED APPROACHES", "title": "4", "to": "FEED-FORWARD SPARSE-VIEW RECONSTRUCTION METHODS", "width": 1}, {"from": "ANTONIO TORRALBA", "title": "9", "to": "BARF", "width": 1}, {"from": "CONTROLLED LIGHTS", "title": "25", "to": "NEUS", "width": 1}, {"from": "VIEW SYNTHESIS", "title": "32", "to": "NERF", "width": 1}, {"from": "VIEW SYNTHESIS", "title": "32", "to": "NEURAL RADIANCE FIELDS", "width": 1}, {"from": "DENSE 3D RECONSTRUCTION METHOD USING A SINGLE PATTERN FOR FAST MOVING OBJECT", "title": "10", "to": "YUICHI OTA", "width": 1}, {"from": "DENSE 3D RECONSTRUCTION METHOD USING A SINGLE PATTERN FOR FAST MOVING OBJECT", "title": "15", "to": "RYO FURUKAWA", "width": 1}, {"from": "DENSE 3D RECONSTRUCTION METHOD USING A SINGLE PATTERN FOR FAST MOVING OBJECT", "title": "10", "to": "NAOKI ASADA", "width": 1}, {"from": "DENSE 3D RECONSTRUCTION METHOD USING A SINGLE PATTERN FOR FAST MOVING OBJECT", "title": "15", "to": "IEEE CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION", "width": 1}, {"from": "C(OK, VKX)", "title": "13", "to": "N", "width": 1}, {"from": "C(OK, VKX)", "title": "13", "to": "M", "width": 1}, {"from": "C(OK, VKX)", "title": "13", "to": "OK", "width": 1}, {"from": "C(OK, VKX)", "title": "15", "to": "\u02c6CKX", "width": 1}, {"from": "C(OK, VKX)", "title": "19", "to": "HIGH-DENSITY AREA", "width": 1}, {"from": "MATIAS TURKULAINEN", "title": "11", "to": "DN-SPLATTER", "width": 1}, {"from": "JUHO KANNALA", "title": "11", "to": "DN-SPLATTER", "width": 1}, {"from": "DEVA RAMANAN", "title": "12", "to": "TOTAL-RECON", "width": 1}, {"from": "MESH", "title": "8", "to": "DS", "width": 1}, {"from": "MESHING", "title": "11", "to": "DN-SPLATTER", "width": 1}, {"from": "IRON", "title": "37", "to": "CAPTURED IMAGES", "width": 1}, {"from": "IRON", "title": "41", "to": "CHAMFER DISTANCE", "width": 1}, {"from": "IRON", "title": "39", "to": "DUST3R", "width": 1}, {"from": "IRON", "title": "45", "to": "SUPERNORMAL", "width": 1}, {"from": "IRON", "title": "47", "to": "ACTIVENEUS", "width": 1}, {"from": "IRON", "title": "43", "to": "PROPOSED METHOD", "width": 1}, {"from": "IRON", "title": "26", "to": "KAI ZHANG", "width": 1}, {"from": "IRON", "title": "26", "to": "FUJUN LUAN", "width": 1}, {"from": "SDF F(PX(T))", "title": "15", "to": "ILLUMINATION", "width": 1}, {"from": "SDF F(PX(T))", "title": "8", "to": "MLPS", "width": 1}, {"from": "2D GAUSSIAN SPLATTING FOR GEOMETRICALLY ACCURATE RADIANCE FIELDS", "title": "9", "to": "ZEHAO YU", "width": 1}, {"from": "2D GAUSSIAN SPLATTING FOR GEOMETRICALLY ACCURATE RADIANCE FIELDS", "title": "13", "to": "ANDREAS GEIGER", "width": 1}, {"from": "PROPOSED METHOD", "title": "21", "to": "MULTIPLE LIGHT SOURCES", "width": 1}, {"from": "PROPOSED METHOD", "title": "21", "to": "NUMBER OF CAMERAS", "width": 1}, {"from": "PROPOSED METHOD", "title": "21", "to": "TABLE 2", "width": 1}, {"from": "PROPOSED METHOD", "title": "40", "to": "SUPERNORMAL", "width": 1}, {"from": "PROPOSED METHOD", "title": "42", "to": "ACTIVENEUS", "width": 1}, {"from": "PROPOSED METHOD", "title": "22", "to": "FIGURE\u202f9", "width": 1}, {"from": "TOTAL-RECON", "title": "10", "to": "CHONGHYUK SONG", "width": 1}, {"from": "TOTAL-RECON", "title": "12", "to": "KANGLE DENG", "width": 1}, {"from": "TOTAL-RECON", "title": "10", "to": "JUN\u2011YAN ZHU", "width": 1}, {"from": "TOTAL-RECON", "title": "17", "to": "ICCV", "width": 1}, {"from": "TOTAL-RECON", "title": "13", "to": "181_NEURAL_4D_SCENE_RECONSTRUC.PDF", "width": 1}, {"from": "3D SHAPE", "title": "4", "to": "SYSTEMS", "width": 1}, {"from": "FEW-SHOT NEURAL VOLUME RENDERING", "title": "11", "to": "INFONERF", "width": 1}, {"from": "GEORGIOS KOPANAS", "title": "13", "to": "3D GAUSSIAN SPLATTING", "width": 1}, {"from": "TAKAFUMI IWAGUCHI", "title": "8", "to": "KYUSHU UNIVERSITY", "width": 1}, {"from": "SIGNED DISTANCE FUNCTION (SDF)", "title": "6", "to": "SCALAR VALUE", "width": 1}, {"from": "SIGNED DISTANCE FUNCTION (SDF)", "title": "11", "to": "SURFACE", "width": 1}, {"from": "METHODS USING A LARGER NUMBER OF CAMERAS", "title": "5", "to": "OCCLUDED AREAS", "width": 1}, {"from": "SCENE", "title": "45", "to": "IMAGES", "width": 1}, {"from": "SCENE", "title": "32", "to": "SURFACE POINT", "width": 1}, {"from": "SCENE", "title": "36", "to": "MULTIPLEXED ILLUMINATION", "width": 1}, {"from": "SCENE", "title": "32", "to": "FIGURE 2", "width": 1}, {"from": "SCENE", "title": "41", "to": "MOVING OBJECTS", "width": 1}, {"from": "SCENE", "title": "33", "to": "MVS", "width": 1}, {"from": "SCENE", "title": "33", "to": "VOLUMETRIC REPRESENTATION", "width": 1}, {"from": "SCENE", "title": "33", "to": "BACKGROUND CURTAIN", "width": 1}, {"from": "SCENE", "title": "39", "to": "MANNEQUIN", "width": 1}, {"from": "SCENE", "title": "41", "to": "PLASTER OBJECT", "width": 1}, {"from": "SCENE", "title": "36", "to": "PERSON", "width": 1}, {"from": "COMPUTER VISION", "title": "28", "to": "ACTIVE LIGHTING", "width": 1}, {"from": "COMPUTER VISION", "title": "30", "to": "NEURAL RADIANCE FIELDS", "width": 1}, {"from": "ACTIVE-LIGHT-BASED APPROACHES", "title": "9", "to": "STRUCTURED LIGHT (SL)", "width": 1}, {"from": "ACTIVE-LIGHT-BASED APPROACHES", "title": "23", "to": "CAMERAS", "width": 1}, {"from": "RECONSTRUCTED SHAPES", "title": "32", "to": "NERF", "width": 1}, {"from": "RECONSTRUCTED SHAPES", "title": "12", "to": "DS", "width": 1}, {"from": "RECONSTRUCTED SHAPES", "title": "22", "to": "DUST3R", "width": 1}, {"from": "ICML", "title": "8", "to": "SHARF: SHAPE-CONDITIONED RADIANCE FIELDS FROM A SINGLE VIEW", "width": 1}, {"from": "P1", "title": "14", "to": "CAMERA IMAGE", "width": 1}, {"from": "GEORGE DRETTAKIS", "title": "13", "to": "3D GAUSSIAN SPLATTING", "width": 1}, {"from": "YASUYUKI MATSUSHITA", "title": "25", "to": "ACTIVE LIGHTING", "width": 1}, {"from": "4D SCENE", "title": "14", "to": "NEURAL REPRESENTATIONS", "width": 1}, {"from": "TORSTEN SATTLER", "title": "13", "to": "MULTI-VIEW STEREO BENCHMARK", "width": 1}, {"from": "VOXEL DENSITIES", "title": "6", "to": "NEURAL IMPLICIT REPRESENTATION METHODS", "width": 1}, {"from": "IRRADIANCE CONSTRAINT", "title": "6", "to": "RAY FROM LIGHT", "width": 1}, {"from": "IRRADIANCE CONSTRAINT", "title": "6", "to": "INTENSITY", "width": 1}, {"from": "IRRADIANCE CONSTRAINT", "title": "6", "to": "MONOTONICITY", "width": 1}, {"from": "JIEPENG WANG", "title": "12", "to": "NERO", "width": 1}, {"from": "METHODS", "title": "10", "to": "3D GAUSSIAN SPLATTING (3DGS)", "width": 1}, {"from": "METHODS", "title": "10", "to": "MONOCULAR", "width": 1}, {"from": "METHODS", "title": "10", "to": "MULTIVIEW DEPTH", "width": 1}, {"from": "METHODS", "title": "10", "to": "RESULTS", "width": 1}, {"from": "LC", "title": "17", "to": "VOLUME RENDERING", "width": 1}, {"from": "LC", "title": "13", "to": "LOSS FUNCTION", "width": 1}, {"from": "VIEW-INDEPENDENT SURFACE", "title": "27", "to": "NERF", "width": 1}, {"from": "MIN CHEN", "title": "10", "to": "NERF\u2212\u2212", "width": 1}, {"from": "TRANSMITTANCE", "title": "14", "to": "EQ. (7)", "width": 1}, {"from": "TRANSMITTANCE", "title": "10", "to": "INTER\u2011REFLECTIONS", "width": 1}, {"from": "TRANSMITTANCE", "title": "18", "to": "VOLUME RENDERING", "width": 1}, {"from": "ILLUMINATION", "title": "14", "to": "MULTI-CHANNEL IMAGE", "width": 1}, {"from": "ILLUMINATION", "title": "14", "to": "IMAGE", "width": 1}, {"from": "MULTIPLEXED ILLUMINATION TECHNIQUE", "title": "29", "to": "ACTIVE LIGHTING", "width": 1}, {"from": "MULTIPLEXED ILLUMINATION TECHNIQUE", "title": "8", "to": "SIGNAL-TO-NOISE RATIO (SNR)", "width": 1}, {"from": "MULTIPLEXED ILLUMINATION TECHNIQUE", "title": "15", "to": "NEURAL REPRESENTATIONS", "width": 1}, {"from": "VITTORIO FERRARI", "title": "8", "to": "SHARF: SHAPE-CONDITIONED RADIANCE FIELDS FROM A SINGLE VIEW", "width": 1}, {"from": "OPTICAL FLOW", "title": "5", "to": "WENGER ET AL.", "width": 1}, {"from": "OPTICAL FLOW", "title": "6", "to": "MOVING SUBJECTS", "width": 1}, {"from": "TAKU KOMURA", "title": "14", "to": "NERO", "width": 1}, {"from": "TAKU KOMURA", "title": "27", "to": "NEUS", "width": 1}, {"from": "DS", "title": "7", "to": "OBJECT MASKS", "width": 1}, {"from": "SIMON LUCEY", "title": "9", "to": "BARF", "width": 1}, {"from": "CVPR", "title": "31", "to": "DUST3R", "width": 1}, {"from": "CVPR", "title": "37", "to": "SUPERNORMAL", "width": 1}, {"from": "NERF\u2212\u2212", "title": "10", "to": "ZIRUI WANG", "width": 1}, {"from": "NERF\u2212\u2212", "title": "10", "to": "SHANGZHE WU", "width": 1}, {"from": "NERF\u2212\u2212", "title": "33", "to": "NEURAL RADIANCE FIELDS", "width": 1}, {"from": "NERF\u2212\u2212", "title": "12", "to": "ARXIV PREPRINT", "width": 1}, {"from": "MUKAIGAWA ET AL.", "title": "4", "to": "BRDF", "width": 1}, {"from": "DIAMETER", "title": "5", "to": "BACKGROUND CURTAIN", "width": 1}, {"from": "SHADOW", "title": "8", "to": "PLANAR SURFACE", "width": 1}, {"from": "TAKASHI MATSUYAMA", "title": "8", "to": "COMPLETE MULTI-VIEW RECONSTRUCTION OF DYNAMIC SCENES FROM PROBABILISTIC FUSION OF NARROW AND WIDE BASELINE STEREO", "width": 1}, {"from": "POSITIONAL ENCODING", "title": "19", "to": "MLP", "width": 1}, {"from": "NARROW BASELINE STEREO", "title": "6", "to": "PROBABILISTIC FUSION", "width": 1}, {"from": "INSTANT NEURAL GRAPHICS PRIMITIVES WITH A MULTIRESOLUTION HASH ENCODING", "title": "8", "to": "THOMAS M\u00dcLLER", "width": 1}, {"from": "INSTANT NEURAL GRAPHICS PRIMITIVES WITH A MULTIRESOLUTION HASH ENCODING", "title": "8", "to": "CHRISTOPH SCHIED", "width": 1}, {"from": "POINT-CLOUD-BASED MULTIVIEW STEREO ALGORITHM", "title": "9", "to": "WENLI XU", "width": 1}, {"from": "POINT-CLOUD-BASED MULTIVIEW STEREO ALGORITHM", "title": "9", "to": "FREE-VIEWPOINT VIDEO", "width": 1}, {"from": "POINT-CLOUD-BASED MULTIVIEW STEREO ALGORITHM", "title": "9", "to": "IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS", "width": 1}, {"from": "POINT-CLOUD-BASED MULTIVIEW STEREO ALGORITHM", "title": "10", "to": "2009", "width": 1}, {"from": "BARF", "title": "9", "to": "CHEN-HSUAN LIN", "width": 1}, {"from": "BARF", "title": "9", "to": "WEI-CHIU MA", "width": 1}, {"from": "BARF", "title": "16", "to": "ICCV", "width": 1}, {"from": "NEURAL\u2011BASED METHODS", "title": "7", "to": "SFM/V\u2011SLAM", "width": 1}, {"from": "NEURAL\u2011BASED METHODS", "title": "6", "to": "STATIC SCENES", "width": 1}, {"from": "NEURAL\u2011BASED METHODS", "title": "15", "to": "MOVING OBJECTS", "width": 1}, {"from": "MIJEONG KIM", "title": "11", "to": "INFONERF", "width": 1}, {"from": "RAY DIRECTION", "title": "8", "to": "INTENSITY CJ(P(T), VJP)", "width": 1}, {"from": "J-TH LIGHT SOURCE", "title": "10", "to": "INTENSITY CJ(P(T), VJP)", "width": 1}, {"from": "4D SCENES", "title": "8", "to": "STATIONARY MULTI\u2011CAMERA SETUPS", "width": 1}, {"from": "4D SCENES", "title": "13", "to": "OUR METHOD", "width": 1}, {"from": "P. DEBEVEC", "title": "4", "to": "HAWKINS", "width": 1}, {"from": "SHARED PARAMETERS", "title": "19", "to": "MLP", "width": 1}, {"from": "MOVING SUBJECTS", "title": "6", "to": "SIGNAL PROCESSING", "width": 1}, {"from": "MEHDI S. M. SAJJADI", "title": "7", "to": "ROBUST MULTIVIEW PHOTOMETRIC STEREO", "width": 1}, {"from": "WENPING WANG", "title": "14", "to": "NERO", "width": 1}, {"from": "WENPING WANG", "title": "27", "to": "NEUS", "width": 1}, {"from": "FUQIANG ZHAO", "title": "13", "to": "MVSNERF", "width": 1}, {"from": "SIGNAL PROCESSING", "title": "5", "to": "SAGAWA ET AL.", "width": 1}, {"from": "REGULARIZATION", "title": "30", "to": "NERF", "width": 1}, {"from": "PANOPTIC STUDIO", "title": "22", "to": "IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION", "width": 1}, {"from": "PANOPTIC STUDIO", "title": "9", "to": "IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV)", "width": 1}, {"from": "ILLUMINANT\u2011CAMERA COMMUNICATION", "title": "15", "to": "MOVING OBJECTS", "width": 1}, {"from": "BASELINE", "title": "25", "to": "CAMERAS", "width": 1}, {"from": "BASELINE", "title": "7", "to": "FIG. 3(A)", "width": 1}, {"from": "TSDF", "title": "8", "to": "OPEN3D", "width": 1}, {"from": "PIXEL X", "title": "7", "to": "CAMERA IMAGE", "width": 1}, {"from": "NEURAL RADIANCE FIELDS", "title": "27", "to": "FRANCESCO SARNO", "width": 1}, {"from": "NEURAL RADIANCE FIELDS", "title": "27", "to": "IEEE/CVF WINTER CONFERENCE ON APPLICATIONS OF COMPUTER VISION", "width": 1}, {"from": "NEURAL RADIANCE FIELDS", "title": "50", "to": "NERF", "width": 1}, {"from": "NEURAL RADIANCE FIELDS", "title": "29", "to": "D-NERF", "width": 1}, {"from": "ACTIVE LIGHTING", "title": "32", "to": "NEURAL REPRESENTATIONS", "width": 1}, {"from": "ACTIVE LIGHTING", "title": "25", "to": "KATSUSHI IKEUCHI", "width": 1}, {"from": "ACTIVE LIGHTING", "title": "30", "to": "RYO FURUKAWA", "width": 1}, {"from": "LEGO", "title": "31", "to": "NEUS", "width": 1}, {"from": "LEGO", "title": "28", "to": "CAMERAS", "width": 1}, {"from": "LEGO", "title": "11", "to": "FIGURE\u202f8", "width": 1}, {"from": "REVISITING UNREASONABLE EFFECTIVENESS OF DATA IN DEEP LEARNING ERA", "title": "8", "to": "SAURABH SINGH", "width": 1}, {"from": "REVISITING UNREASONABLE EFFECTIVENESS OF DATA IN DEEP LEARNING ERA", "title": "14", "to": "IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION", "width": 1}, {"from": "SURFACE", "title": "11", "to": "HIGH\u2011DENSITY AREA", "width": 1}, {"from": "SURFACE", "title": "15", "to": "HIGH-DENSITY AREA", "width": 1}, {"from": "SCHECHNER ET AL.", "title": "8", "to": "MULTIPLEXED ILLUMINATION", "width": 1}, {"from": "NEURAL REPRESENTATION", "title": "10", "to": "SHAPE INFORMATION", "width": 1}, {"from": "NEURAL REPRESENTATION", "title": "13", "to": "MULTIVIEW STEREO", "width": 1}, {"from": "\u02c6CKX", "title": "12", "to": "HIGH-DENSITY AREA", "width": 1}, {"from": "STRUCTURED LIGHT (SL)", "title": "8", "to": "HIGH-FREQUENCY PATTERNS", "width": 1}, {"from": "REN NG", "title": "27", "to": "NERF", "width": 1}, {"from": "HIGH-SPEED CAMERAS", "title": "13", "to": "DEMULTIPLEXED IMAGES", "width": 1}, {"from": "3 CAMERAS", "title": "4", "to": "6 LIGHTS FOR LEGO", "width": 1}, {"from": "SUNGHO JO", "title": "7", "to": "ACTIVE 3D MODELING VIA ONLINE MULTI-VIEW STEREO", "width": 1}, {"from": "2023", "title": "13", "to": "ICCV", "width": 1}, {"from": "CAPTURED IMAGES", "title": "17", "to": "LOSS", "width": 1}, {"from": "CAPTURED IMAGES", "title": "34", "to": "SUPERNORMAL", "width": 1}, {"from": "CAPTURED IMAGES", "title": "36", "to": "ACTIVENEUS", "width": 1}, {"from": "CAPTURED IMAGES", "title": "24", "to": "DEMULTIPLEXED IMAGES", "width": 1}, {"from": "CAPTURED IMAGES", "title": "24", "to": "PLASTER OBJECT", "width": 1}, {"from": "STATIONARY CAMERAS", "title": "9", "to": "SPARSE VIEWS", "width": 1}, {"from": "RAVI RAMAMOORTHI", "title": "27", "to": "NERF", "width": 1}, {"from": "LREG", "title": "10", "to": "LOSS FUNCTION", "width": 1}, {"from": "ESA RAHTU", "title": "11", "to": "DN-SPLATTER", "width": 1}, {"from": "NEUS", "title": "30", "to": "FREQUENCY REGULARIZATION", "width": 1}, {"from": "NEUS", "title": "40", "to": "CHAMFER DISTANCE", "width": 1}, {"from": "NEUS", "title": "27", "to": "PENG WANG", "width": 1}, {"from": "NEUS", "title": "27", "to": "LINGJIE LIU", "width": 1}, {"from": "NEUS", "title": "27", "to": "YUAN LIU", "width": 1}, {"from": "NEUS", "title": "25", "to": "CHRISTIAN THEOBALT", "width": 1}, {"from": "NEUS", "title": "25", "to": "NEURAL IMPLICIT SURFACES", "width": 1}, {"from": "NEUS", "title": "34", "to": "VOLUME RENDERING", "width": 1}, {"from": "NEUS", "title": "27", "to": "MULTI-VIEW RECONSTRUCTION", "width": 1}, {"from": "MULTI-CAMERA VIDEOS", "title": "11", "to": "MULTI-VIEW STEREO BENCHMARK", "width": 1}, {"from": "S.K. NAYAR", "title": "10", "to": "IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION", "width": 1}, {"from": "JINGYI YU", "title": "13", "to": "MVSNERF", "width": 1}, {"from": "CJ(P3, VJP3)", "title": "7", "to": "P3", "width": 1}, {"from": "SIGNAL-TO-NOISE RATIO", "title": "13", "to": "DEMULTIPLEXED IMAGES", "width": 1}, {"from": "PHOTOMETRIC STEREO", "title": "6", "to": "KAYA ET AL.", "width": 1}, {"from": "PHOTOMETRIC STEREO", "title": "21", "to": "MLP", "width": 1}, {"from": "RICARDO MARTIN-BRUALLA", "title": "15", "to": "MIP-NERF", "width": 1}, {"from": "RICARDO MARTIN-BRUALLA", "title": "12", "to": "SHARF: SHAPE-CONDITIONED RADIANCE FIELDS FROM A SINGLE VIEW", "width": 1}, {"from": "HAO SU", "title": "13", "to": "MVSNERF", "width": 1}, {"from": "MOVING\u2011CAMERA APPROACHES", "title": "7", "to": "MVS METHODS", "width": 1}, {"from": "MOVING\u2011CAMERA APPROACHES", "title": "6", "to": "SFM/V\u2011SLAM", "width": 1}, {"from": "MULTI-VIEW STEREO BENCHMARK", "title": "13", "to": "KONRAD SCHINDLER", "width": 1}, {"from": "MULTI-VIEW STEREO BENCHMARK", "title": "15", "to": "ANDREAS GEIGER", "width": 1}, {"from": "MULTI-VIEW STEREO BENCHMARK", "title": "11", "to": "HIGH-RESOLUTION IMAGES", "width": 1}, {"from": "MULTI-VIEW STEREO BENCHMARK", "title": "16", "to": "IEEE CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION", "width": 1}, {"from": "NEURAL REPRESENTATIONS", "title": "17", "to": "OUR METHOD", "width": 1}, {"from": "NEURAL REPRESENTATIONS", "title": "12", "to": "PS", "width": 1}, {"from": "MANNEQUIN", "title": "20", "to": "PLASTER OBJECT", "width": 1}, {"from": "MANNEQUIN", "title": "29", "to": "CAMERAS", "width": 1}, {"from": "ANYSPLAT", "title": "6", "to": "ACM TRANSACTIONS ON GRAPHICS (TOG)", "width": 1}, {"from": "ANYSPLAT", "title": "19", "to": "IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION", "width": 1}, {"from": "ANYSPLAT", "title": "6", "to": "LIHAN JIANG", "width": 1}, {"from": "IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION", "title": "17", "to": "ICCV", "width": 1}, {"from": "CHANGYU DIAO", "title": "17", "to": "MULTI-VIEW PHOTOMETRIC STEREO", "width": 1}, {"from": "ACTIVENEUS", "title": "40", "to": "CHAMFER DISTANCE", "width": 1}, {"from": "ACTIVENEUS", "title": "38", "to": "DUST3R", "width": 1}, {"from": "ACTIVENEUS", "title": "44", "to": "SUPERNORMAL", "width": 1}, {"from": "ACTIVENEUS", "title": "26", "to": "MVPS-BASED METHODS", "width": 1}, {"from": "ACTIVENEUS", "title": "25", "to": "HIGH-FREQUENCY DETAILS", "width": 1}, {"from": "ACTIVENEUS", "title": "27", "to": "ACTIVE STEREO", "width": 1}, {"from": "ACTIVENEUS", "title": "27", "to": "IEEE", "width": 1}, {"from": "XUQIAN REN", "title": "11", "to": "DN-SPLATTER", "width": 1}, {"from": "PERSON", "title": "11", "to": "WALKING", "width": 1}, {"from": "PERSON", "title": "11", "to": "SITTING DOWN", "width": 1}, {"from": "NEAR-FIELD DEVIATIONS", "title": "4", "to": "VOLUMETRIC RENDERING", "width": 1}, {"from": "DYNAMIC SCENES", "title": "6", "to": "MULTI-VIEW RECONSTRUCTION", "width": 1}, {"from": "RAY ENTROPY", "title": "11", "to": "INFONERF", "width": 1}, {"from": "ZHE WU", "title": "17", "to": "MULTI-VIEW PHOTOMETRIC STEREO", "width": 1}, {"from": "REFLECTANCE TRANSFORMATION", "title": "6", "to": "PERFORMANCE RELIGHTING", "width": 1}, {"from": "SPARSE VIEWS", "title": "11", "to": "WIDE BASELINE", "width": 1}, {"from": "ROBUST SOLUTION AND BENCHMARK DATASET FOR SPATIALLY VARYING ISOTROPIC MATERIALS", "title": "17", "to": "MULTI-VIEW PHOTOMETRIC STEREO", "width": 1}, {"from": "ILLUMINATION CONDITIONS", "title": "6", "to": "MULTI-VIEW PHOTOMETRIC STEREO (MVPS)", "width": 1}, {"from": "LOSS", "title": "21", "to": "MLP", "width": 1}, {"from": "ZHENGLONG ZHOU", "title": "17", "to": "MULTI-VIEW PHOTOMETRIC STEREO", "width": 1}, {"from": "SDF", "title": "27", "to": "MLP", "width": 1}, {"from": "SDF", "title": "12", "to": "\u2207F(P(T))", "width": 1}, {"from": "PLASTER OBJECT", "title": "13", "to": "3D SCANNER", "width": 1}, {"from": "PLASTER OBJECT", "title": "22", "to": "DEMULTIPLEXED IMAGES", "width": 1}, {"from": "PIETER ABBEEL", "title": "28", "to": "NERF", "width": 1}, {"from": "SYSTEM SETUP COSTS", "title": "22", "to": "CAMERAS", "width": 1}, {"from": "DEMULTIPLEXED IMAGES", "title": "13", "to": "CAPTURED CAMERA IMAGES", "width": 1}, {"from": "PROJECTED POINTS", "title": "6", "to": "HIGH\u2011DENSITY AREA", "width": 1}, {"from": "INTENSITY CJ(P(T), VJP)", "title": "11", "to": "MLPS", "width": 1}, {"from": "IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE", "title": "11", "to": "MULTIVIEW PHOTOMETRIC STEREO", "width": 1}, {"from": "IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE", "title": "9", "to": "ROBUST MULTIVIEW PHOTOMETRIC STEREO", "width": 1}, {"from": "MESHES", "title": "4", "to": "MULTIPLE VIEWS", "width": 1}, {"from": "NEURAL IMPLICIT REPRESENTATION METHODS", "title": "6", "to": "SIGNED DISTANCE FIELDS (SDFS)", "width": 1}, {"from": "DIFFERENTIABLE STEREOPSIS", "title": "4", "to": "DIFFERENTIABLE RENDERING", "width": 1}, {"from": "SURFACE SHAPE", "title": "11", "to": "HIGH-DENSITY AREA", "width": 1}, {"from": "DN-SPLATTER", "title": "15", "to": "GAUSSIAN SPLATTING", "width": 1}, {"from": "MLP", "title": "22", "to": "CAMERA RAY", "width": 1}, {"from": "NERO", "title": "14", "to": "YUAN LIU", "width": 1}, {"from": "NERO", "title": "14", "to": "PENG WANG", "width": 1}, {"from": "NERO", "title": "14", "to": "LINGJIE LIU", "width": 1}, {"from": "PING TAN", "title": "17", "to": "MULTI-VIEW PHOTOMETRIC STEREO", "width": 1}, {"from": "GAUSSIAN SPLATTING", "title": "16", "to": "SPARSEGS", "width": 1}, {"from": "POINT CLOUDS", "title": "10", "to": "3DGS-BASED METHODS", "width": 1}, {"from": "POINT CLOUDS", "title": "9", "to": "MVPS METHODS", "width": 1}, {"from": "ICCV", "title": "16", "to": "GRID-BASED ACTIVE STEREO WITH SINGLE-COLORED WAVE PATTERN FOR DENSE ONE-SHOT 3D SCAN", "width": 1}, {"from": "ROBUST MULTIVIEW PHOTOMETRIC STEREO", "title": "6", "to": "PLANAR MESH PARAMETERIZATION", "width": 1}, {"from": "AJAY JAIN", "title": "28", "to": "NERF", "width": 1}, {"from": "SPARSEGS", "title": "27", "to": "CHAMFER DISTANCE", "width": 1}, {"from": "SPARSEGS", "title": "25", "to": "DUST3R", "width": 1}, {"from": "PRATUL P SRINIVASAN", "title": "13", "to": "MIP-NERF", "width": 1}, {"from": "TRIANGULATION", "title": "5", "to": "ANGLES", "width": 1}, {"from": "IEEE INTERNATIONAL CONFERENCE ON ROBOTICS AND AUTOMATION", "title": "8", "to": "ACTIVE 3D MODELING VIA ONLINE MULTI-VIEW STEREO", "width": 1}, {"from": "MVSNERF", "title": "15", "to": "ARXIV PREPRINT", "width": 1}, {"from": "MVSNERF", "title": "26", "to": "IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION", "width": 1}, {"from": "ARTIFACTS", "title": "5", "to": "CONSTRAINT OF LG", "width": 1}, {"from": "EQ. (3)", "title": "23", "to": "CAMERAS", "width": 1}, {"from": "LOSS TO MINIMIZE", "title": "5", "to": "EQ. (6)", "width": 1}, {"from": "INFONERF", "title": "11", "to": "RAY ENTROPY MINIMIZATION", "width": 1}, {"from": "POSITION", "title": "8", "to": "CAMERA RAY", "width": 1}, {"from": "SHOHEI NOBUHARA", "title": "9", "to": "COMPLETE MULTI-VIEW RECONSTRUCTION OF DYNAMIC SCENES FROM PROBABILISTIC FUSION OF NARROW AND WIDE BASELINE STEREO", "width": 1}, {"from": "IEEE/CVF CON.", "title": "6", "to": "D-NERF", "width": 1}, {"from": "GRID-BASED ACTIVE STEREO WITH SINGLE-COLORED WAVE PATTERN FOR DENSE ONE-SHOT 3D SCAN", "title": "14", "to": "RYO FURUKAWA", "width": 1}, {"from": "CHAMFER DISTANCE", "title": "38", "to": "SUPERNORMAL", "width": 1}, {"from": "SHUZHE WANG", "title": "17", "to": "DUST3R", "width": 1}, {"from": "MULTI-VIEW PHOTOMETRIC STEREO", "title": "17", "to": "MIN LI", "width": 1}, {"from": "MULTI-VIEW PHOTOMETRIC STEREO", "title": "18", "to": "DYNAMIC SHAPE CAPTURE", "width": 1}, {"from": "SUPERNORMAL", "title": "36", "to": "DUST3R", "width": 1}, {"from": "IEEE 12TH INTERNATIONAL CONFERENCE ON COMPUTER VISION", "title": "8", "to": "COMPLETE MULTI-VIEW RECONSTRUCTION OF DYNAMIC SCENES FROM PROBABILISTIC FUSION OF NARROW AND WIDE BASELINE STEREO", "width": 1}, {"from": "TARGET SCENE", "title": "5", "to": "SIMILARITY", "width": 1}, {"from": "COMPLETE MULTI-VIEW RECONSTRUCTION OF DYNAMIC SCENES FROM PROBABILISTIC FUSION OF NARROW AND WIDE BASELINE STEREO", "title": "10", "to": "PROBABILISTIC FUSION", "width": 1}, {"from": "ACTIVE 3D MODELING VIA ONLINE MULTI-VIEW STEREO", "title": "7", "to": "DAEKYUM KIM", "width": 1}, {"from": "WIDE BASELINE STEREO", "title": "6", "to": "PROBABILISTIC FUSION", "width": 1}, {"from": "MIP-NERF", "title": "24", "to": "IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION", "width": 1}, {"from": "STATIONARY MULTI\u2011CAMERA SETUPS", "title": "7", "to": "MVS METHODS", "width": 1}, {"from": "LOSS FUNCTION", "title": "9", "to": "OPTIMIZATION", "width": 1}]);

                  nodeColors = {};
                  allNodes = nodes.get({ returnType: "Object" });
                  for (nodeId in allNodes) {
                    nodeColors[nodeId] = allNodes[nodeId].color;
                  }
                  allEdges = edges.get({ returnType: "Object" });
                  // adding nodes and edges to the graph
                  data = {nodes: nodes, edges: edges};

                  var options = {
    "configure": {
        "enabled": true,
        "filter": [
            "physics"
        ]
    },
    "edges": {
        "color": {
            "inherit": true
        },
        "smooth": {
            "enabled": true,
            "type": "dynamic"
        }
    },
    "interaction": {
        "dragNodes": true,
        "hideEdgesOnDrag": false,
        "hideNodesOnDrag": false
    },
    "physics": {
        "enabled": true,
        "repulsion": {
            "centralGravity": 0.2,
            "damping": 0.09,
            "nodeDistance": 100,
            "springConstant": 0.05,
            "springLength": 200
        },
        "solver": "repulsion",
        "stabilization": {
            "enabled": true,
            "fit": true,
            "iterations": 1000,
            "onlyDynamicEdges": false,
            "updateInterval": 50
        }
    }
};

                  


                  
                  // if this network requires displaying the configure window,
                  // put it in its div
                  options.configure["container"] = document.getElementById("config");
                  

                  network = new vis.Network(container, data, options);

                  

                  

                  


                  
                      network.on("stabilizationProgress", function(params) {
                          document.getElementById('loadingBar').removeAttribute("style");
                          var maxWidth = 496;
                          var minWidth = 20;
                          var widthFactor = params.iterations/params.total;
                          var width = Math.max(minWidth,maxWidth * widthFactor);
                          document.getElementById('bar').style.width = width + 'px';
                          document.getElementById('text').innerHTML = Math.round(widthFactor*100) + '%';
                      });
                      network.once("stabilizationIterationsDone", function() {
                          document.getElementById('text').innerHTML = '100%';
                          document.getElementById('bar').style.width = '496px';
                          document.getElementById('loadingBar').style.opacity = 0;
                          // really clean the dom element
                          setTimeout(function () {document.getElementById('loadingBar').style.display = 'none';}, 500);
                      });
                  

                  return network;

              }
              drawGraph();
        </script>
    </body>
</html>