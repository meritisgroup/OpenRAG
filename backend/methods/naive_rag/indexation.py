from ...utils.splitter import get_splitter
from ...database.data_extraction import DocumentText
from ...database.rag_classes import Document
from tqdm.auto import tqdm
from ...utils.progress import ProgressBar
import os
import numpy as np
import concurrent.futures
from pathlib import Path
from ...database.rag_classes import Chunk
from ...utils.threading_utils import get_executor_threads


def indexation(data_manager, doc_chunks, path_docs):
        """
        Adds a batch of chunks from doc_chunks to the indexation vectorbase
        Args:
            doc_chunks (list[str]) : Chunks to be indexed
            name_docs (list[str]) : Name of docs each chunk is from

        Returns
            None
        """

        tokens = 0
        taille_batch = 1000
        range_chunks = range(0, len(doc_chunks), taille_batch)
        for i in range_chunks:
            tokens += np.sum(
                data_manager.add_str_batch_elements(
                    chunks=doc_chunks[i : i + taille_batch],
                    path_docs=path_docs[i : i + taille_batch],
                    display_message=False,
                )
            )
        return tokens


def process_single_doc(
    data_manager,
    path_doc: str,
    doc_index: int,
    config_server: dict,
    splitter, 
    chunk_size: int,
    chunk_overlap: bool,
    reset_preprocess: bool
) -> dict:
    
    doc = DocumentText(path=path_doc,
                       doc_index=doc_index,
                       config_server=config_server,
                       splitter=splitter,
                       reset_preprocess=reset_preprocess)

    doc_chunks = doc.chunks(chunk_size=chunk_size,
                            chunk_overlap=chunk_overlap)
    path_docs = [str(Path(path_doc).parent)] * len(doc_chunks)

    doc_indexation_tokens = 0

    doc_indexation_tokens = indexation(data_manager=data_manager,
                                       doc_chunks=doc_chunks,
                                       path_docs=path_docs)

    return {
        "name": str(Path(path_doc).name),
        "path": str(path_doc),
        "embedding_tokens": int(doc_indexation_tokens),
        "parent_path": str(Path(path_doc).parent),
    }



class NaiveRagIndexation:
    def __init__(
        self,
        data_manager,
        agent,
        embedding_model,
        type_text_splitter="TextSplitter",
    ) -> None:
        """
        Args:
            data_path (str) : path of the folder containing texts you want to have a RAG on
            db (DataBase) : database from ContextualRetrievalRagAgent
            vb (VectorBase) : vectorbase from ContextualRetrievalRagAgent
            model (str) :  Model used to generate chunk context
            language (str) : language the prompts will be written in ("FR" and "EN" available)
            params_host_llm(dict): parameters for Ollama or VLLM, to be set in backend/config_server.json file

        Returns:
            None
        """
        self.data_manager = data_manager
        self.agent = agent
        self.embedding_model = embedding_model

        self.splitter = get_splitter(type_text_splitter=type_text_splitter,
                                     agent=self.agent,
                                     embedding_model=self.embedding_model)


    def run_pipeline(self,
                     config_server,
                     reset_preprocess: bool = False,
                     chunk_size: int = 500,
                     chunk_overlap: bool = True,
                     max_workers: int = 10) -> None:
        """
        Split texts from self.data_path, embed them and save them in a vector base.

        Args:
            chunk_size (int): Size of chunks for text splitting.
            chunk_overlap (bool): True if you want the end of the chunk n-1 be the beginning of the chunk n.

        Returns:
            None
        """

        docs_already_processed = [
            res[0] for res in self.data_manager.query(Document.path)
        ]

        to_process_norm = [
            Path(p).resolve().as_posix()
            for p in self.data_manager.get_list_path_documents()
        ]
        docs_already_norm = [
            Path(p).resolve().as_posix() for p in docs_already_processed
        ]
        docs_to_process = [
            doc for doc in to_process_norm if doc not in docs_already_norm
        ]
        if max_workers<=get_executor_threads():
            max_workers = 1


        self.data_manager.create_collection()
        progress_bar = ProgressBar(total=len(docs_to_process))
        index = 0
        print("max", max_workers)
        with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
            futures = {
                executor.submit(process_single_doc,
                                self.data_manager,
                                path_doc,
                                i,
                                config_server,
                                self.splitter,
                                chunk_size,
                                chunk_overlap,
                                reset_preprocess
                            ): path_doc for i, path_doc in enumerate(docs_to_process)
                        }

            for future in concurrent.futures.as_completed(futures):
                    path_doc = futures[future]

                    result = future.result()

                    new_doc = Document(name=result["name"],
                                       path=result["path"],
                                       embedding_tokens=result["embedding_tokens"],
                                       input_tokens=0,
                                       output_tokens=0)

                    progress_bar.update(index)
                    index+=1
                    print(index)
                    self.data_manager.add_instance(instance=new_doc,
                                           path=result["parent_path"]
                    )
        progress_bar.clear()



def contexts_to_prompts(contexts, docs_name):
    context = ""
    docs_context = []
    for i in range(len(contexts)):
        if contexts[i] not in context:
            context += contexts[i] + "\n[...]\n"

            if len(docs_name) > 0:
                docs_context.append(docs_name[i])
    if len(docs_name) > 0:
        return context[:-7], docs_context
    else:
        return context[:-7]


def concat_chunks(chunk_list: list[Chunk]) -> str:
    context = ""
    for chunk in chunk_list:
        if chunk.text not in context:
            context += chunk.text + "\n[...]\n"
    return context
