# -*- coding: utf-8 -*-
"""
Created on Thu Feb  6 16:37:47 2025

@author: chardy
"""
from ...base_classes import RagAgent
from ..naive_rag.agent import NaiveRagAgent
from ..advanced_rag.agent import AdvancedRag
from ...database.database_class import get_management_data
from ...utils.agent import get_Agent
from .prompts import prompts
import numpy as np
from pydantic import BaseModel
from ..query_reformulation.query_reformulation import query_reformulation


class CompareQueryAnswer(BaseModel):
    Decision: bool


class AgenticRagAgent(NaiveRagAgent):
    "Iterative RAG"

    def __init__(
        self, config_server: dict, dbs_name: list[str], data_folders_name: list[str]
    ) -> None:
        """
        Args:
            model (str): model used to generate answer, to be set in backend/methods/naive_rag/config.json file
            storage_path: folder in which database will be stored
            params_host_llm(dict): parameters for Ollama or VLLM, to be set in backend/config_server.json file
            params_vectorbase(dict): vectorbase connection parameters, to be set in backend/config_server.json file
            embedding_model (str): Model used to embed documents and queries, to be set in backend/methods/naive_rag/config.json file
            language (str) : Sets the language of the prompts (available "EN", "FR"), to be set in in backend/methods/naive_rag/config.json file
            api_key (str) : API key to be used if needed, to be set in backend/config_server.json file (not mandatory if using Ollama or VLLM)
            db_name (str) : Name given to the database that keeps track of already processed docs, if it already exists adds new documents to the existing database (stored in storage/ folder)
            vb_name (str) : Name given to the vectorbase, if it already exists adds new documents to the existing vectorbase (stored in milvus/elasticsearch docker)
            type_retrieval (str) : How documents will be retrieved (embeddings, BM25, vlm_embeddings are available plus hybrid if using elasticsearch)
        """

        super().__init__(
            config_server=config_server,
            dbs_name=dbs_name,
            data_folders_name=data_folders_name,
        )

        self.prompts = prompts[self.language]

    def get_nb_token_embeddings(self):
        return self.vb.get_nb_token_embeddings()

    def evaluate(self, query: str, answer: str, agent) -> bool:
        """
        Uses an LLM to determine whether the given answer fully satisfies the original question.

        Args:
            query (str): The user's original question.
            answer (str): The generated answer so far.
            agent (Agent): An Agent object with a `predict` method.

        Returns:
            bool: True if the answer is judged to be complete and correct, False otherwise.
        """
        system_prompt = self.prompts["evaluate"]["SYSTEM_PROMPT"]
        user_prompt = self.prompts["evaluate"]["QUERY_TEMPLATE"].format(
            query=query, answer=answer
        )

        result = agent.predict_json(
            system_prompt=system_prompt,
            prompt=user_prompt,
            json_format=CompareQueryAnswer,
        )

        return result.Decision

    def reformulate(self, query: str, answer: str, agent) -> str:
        """
        Compares the original query and the generated answer, then generates a new query
        that targets only the missing information in the answer.

        Args:
            query (str): The user's original question.
            answer (str): The previously generated answer.
            agent: The LLM agent used for query reformulation.

        Returns:
            str: A new query focused solely on the missing elements.
        """

        system_prompt = self.prompts["reformulate"]["SYSTEM_PROMPT"]
        user_prompt = self.prompts["reformulate"]["QUERY_TEMPLATE"].format(
            query=query, answer=answer
        )
        new_query = agent.predict(system_prompt=system_prompt, prompt=user_prompt)

        return new_query

    def concatene(
        self,
        answer_init: str,
        answer_add: str,
        query: str,
        agent,
        options_generation=None,
    ) -> str:
        """
        Combines the initial and additional answers into a single, complete response
        that best answers the original user query.

        Args:
            answer_init (str): The initial answer generated by the system.
            answer_add (str): The additional answer obtained from a follow-up query.
            query (str): The original user question.
            agent: The LLM agent used to synthesize the final answer.

        Returns:
            str: A single, coherent, and complete response answering the original query.
        """

        system_prompt = self.prompts["concatenete"]["SYSTEM_PROMPT"]
        user_prompt = self.prompts["concatenete"]["QUERY_TEMPLATE"].format(
            query=query, answer_init=answer_init, answer_add=answer_add
        )

        final_answer = agent.predict(
            system_prompt=system_prompt,
            prompt=user_prompt,
            options_generation=options_generation,
        )

        return final_answer["texts"]

    def generate_answer(
        self, query: str, nb_chunks: int = 5, max_iter=0, options_generation=None
    ) -> str:
        """
        Takes a query, retrieves appropriated context and generates an answer
        Args:
            query (str) : The query that needs answering
            model (str) : name of the model used to answer
            nb_chunks (int) : number of chunks to retrieve

        Output:
            answer (str) : The answer to the query given the retrieved context
        """
        agent = self.agent

        iter = 0
        info = super().generate_answer(
            query, nb_chunks=nb_chunks, options_generation=options_generation
        )
        answer = info["answer"]

        nb_input_tokens = info["nb_input_tokens"]
        nb_output_tokens = info["nb_output_tokens"]
        context_tot = info["context"]
        docs_name = info["docs_name"]
        impacts, energies = info["impacts"], info["energy"]

        while iter <= max_iter and not self.evaluate(query, answer, agent):
            iter += 1
            query_additional = self.reformulate(query, answer, agent)["texts"]

            info = super().generate_answer(
                query_additional,
                nb_chunks=nb_chunks,
                options_generation=options_generation,
            )
            answer_additional = info["answer"]

            nb_input_tokens += info["nb_input_tokens"]
            nb_output_tokens += info["nb_output_tokens"]
            context_tot += info["context"]
            docs_name += info["docs_name"]

            impacts[0] += info["impacts"][0]
            impacts[1] += info["impacts"][1]
            impacts[2] = info["impacts"][2]

            energies[0] += info["energy"][0]
            energies[1] += info["energy"][1]
            energies[2] = info["energy"][2]

            answer = self.concatene(
                answer,
                answer_additional,
                query,
                agent,
                options_generation=options_generation,
            )

        return {
            "answer": answer,
            "nb_input_tokens": nb_input_tokens,
            "nb_output_tokens": nb_output_tokens,
            "context": context_tot,
            "docs_name": docs_name,
            "impacts": impacts,
            "energy": energies,
        }

    def release_gpu_memory(self):
        self.agent.release_memory()

    def get_rag_contexts(self, queries: list[str], nb_chunks: int = 5):
        contexts = []
        names_docs = []
        for query in queries:
            context, name_docs = self.get_rag_context(query=query, nb_chunks=nb_chunks)
            contexts.append(context)
            names_docs.append(name_docs)
        return contexts, names_docs

    def generate_answers(
        self, queries: list[str], nb_chunks: int = 2, options_generation=None
    ):
        answers = []
        for query in queries:
            answer = self.generate_answer(
                query=query, nb_chunks=nb_chunks, options_generation=options_generation
            )
            answers.append(answer)
        return answers
